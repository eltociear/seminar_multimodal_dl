## Multi-purpose Models

*Author: Philipp Koch*

*Supervisor: Rasmus Hvingelby*

### Intro

After we describe further modalities in the previous sections, we will look at truly multipurpose models. Multitask multimodal models have already been proposed, like UniT, which extends the transformer architecture to deal with different modalities and tasks. However, previous multitask multimodal models remain limited in different aspects, which we will describe and discuss further. To become genuinely multipurpose, however, a model must be able to solve different tasks without fine-tuning and must be capable of dealing with different modalities. Thus, it must be able to transfer knowledge in-between tasks but must also be able to allocate capabilities for different modalities.

The recently introduced deep learning architecture Pathways is designed to be multipurpose. Pathways builds on newly designed hardware and software dedicated to addressing the challenges of contemporary deep learning models, which are ever-growing, where GPT-3 might be the most prominent example. We will discuss previous drawbacks and describe how Pathways aims to solve these issues. Besides the hardware aspect, Pathways provides a large neural network constructed as a directed acyclic graph (DAG). The input is passed through the network on different paths. Each node of the network is itself a neural network aimed at solving a specific aspect of a task. Using these different neural networks inside the model allows the model to be multitask and transfer knowledge in-between tasks. Another important aspect of this architecture is the obtained sparsity. When computed, just necessary nodes are computed, resulting in higher overall performance.

Furthermore, the model is intended to absorb different modalities as input, where no implementation has been found. Multimodality is further used hypothetically in the initial blog post. However, the similar model PathNet also achieves multimodality. The only model based on Pathways is the language model (PaLM), which is multilingual and capable of understanding code and solving mathematical tasks. However, the multimodality here remains questionable. Future Pathways-based models might provide more insight if the claim to step further toward artificial general intelligence (AGI) of the authors of Pathways and PathNet is true or not. Eventually, we will discuss the impact of the new Pathways multipurpose model since it might have a large impact on deep learning in the upcoming future. Broader applicable models will become feasible yet also centralize the usage, thus reducing accessibility and subsequently research on these models.


### Introduction
In recent years and months, more and more focus has shifted to transformers being used in a multimodal setting (e.g. @Lu2019). However, with the introduction of ViT (@Dosovitskiy2020), it became clear that these models are not just appropriate for NLP. Recent developments have proven transformers to be general models as long as input can be tokenized and presented to them. Although transformers have been successful in a multimodal and multitask learning setting, other models were also around too, and transformers might be further enhanced by using so-called Mixture-of-Expert layers as done in @Fedus2021 and recently @Mustafa2022.
In this chapter, multi-purpose models will be surveyed. At first, the term multi-purpose will be clarified since there is, to our best knowledge, no standard definition for this term. Different approaches from recent work will be presented, and eventually, the chapter will diverge to future developments, as outlined in @Dean2021. Jeff Dean proposed a promising architecture for future multi-purpose models, which will be further examined on how this proposal has already been implemented. The chapter will conclude with an outlook and a discussion on how the field will likely evolve in the following years.



### TODO
 - Broader survey on previous work (also VisualBERT, VilBERT etc.)
 - Adjacent models like FLAVA

#### Multipurpose Models
Since the early years of machine learning, multitask and multimodal learning paradigms have been around. Multitask learning @Crawshaw2020 is the paradigm of training a model on different tasks with the intention that the model transfers the learned knowledge to new tasks, such that fewer resources are required to learn new tasks. Akin to humans, it is intended that the model benefits from previously learned tasks. Humans do not learn every task from scratch. However, machine learning models do. It is assumed that related tasks let the model further generalize. Although there exists field-specific issues like catastrophic forgetting and negative transfer, this approach is also promising for future implementations.
Multimodal learning @Baltrusaitis2019 is a paradigm in which a machine learning model is supplied with multiple modalities like images, text, tables, etc. As in multitask learning, this approach is also inspired by human intelligence since humans perceive the world through multiple senses. It is thus assumed that multimodal models achieve better performance due to the higher input quality of the provided data. However, this field also has some specific problems, mainly focusing on how the different representations can be aligned when fused.
We want to marry the two paradigms to form a so-called multi-purpose model for this work. These kinds of models are both multimodal and multitask. We assume that this merge even further improves the quality of the predictions. Due to the novelty of this fusion (only a few models have been proposed, as we will see soon), there is no knowledge of any specific problems in this setup. Directions and possible implications will be discussed at the end of the chapter.

### Previous Work

#### MultiModel
The first prominent multi-purpose model is the so-called MultiModel (@Kaiser2017). This model, from the pre-transformer time, combines multiple architectural approaches from different fields to tackle both multimodality and multiple tasks. The model itself is itself inspired by the encoder-decoder architecture, popular in NLP at that time, making it an autoregressive model.
The model consists of four important modules, which are the so-called modality nets, the encoder, the  I/O Mixer, and the decoder.
The modality nets are used to form a representation on which the other modules can work such that it can be fed into the encoder or the I/O encoder (which is necessary because of the autoregressive structure) but also construct the output since they are also used to decode from the internal representation to the specific modality. For the language task, the modality net tokenizes the input sequences and is then transformed into the internal representation using learned embeddings?. For the output, the representation is fed into a simple feed-forward network, which is then fed into a softmax function. The modality net for images is multiple stacked convolution operations as done in Xception (X). Furthermore, there are also nets for audio and categorical modalities.
Inside the model, where the unified representations are used, there is the encoder, which consists of multiple convolution operations and a mixture-of-expert layer block. The output of the encoder is further passed on to the I/O mixer and the decoder which are now used to produce the output in an autoregressive way. The decoder produces the output, and the I/O mixer reads the previous output and combines it with the output of the encoder using attention and convolutional operations. Since this architecture is from the pre-transformer era, the attention mechanism used here is cross-attention.
The decoder eventually processes the output of the encoder and the I/O mixer, thus the input sequence and also the generated sequence, to produce proper output, which is done using attention and convolutional operations.

#### Unified Transformer (UniT)
The Unified Transformer (UniT) (@Hu2021) is a thoroughly used transformer network with multiple encoders for each modality. Only a visual and a text encoder have been used in the initial setting. However, the authors state that an arbitrary amount of encoders can be used. For the textual input, a BERT model (@Devlin2019) has been used, while for the visual encoder, a DETR (@Carion2020) has been used. In this approach, the images are first pre-encoded using a ResNet (@He2016). After the input image is encoded and linearly projected to the hidden dimension of the transformer, another task-specific vector is added to the data and fed into the here used visual transformer, which follows DETR. The authors chose BERT (Devlin et a. 2019) as an encoder for the textual representation. To encode the text, words of sentences are tokenized, and BERT-specific tokens, like the [CLS] token, are added. As in the visual encoder, a task-specific vector is added to the input, which is later removed from the output sequence. After the data from all modalities is encoded, it is concatenated and passed to the decoder, a vanilla transformer decoder according to Vaswani et al. 2017 and the one used in DETR. To the embedded sequence, a task-specific query representation sequence is also passed. Initially, the authors used both task-specific and task-agnostic decoders for their experiments.

After some developments in the field of multimodal transformers took place (VilBERT, ViT, etc.), the Unified Transformer (UniT) (@Hu2021) was introduced. Compared to previous approaches in the multi-purpose models, UniT aims to simplify the architecture. Achieving the capability of multitasking resulted in many hyperparameters and specific submodels to be set by hand for each task and or modality. UniT tried to achieve independence of this caveat, despite also using some task-specific submodules. The approach is as follows; transformers encode the input sequence on each domain, and the input encodings are concatenated and then passed on to a transformer decoder which is connected to task-specific output heads. Even though these heads were to be set manually, the model proved the fit of transformers for multi-purpose models. On top of the decoder, task-specific heads are used to transform the obtained sequence into a solution to the given tasks. These heads are trainable networks, which are to be switched if specific tasks are used.
The model often showed better results than a single-task specifically learned model. The model outperformed the single-task trained model for visual question answering (vqa), COCO (@Lin2014), and visual genome detection @Krishna2017. On further tasks, when the model was trained for up to 8 tasks, it showed still comparable performance, however most of the time lower, than domain-specific models like BERT, VisualBERT (@Li2019), and DETR.

#### OFA
Another transformer-based model is OFA (@Wang2022). The multi-purpose approach is implemented such that every input is tokenized into a unified vocabulary, which becomes possible since also images can be turned into tokens. Also, output sequences can be turned into their original or intended modality again, such that something like image generation from the text also becomes possible.

#### Gato
To combine different modalities and create a model which is also capable of solving different tasks, the generalist agent “gato” was introduced. To additionally improve the model, reinforcement learning was also included to allow the model to become sequential and interact with it’s environment. The model is not just able of solving text and visual tasks, but also to solve classic reinforcement learning tasks like playing ATARI games and proprioception. The results of the model are state-of-the-art.
Reinforcement Learning is another approach in machine learning, where there is supervised learning, unsupervised learning and reinforcement learning. This technique is used to model sequential decision propblems namely, modeling the decision of a model depending on a specific state. The classic RL setup consists of an environment and an agent. The agent is led by a policy function and is informed about it’s decision using reward from the environment. The policy is then updated to optimize for the as best as possible policy by maximizing the reward of the agent. RL proved to be beneficial in many different setups and led in 2016 with DeepMind’s AlphaGo to a breakthrough by beating the grandmaster of go, which is considered a very complex game.
The internally used policy is a transformer, which is capable of dealing with discrete input (tokens) and also continuous properties of it’s environment. Text, visual properties, buttons and movements are tokenized such that these entities can be embedded as it is commonly used in natural language processing. The model itself is a decoder of a transformer and is trained autoregressively, such that based on the previous sequence, the next token is predicted. This approach is akin to the GPT family. To predict multiple modalities based on previous inputs, the model needs to work with embeddings which itself are based on tokens. Although the model is is multi-purpose, different modalities are first processed using specific models at their entry points. Natural language data is encoded using SentencePiece, images are tokenized using the same procedure as in ViT and subsequently encoded using a ResNet v2 @He2016b, real world entities like buttons for games are also encoded to tokens allowing a transformer to become a multimodal model. Different techniques are applied to further represent these embeddings as sequences since transformer models are specifically designed for translation tasks and thus for sequence modeling. Text tokens remain in their intended order, while image tokens are represented as a raster to represent specific entities in a correct spatial representation. Tensors are also represented in a way such that rows are an important feature for sequencing. Nested structures are also ordered using keys to represent nesting. Specific observations for reinforcement learning are also sequenced such that actions and observations are also tokenized and sequenced.

### TODO
 - Results on tasks
 - Describe architecture in detail

### Pathway Proposal
In 2021 Dean proposed a model, which entails many similarities to the Pathnet architecture from 2017. Aiming at making models multi-purpose, the model is meant to be sparse and subdivided into many experts, allowing to deal with multiple Tasks. Instead of previous approaches where a model is solely trained to be an expert on one task, Pathways is, similar to Pathnet, aimed to be a graph of models, where each node is an ffn where data is passed along edges. Using this approach, it is intended to transfer knowledge on specific tasks between the nodes and direct problems to respective experts through the network. Another main aspect of Pathways is to increase efficiency in deep learning by sparsing out networks. The idea is not to use the whole network but to only call respective experts, which leads to a severe drop in parameters used for downstream tasks.
Pathways Architecture
To address the issues in computational limitations and further direct into the direction shown by Dean in the Pathway proposal, a new deep learning framework was introduced aiming to build the foundations of future needs in deep learning.
Pathway builds on the advances of recent years in distributed high performance systems, such that it includes sharding.
A novel approach introduced is parallel asynchronous dispatch mechanism, which allows to use the resources of the TPU Pods more effectively on smaller programs.
An important feature considering the proposal of dean however, is the aim to access fine-grained details of the model. Previous approaches are assumed to train the whole model and update every weights, in this approach for Pathways however, it was specifically addresssed to update fine-grained details fo the mdoel like weights to support sparse models like MIxture of Expert models.
Pathway Implementations
However, no truly multi-purpose model has been publish so far. The only models based on Pathways are the language model PaLM  @Chowdhery2022 and the text-to-image model parti @Yu2022. Both published less than a year after the proposal, indicating that more is to come in the next months.

### TODO
 - More details on how hardware plays in the grand Pw proposal

#### PathNet
An architecture for neural networks @Fernando2017. Also achieved multitask solving by introducing a novel training algorithm. In contrast to single-task neural networks, PathNet does not train the network solely on one task but on the whole network, but partially on one task and on a fraction of the network, randomly chosen. With this approach, knowledge sharing becomes possible.
The pathway consists of a graph of networks where the networks are organized in columns of hidden layers in the network. Each node in this graph is a neural network itself. This design intends to only train a path throughout the network on a specific task and subsequently train the network on an alternative path on another task. Thereby allowing the network to transfer already trained capability to the new task.
The model achieves this transfer by selecting the best path using an evolutional approach. At first, an initial population of paths is initialized and evaluated against each other in a binary way after trained T/some epochs. The better-performing algorithm will then be modified to further compete against other pathways. After the winner is found, the winner’s path is frozen, meaning that all weights are not updated anymore to keep the performance on the trained task and avoid catastrophic forgetting. After fixing the winning path, all other parameters are newly initialized.
Now, the same procedure is applied again to another task that is intended to benefit from the knowledge of the previous task. During this procedure, the tournament of different paths starts again, where the paths are trained and evaluated without the nodes from the previous winner path and eventually fixed again.

#### LIMoE
Nevertheless, another worth-mentioning model in the context of Pathways is the Language Image Mixture of Expert Model LIMoE (Mustafa et al. 2022). This model changes the transformer encoder structure by swapping the feed-forward layer with a mixture of the expert layer.
The model can input text and images into a single encoder without decoding the different modalities differently. At first, both images and text are tokenized and linearly transformed to fit the dimension of the encoder.

### Discussion

Considering the pace of the field and that results, as seen in Gato, flamingo and parti would not have been anticipated a few years ago, it is highly likely to see further breakthroughs in the field of multi-purpose models, which is especially the case since hardware-related issues are now being addressed and further developed. Based on these developments, more general models will be published in the near future. Models with even higher capabilities will also let new questions and problems arise. At this point, there is already the first issue arising, which is the trend of proprietary models. When GPT-3 was published, OpenAI closed access to the model leaving only a few with API keys to access the model.
In contrast to the introduction of BERT, which was open source, there did not follow a trend of GPTology as it was done with BERTology, leaving the model underresearched compared to its open source and significantly smaller peers. On the other side, there also exists a trend of open sourcing some models, as seen with GPT-J and GPT-Neo or recently with the introduction of OPT by meta. However, even though this trend exists, there is the issue of the increasing size of the models, which makes it almost impossible to train or even fine-tune these models without using massive amounts of computational resources.
Another issue that comes with the higher capability of these models is the societal impact of these models. Pop culture has severely impacted the perception of artificial intelligence, which might also become a topic in the future. It has already been a public issue that a google employee claimed that LamDA is sentient. With further high-quality models, there might be more discussions on how to deal with AI in the future. Since this already showed that the Turing test might not be an appropriate metric anymore, it might also be helpful to research new metrics for AI and possibly AGI.
In his TED-talk, Dean saw Pathway as a promising path toward AGI, which is a bold statement. However, considering the quality of already existing models, it might be a significant step in increasing the quality of generative models for solving multiple general tasks. Nevertheless, new problems will arise which are likely not solved with this model like continuous learning. The examined models and pathways are only trained once and then remain frozen in their knowledge. This issue is already visible in older language models like BERT and GPT, where Donald Trump is still president and COVID-19 does not exist.

### TODO
 - Proposal for unifying standardizing evaluation, common benchmarks etc.
 - Outlook

