<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Introducing the modalities | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Introducing the modalities | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Introducing the modalities | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="c02-00-multimodal.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Multimodal Deep Learning</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-to-multimodal-deep-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Multimodal Deep Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-01-sota-nlp"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in NLP</a></li>
<li class="chapter" data-level="2.2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-02-sota-cv"><i class="fa fa-check"></i><b>2.2</b> State-of-the-art in Computer Vision</a></li>
<li class="chapter" data-level="2.3" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-03-benchmarks"><i class="fa fa-check"></i><b>2.3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-01-img2text"><i class="fa fa-check"></i><b>3.1</b> Image2Text</a></li>
<li class="chapter" data-level="3.2" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-02-text2img"><i class="fa fa-check"></i><b>3.2</b> Text-2-image</a></li>
<li class="chapter" data-level="3.3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-03-img-support-text"><i class="fa fa-check"></i><b>3.3</b> Images supporting Language Models</a></li>
<li class="chapter" data-level="3.4" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-04-text-support-img"><i class="fa fa-check"></i><b>3.4</b> Text supporting Computer Vision Models</a></li>
<li class="chapter" data-level="3.5" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-05-text-plus-img"><i class="fa fa-check"></i><b>3.5</b> Models for both modalities</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c03-00-further.html"><a href="c03-00-further.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-01-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a></li>
<li class="chapter" data-level="4.2" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-02-structured-unstructured"><i class="fa fa-check"></i><b>4.2</b> Structured + Unstructured Data</a></li>
<li class="chapter" data-level="4.3" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-03-multi-purpose"><i class="fa fa-check"></i><b>4.3</b> Multi-Purpose Models</a></li>
<li class="chapter" data-level="4.4" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-04-usecase"><i class="fa fa-check"></i><b>4.4</b> Generative Art</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>6</b> Epilogue</a></li>
<li class="chapter" data-level="7" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>7</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="c01-00-intro-modalities" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 2</span> Introducing the modalities<a href="c01-00-intro-modalities.html#c01-00-intro-modalities" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Authors: Cem Akkus, Vladana Djakovic, Christopher Benjamin Marquardt</em></p>
<p><em>Supervisor: Dr. Matthias Aßenmacher</em></p>
<p>Natural Language Processing (NLP) has existed for about 50 years, but it is more relevant than ever. There have been several breakthroughs in this branch of machine learning that is concerned with spoken and written language. For example, learning internal representations of words was one of the greater advances of the last decade. Word embeddings (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Mikolov2013"><strong>???</strong></span>)</span>, <span class="citation">(<span class="citeproc-not-found" data-reference-id="Bojanowski2016"><strong>???</strong></span>)</span>) made it possible and allowed developers to encode words as dense vectors that capture their underlying semantic content. In this way, similar words are embedded close to each other in a lower-dimensional feature space. Another important challenge was solved by Encoder-decoder (also called sequence-to-sequence) architectures <span class="citation">(<span class="citeproc-not-found" data-reference-id="Sutskever2014"><strong>???</strong></span>)</span>, which made it possible to map input sequences to output sequences of different lengths. They are especially useful for complex tasks like machine translation, video captioning or question answering. This approach makes minimal assumptions on the sequence structure and can deal with different word orders and active, as well as passive voice.</p>
<p>A definitely significant state-of-the-art technique is Attention <span class="citation">(<span class="citeproc-not-found" data-reference-id="Bahdanau2014"><strong>???</strong></span>)</span>, which enables models to actively shift their focus – just like humans do. It allows following one thought at a time while suppressing information irrelevant to the task. As a consequence, it has been shown to significantly improve performance for tasks like machine translation. By giving the decoder access to directly look at the source, the bottleneck is avoided and at the same time, it provides a shortcut to faraway states and thus helps with the vanishing gradient problem. One of the most recent sequence data modeling techniques is Transformers (<span class="citation">Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a><a href="#ref-vaswani2017attention" role="doc-biblioref">b</a>)</span>), which are solely based on attention and do not have to process the input data sequentially (like RNNs). Therefore, the deep learning model is better in remembering context-induced earlier in long sequences. It is the dominant paradigm in NLP currently and even makes better use of GPUs, because it can perform parallel operations. Transformer architectures like BERT (<span class="citation">Devlin et al. (<a href="#ref-Devlin2018" role="doc-biblioref">2018</a><a href="#ref-Devlin2018" role="doc-biblioref">c</a>)</span>), T5 (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Raffel2019"><strong>???</strong></span>)</span>) or GPT-3 (<span class="citation">T. Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>) are pre-trained on a large corpus and can be fine-tuned for specific language tasks. They have the capability to generate stories, poems, code and much more. With the help of the aforementioned breakthroughs, deep networks have been successful in retrieving information and finding representations of semantics in the modality text. In the next paragraphs, developments for another modality image are going to be presented.</p>
<p>Computer vision (CV) focuses on replicating parts of the complexity of the human visual system and enabling computers to identify and process objects in images and videos in the same way that humans do. In recent years it has become one of the main and widely applied fields of computer science. However, there are still problems that are current research topics, whose solutions depend on the research’s view on the topic. One of the problems is how to optimize deep convolutional neural networks for image classification. The accuracy of classification depends on width, depth and image resolution. One way to address the degradation of training accuracy is by introducing a deep residual learning framework <span class="citation">(He et al. <a href="#ref-ResNet" role="doc-biblioref">2015</a>)</span>. On the other hand, another less common method is to scale up ConvNets, to achieve better accuracy is by scaling up image resolution. Based on this observation, there was proposed a simple yet effective compound scaling method, called EfficientNets <span class="citation">(Tan and Le <a href="#ref-EfficientNet" role="doc-biblioref">2019</a>)</span>.</p>
<p>Another state-of-the-art trend in computer vision is learning effective visual representations without human supervision. Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-the-art results, but the simple framework for contrastive learning of visual representations, which is called SimCLR, outperforms previous work <span class="citation">(Chen et al. <a href="#ref-SimCLR" role="doc-biblioref">2020</a>)</span>. However, another research proposes as an alternative a simple “swapped” prediction problem where we predict the code of a view from the representation of another view. Where features are learned by Swapping Assignments between multiple Views of the same image (SwAV) <span class="citation">(<span class="citeproc-not-found" data-reference-id="SwAV"><strong>???</strong></span>)</span>.
Further recent contrastive methods are trained by reducing the distance between representations of different augmented views of the same image (‘positive pairs’) and increasing the distance between representations of augmented views from different images (‘negative pairs’). Bootstrap Your Own Latent (BYOL) is a new algorithm for self-supervised learning of image representatios <span class="citation">(Grill, Strub, Altché, Tallec, Richemond, et al. <a href="#ref-BYOL" role="doc-biblioref">2020</a><a href="#ref-BYOL" role="doc-biblioref">b</a>)</span>.</p>
<p>Self-attention-based architectures, in particular, Transformers have become the model of choice in natural language processing (NLP). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention, some replacing the convolutions entirely. The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Inspired by the Transformer scaling successes in NLP, one of the experiments is applying a standard Transformer directly to the image <span class="citation">(<span class="citeproc-not-found" data-reference-id="ImageT"><strong>???</strong></span>)</span>. Due to the widespread application of computer vision, these problems differ and are constantly being at the center of attention of more and more research.</p>
<p>With the rapid development in NLP and CV in recent years, it was just a question of time to merge both modalities to tackle multi-modal tasks. The release of DALL-E 2 just hints at what one can expect from this merge in the future. DALL-E 2 is able to create photorealistic images or even art from any given text input. So it takes the information of one modality and turns it into another modality. It needs multi-modal datasets to make this possible, which are still relatively rare. This shows the importance of available data and the ability to use it even more. Nevertheless, all modalities are in need of huge datasets to pre-train their models. It’s common to pre-train a model and fine-tune it afterwards for a specific task on another dataset. For example, every state-of-the-art CV model uses a classifier pre-trained on an ImageNet based dataset. The cardinality of the datasets used for CV is immense, but the datasets used for NLP are of a completely different magnitude. BERT uses the English Wikipedia and the Bookscorpus to pre-train the model. The latter consists of almost 1 billion words and 74 million sentences. The pre-training of GPT-3 is composed of five huge corpora: CommonCrawl, Books1 and Books2, Wikipedia and WebText2. Unlike language model pre-training that can leverage tremendous natural language data, vision-language tasks require high-quality image descriptions that are hard to obtain for free. Widely used pre-training datasets for VL-PTM are Microsoft Common Objects in Context (COCO), Visual Genome (VG), Conceptual Captions (CC), Flickr30k, LAION-400M and LAION-5B, which is now the biggest openly accessible image-text dataset.</p>
<p>Besides the importance of pre-training data, there must also be a way to test or compare the different models. A reasonable approach is to compare the performance on specific tasks, which is called benchmarking. A nice feature of benchmarks is that they allow us to compare the models to a human baseline. Different metrics are used to compare the performance of the models. Accuracy is widely used, but there are also some others. For CV the most common benchmark datasets are ImageNet, ImageNetReaL, CIFAR-10(0), OXFORD-IIIT PET, OXFORD Flower 102, COCO and Visual Task Adaptation Benchmark (VTAB). The most common benchmarks for NLP are General Language Understanding Evaluation (GLUE), SuperGLUE, SQuAD 1.1, SQuAD 2.0, SWAG, RACE, ReCoRD, and CoNLL-2003. VTAB, GLUE and SuperGLUE also provide a public leader board. Cross-modal tasks such as Visual Question Answering (VQA), Visual Commonsense Reasoning (VCR), Natural Language Visual Reasoning (NLVR), Flickr30K, COCO and Visual Entailment are common benchmarks for VL-PTM.</p>

<!--

## title

*Author: *

*Supervisor: *
-->
<div id="c01-01-sota-nlp" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.1</span> State-of-the-art in NLP<a href="c01-00-intro-modalities.html#c01-01-sota-nlp" class="anchor-section" aria-label="Anchor link to header"></a></h2>

</div>
<div id="c01-02-sota-cv" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.2</span> State-of-the-art in Computer Vision<a href="c01-00-intro-modalities.html#c01-02-sota-cv" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: </em> Vladana Djakovic</p>
<p><em>Supervisor:</em> Daniel Schalk</p>
<div id="history" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.1</span> History<a href="c01-00-intro-modalities.html#history" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The first research about visual perception comes from neurophysiological research performed in the 1950s and 1960s on cats, where they used cats as a model to understand how human vision is compounded. Scientists concluded that human vision is hierarchical, and neurons detect simple features like edges, followed by more complex features like shapes and more complex visual representations. Inspired by this knowledge, computer scientists focused on recreating human neurological structures.</p>
<p>At around the same time, as computers became more advanced, computer scientists worked on imitating human neurons’ behavior and simulating a hypothetical neural network. In his book “The Organization of Behaviour”(1949) Donald Hebbin stated that neural pathways strengthen over each successive use, especially between neurons that tend to fire at the same time, thus beginning the long journey towards quantifying the complex processes of the brain. The first Hebbian network, inspired by this neurological research, was successfully implemented at MIT in 1954 <span class="citation">(<span class="citeproc-not-found" data-reference-id="history1"><strong>???</strong></span>)</span>.</p>
<p>New findings led to the establishment of the field of artificial intelligence in 1956 on-campus at Dartmouth College. Scientists began to develop ideas and research how to create techniques that would imitate the human eye.</p>
<p>In 1959 early research on developing neural networks was performed at Stanford University, where models called “ADALINE” and “MADALINE,” Multiple ADAptive LINear Elements, were developed. Those models aimed to recognize binary patterns and could predict the next bit <span class="citation">(<span class="citeproc-not-found" data-reference-id="history2"><strong>???</strong></span>)</span>.</p>
<p>Starting optimism about Computer Vision and neural networks disappeared after 1969 and the publication of the book “Perceptrons” by Marvin Minsky, founder of the MIT AI Lab. According to the authors of this book, the single perception approach to neural networks could not be translated effectively into multi-layered neural networks. The period that followed was known as AI Winter, which lasted until 2010, when the technological development of computer and the internet became widely used. In 2012 breakthroughs in Computer Vision happened at the ImageNet Large Scale Visual Recognition Challenge (ILSVEC). The team from the University of Toronto issued a deep neural network called AlexNet <span class="citation">(<span class="citeproc-not-found" data-reference-id="alexnet"><strong>???</strong></span>)</span> that changed the field of artificial intelligent Computer Vision (CV). AlexNet achieved an error rate of 16.4%.</p>
<p>From then until today, Computer Vision has been one of the fastest developing fields. Researchers are competing to develop a model that would be the most similar to the human eye and help humans in everyday life. In this work the author will describe only a few recent state-of-the-art models.</p>
</div>
<div id="supervised-and-unsupervised-learning" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.2</span> Supervised and unsupervised learning<a href="c01-00-intro-modalities.html#supervised-and-unsupervised-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As part of artificial intelligence (AI) and machine learning (ML), there are two basic approaches:</p>
<ul>
<li>supervised learning;</li>
<li>unsupervised learning.</li>
</ul>
<p>Supervised learning <span class="citation">(<span class="citeproc-not-found" data-reference-id="supervised"><strong>???</strong></span>)</span> is used to train algorithms on labeled datasets that accurately classify data or predict outcomes. With labeled inputs and outputs model can measure its accuracy and learn over time. We can distinguish two types of data mining problems:</p>
<ul>
<li>classification,</li>
<li>regression.</li>
</ul>
<p>In unsupervised learning <span class="citation">(<span class="citeproc-not-found" data-reference-id="unsupervised"><strong>???</strong></span>)</span>, unlabelled datasets are analyzed and clustered using machine learning algorithms. These algorithms aim to discover hidden patterns or data groupings without previous human intervention. The ability to find similarities and differences in information is mainly used for three main tasks:</p>
<ul>
<li>clustering,</li>
<li>association,</li>
<li>dimensionality reduction.</li>
</ul>
<p>Solving the problems where the dataset can be both labeled and unlabeled requires asemi-supervised approach that lies between supervised and unsupervised learning. It is useful when extracting relevant features from data that is complex and when data is high volume, i.e., medical images.
Nowadays, a new research topic appeared in the machine learning community, and it is Self-Supervised Learning. Self-Supervised learning is a process where the model trains itself to learn one part of the input from another <span class="citation">(<span class="citeproc-not-found" data-reference-id="selfsup"><strong>???</strong></span>)</span>. As a subset of unsupervised learning, it involves machines labeling, categorizing, and analyzing information independently and drawing conclusions based on connections and correlations. It can also be considered an autonomous form of supervised learning since it does not require human input to label data. Unlike unsupervised learning, self-supervised learning does not focus on clustering nor grouping <span class="citation">(<span class="citeproc-not-found" data-reference-id="selfsup2"><strong>???</strong></span>)</span>. One part of Self-Supervised learning is contrastive learning, which is used to learn the general features of an unlabeled dataset identifying similar and dissimilar data points. It is utilized to train the model to learn about our data without any annotations or labels <span class="citation">(<span class="citeproc-not-found" data-reference-id="contrastive"><strong>???</strong></span>)</span>.</p>
</div>
<div id="scaling-networks" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.3</span> Scaling networks<a href="c01-00-intro-modalities.html#scaling-networks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Ever since the introduction of AlexNet in 2012, the problem of scaling convolutional neural networks has become the topic of active research. ConvNet can be scaled in all three dimensions: depth, width, or image size. One of the first researches in 2015 showed that network depth is crucial for image classification. The question whether stacking more layers enables the network to learn better leads He K., Zhang X., et al. to deep residual networks called ResNet <span class="citation">(He et al. <a href="#ref-ResNet" role="doc-biblioref">2015</a>)</span>, which will be described in this work. Later on, scaling networks by their depth became the most popular way to improve their performance.
The second solution was to scale ConvNets by their width. Wider networks tend to be able to capture more fine-grained features and are easier to train <span class="citation">(<span class="citeproc-not-found" data-reference-id="width"><strong>???</strong></span>)</span>.
Lastly, scaling the image’s resolution can improve the network’s performance. With higher resolution input images, ConvNets could capture more fine-grained patterns. GPipe is one of the most famous networks created by this technique <span class="citation">(<span class="citeproc-not-found" data-reference-id="gpipe"><strong>???</strong></span>)</span>.
The question of possibility of scaling by all three dimensions was answered by Tan M. et al. in 2019 in the work presenting EfficientNet. This network was built by scaling up ConvNets by all three dimensions and will also be described here.</p>
</div>
<div id="deep-residual-networks" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.4</span> Deep residual networks<a href="c01-00-intro-modalities.html#deep-residual-networks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The deep residual networks, called ResNets, <span class="citation">(<span class="citeproc-not-found" data-reference-id="esnet"><strong>???</strong></span>)</span> were presented as the answer on the question whether stacking more layers would enable network to learn better. Until then one obstacle for simply stacking layers was the problem of vanishing/exploding gradients. It has been primarily addressed by normalized initialization and intermediate normalization layers. That enabled networks with tens of layers to start converging for stochastic gradient descent (SGD) with backpropagation.</p>
<p>Another obstacle was a degradation problem. It occurs when the network depth increases, followed by saturating and then rapidly decreasing accuracy. Overfitting is not caused by such degradation, and adding more layers to a suitably deep model leads to higher training error, which indicates that not all systems are similarly easy to optimize.</p>
<p>For example, it was suggested to consider a shallower architecture and its deeper counterpart that adds more layers. One way to avoid the degradation problem is to create a deeper model, where the auxiliary layers are identity mappings, and other layers are copied from a shallower model. The deeper model should produce no higher training error than its shallower counterpart. However, in practice, it is not the case, and it is hard to find comparably good to construct or better solutions. The solution to this degradation problem proposed by them is a deep residual learning framework.</p>
<div id="deep-residual-learning" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.4.1</span> Deep Residual Learning<a href="c01-00-intro-modalities.html#deep-residual-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="residual-learning" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.4.1.1</span> Residual Learning<a href="c01-00-intro-modalities.html#residual-learning" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The idea of residual learning is to replace the approximation of underlying mapping <span class="math inline">\(H\left( x\right)\)</span>, which is approximated by a few stacked layers (not necessarily the entire net), with an approximation of residual function <span class="math inline">\(F(x):= H\left( x \right) − x\)</span>. Here x denotes the inputs to the first of these layers, and it is assumed that both inputs and outputs have the same dimensions. The original function change its form <span class="math inline">\(F\left( x \right)+x\)</span>.</p>
<p>A counterintuitive phenomenon about degradation motivated this reformulation. The new deeper model should have no more significant training error when added layers are constructed as identity mappings. However,due to degradation problem, solvers may have challenges approximating identity mappings by multiple nonlinear layers. Using the residual learning reformulation, can drive the weights of the nonlinear layers toward zero to approach identity mappings if they are optimal.
Generally, identity mappings are not optimal, but new reformulations may help to precondition the problem. When an optimal function is closer to an identity mapping than a zero mapping, finding perturbations concerning an identity mapping should be easier than learning the function from scratch.</p>
</div>
<div id="identity-mapping-by-shortcuts" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.4.1.2</span> Identity Mapping by Shortcuts<a href="c01-00-intro-modalities.html#identity-mapping-by-shortcuts" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Residual learning is adopted to every few stacked layers where a building block is defined as shown in Fig. 1:</p>
<p><span class="math display">\[\begin{equation}
\tag{1}
y = F  \left( x,\left\{  W_i\right\} \right) + x
\end{equation}\]</span></p>
<p>x and y present the input and output vectors of the layers.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure01"></span>
<img src="figures/01-chapter1/resnetBlock.png" alt="Figure 1. Building block of residual learning" width="35%" />
<p class="caption">
FIGURE 2.1: Figure 1. Building block of residual learning
</p>
</div>
<p>The function <span class="math inline">\(F \left( x,\left\{ W_i\right\} \right)\)</span> represents the residual mapping that is to be learned. For the example with two layers from Fig.1 that has two layers, <span class="math inline">\(F = W_2\sigma\left( W_1x\right)\)</span> in which <span class="math inline">\(\sigma\)</span> denotes the ReLU activation function, and to simplify the notations, biases are left out. The operation <span class="math inline">\(F + x\)</span> is conducted with a shortcut connection and element-wise addition. Afterward, He et al. applied second nonlinearity ( i.e., <span class="math inline">\(\sigma \left( y \right)\)</span>, Fig.1).</p>
<p>The shortcut connections in Eq. (1) neither adds an extra parameter nor increases computation complexity, which enables comparisons between plain and residual networks that concurrently have the same number of parameters, depth, width, and computational cost (except for the negligible element-wise addition).
Dimensions of x and F must be equal in Eq. (1). Alternatively, to match the dimensions, linear projection <span class="math inline">\(W_s\)</span> by the shortcut connections can be applied:</p>
<p><span class="math display">\[\begin{equation}
\tag{2}
y = F  \left( x,\left\{  W_i\right\} \right)+ W_sx.
\end{equation}\]</span></p>
<p>The square matrix <span class="math inline">\(W_s\)</span> can be used in Eq. (1). However, experiments showed that identity mapping is enough to solve the degradation problem. Therefore, <span class="math inline">\(W_s\)</span> only aims to match dimensions. Although more levels are possible, it was experimented with function F having two or three layers without stating the exact form of it. Assuming F only has one layer, Eq (1) it is comparable to a linear layer: <span class="math inline">\(y = W_1 x + x\)</span>. The theoretical notations are about fully-connected layers, but convolutional layers were used. The function <span class="math inline">\(F \left( x,\left\{ W_i\right\} \right)\)</span> can be applied to represent multiple convolutional layers. Two feature maps are added element-wise, channel by channel.</p>
</div>
</div>
<div id="network-architectures" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.4.2</span> Network Architectures<a href="c01-00-intro-modalities.html#network-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To construct efficient residual network, various plain/residual networks were tested. They trained the network on benchmarked datasets, one of them being the ImageNet dataset, which will be used for a comparison of network architectures. Fig. 2 shows that every residual network needs a plain baseline network inspired by the VGG network <span class="citation">(<span class="citeproc-not-found" data-reference-id="vgg"><strong>???</strong></span>)</span> on which identity mapping by shortcuts is applied.</p>
<p><em>Plain Network</em> The philosophy of VGG nets 41 mainly inspires plain baselines. There are two rules that convolution layers, which usually have 3x3 filters, follow:</p>
<ul>
<li>feature maps with the same output size have the same number of layers;</li>
<li>reducing the size of a feature map by half doubles the number of filters per layer to maintain time complexity per layer</li>
</ul>
<p>Convolutional layers with a stride of 2 perform downsampling directly. A global average pooling layer and a 1000-way fully-connected layer with softmax are at the end of the network. The number of weighted layers sums up to 34 in Fig. 2 (middle).Compared to VGG nets, this model has fewer filters and lower complexity. (Fig. 2, left).</p>
<p><em>Residual Network</em> Based on the above plain network, additional shortcut connections (Fig. 2, right) turn the network into its associate residual variant. The identity shortcuts (Eq. (1)) can be directly used in the case of the exact dimensions of the input and output (solid line shortcuts in Fig. 2). For the different dimensions (dotted line shortcuts in Fig. 2), two options are considered:</p>
<ul>
<li>The shortcut still performs identity mapping, but with extra zero entries padded to cope with the increasing dimensions, without adding new parameters;</li>
<li>The projection shortcut in Eq. (2) matches dimensions (due to 1×1 convolutions).</li>
</ul>
<p>In both cases, shortcuts will be done with a stride of two when they go across feature maps of two sizes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure02"></span>
<img src="figures/01-chapter1/ResNet_architecture.png" alt="Figure 2. Architecture of ResNet" width="100%" />
<p class="caption">
FIGURE 2.2: Figure 2. Architecture of ResNet
</p>
</div>
</div>
</div>
<div id="efficientnet" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.5</span> EfficientNet<a href="c01-00-intro-modalities.html#efficientnet" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Until Tan M. introduced EfficientNet <span class="citation">(<span class="citeproc-not-found" data-reference-id="effecient"><strong>???</strong></span>)</span>, it was popular to scale only one of the three dimensions – depth, width, or image size. The empirical study shows that it is critical to balance all network dimensions, which can be achieved by simply scaling each with a constant ratio. Based on this observation, a simple yet effective compound scaling method was proposed, which uniformly scales network width, depth, and resolution with a set of fixed scaling coefficients.
For example, if 2N times more computational resources are available. In that case, increasing the network depth by <span class="math inline">\(\alpha N\)</span>, width by <span class="math inline">\(\beta N\)</span>, and image size by <span class="math inline">\(\gamma N\)</span> would be possible.Here <span class="math inline">\(\alpha,\beta,\gamma\)</span> are constant coefficients determined by a small grid search on the original miniature model. Fig. 3 illustrates the difference between this scaling method and conventional methods.
A compound scaling method makes sense if an input image is bigger since a larger receptive field requires more layers and more significant channel features to capture fine-grained patterns. Theoretically and empirically, there has been a special relationship between network width and depth <span class="citation">(<span class="citeproc-not-found" data-reference-id="depthwidth"><strong>???</strong></span>)</span>. Existing MobileNets <span class="citation">(<span class="citeproc-not-found" data-reference-id="mobilenet"><strong>???</strong></span>)</span> and ResNet are used to demonstrated new scaling method.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure03"></span>
<img src="figures/01-chapter1/Model_scaling.png" alt="Figure 3. Model scaling" width="100%" />
<p class="caption">
FIGURE 2.3: Figure 3. Model scaling
</p>
</div>
<div id="compound-model-scaling" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.5.1</span> Compound Model Scaling<a href="c01-00-intro-modalities.html#compound-model-scaling" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="problem-formulation" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.5.1.1</span> Problem Formulation<a href="c01-00-intro-modalities.html#problem-formulation" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A function <span class="math inline">\(Y_i = \mathcal{F}i \left( X_i \right)\)</span> with the operator <span class="math inline">\(\mathcal{F}_i\)</span>, output tensor <span class="math inline">\(Y_i\)</span>, input tensor <span class="math inline">\(X_i\)</span> of shape <span class="math inline">\(\left( H_i, W_i, C_i \right)\)</span>, spatial dimensions <span class="math inline">\(H_i\)</span>, <span class="math inline">\(W_i\)</span>, and channel dimension <span class="math inline">\(C_i\)</span> is called a ConvNet Layer <span class="math inline">\(i\)</span>. A ConvNet N appears as a list of composing layers:
<span class="math display">\[\begin{equation}
\tag{3}
\mathcal{N}=\mathcal{F_k}\odot \cdots \mathcal{F_2}\odot\mathcal{F_1}\left( X_1 \right)=\bigodot{j=1\cdots k}\mathcal{F_j}\left( X_1 \right)
\end{equation}\]</span></p>
<p>Effectively, these layers are often partitioned into multiple stages, and all layers in each stage share the same architecture. For example, ResNet has five stages, with all layers in every stage being the same convolutional type, except for the first layer that performs down-sampling. Therefore, a ConvNet can be defined as:</p>
<p><span class="math display">\[\begin{equation}
\tag{4}
 \mathcal{N}=\bigodot_{i=1\cdots s}\mathcal{F_i}^{L_i}\left( X_{\left( H_i, W_i, C_i  \right)} \right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathcal{F_i}^{L_i}\)</span> denotes layer <span class="math inline">\(\mathcal{F_i}\)</span> which is repeated <span class="math inline">\(L_i\)</span> times in stage <span class="math inline">\(i\)</span>, and <span class="math inline">\(\left( H_i, W_i, C_i \right)\)</span> is the shape of input tensor <span class="math inline">\(X\)</span> of layer <span class="math inline">\(i\)</span>.</p>
<p>In comparison to the regular ConvNet focusing on best layer architecture <span class="math inline">\(\mathcal{F_i}\)</span> search, model scaling centers on the expansion of the network length <span class="math inline">\(\left( L_i\right)\)</span>, width <span class="math inline">\(\left( C_i \right)\)</span>, and/or resolution <span class="math inline">\(\left( H_i, W_i\right)\)</span> without changing <span class="math inline">\(\mathcal{F_i}\)</span> that was predefined in the baseline network. Although model scaling simplifies the design problem of the new resource constraints through the fixing <span class="math inline">\(\mathcal{F_i}\)</span>, different <span class="math inline">\(\left( L_i, H_i, W_i, C_i \right)\)</span> for each layer, the large designe space remains to be explored. To further reduce design space, all layers are restricted to be scaled uniformly with a constant ratio. In this case, the goal is to maximize the model’s accuracy for any given resource constraints, which can be presented as an optimization problem:</p>
<p><span class="math display">\[\begin{equation}
\tag{5}
\max_{d,w,r}  Accuracy \left( \mathcal{N}\left( d,w,r \right) \right) \\
s.t.\mathcal{N}\left( d,w,r \right)=\bigodot_{I=1...s}\hat{\mathcal{F}}{i}^{d\cdot \hat{L{i}}}\left( X_{\left\langle r\cdot \hat{H_i},r\cdot \hat{W_i},w\cdot \hat{C_i}\right\rangle} \right) \\
Memory\left( \mathcal{N} \right)≤ targetMemory \\
FLOPS\left( \mathcal{N} \right) ≤ targetFlops \\
\end{equation}\]</span>
where w,d,r are coefficients for scaling network width, depth, and resolution; <span class="math inline">\(\left(\widehat{\mathcal{F}}_i, \widehat{L}_i, \widehat{H}_i, \widehat{W}_i, \widehat{C}_i \right)\)</span> are predefined parameters in baseline network.</p>
</div>
<div id="scaling-dimensions" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.5.1.2</span> Scaling Dimensions<a href="c01-00-intro-modalities.html#scaling-dimensions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The main difficulty of this optimization problem is that the optimal <em>d, w, r</em> depend on each other, and the values change under different resource constraints. Due to this difficulty, conventional methods mostly scale ConvNets in one of these dimensions:</p>
<p><strong>Depth (<span class="math inline">\(d\)</span>):</strong> One of the most significant networks previously described as ResNet. As it was described, the problem of ResNets is that accuracy gain of a very deep network diminishes. For example, ResNet-1000 has similar accuracy to ResNet-101 even though it has many more layers.</p>
<p><strong>Width (<span class="math inline">\(w\)</span>):</strong> Scaling network width is commonly used for small-size models. However, wide but shallow networks tend to have difficulty grasping higher-level features.</p>
<p><strong>Resolution (<span class="math inline">\(r\)</span>):</strong> Starting from 224x224 in early ConvNets, modern ConvNets tend to use 299x299 or 331x331 for better accuracy. GPipe <span class="citation">(<span class="citeproc-not-found" data-reference-id="gpipe"><strong>???</strong></span>)</span> recently achieved state-of-the-art ImageNet accuracy with 480x480 Resolution. Higher resolutions, such as 600x600, are also widely used in object detection ConvNets.</p>
<p>The above analyses lead to the first observation:</p>
<p><strong>Observation 1:</strong> Scaling up any network width, depth, or resolution dimension improves accuracy, without gain diminishes for bigger models.</p>
</div>
<div id="compound-scaling" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.5.1.3</span> Compound Scaling<a href="c01-00-intro-modalities.html#compound-scaling" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Firstly, it was observed that different scaling dimensions are not independent because higher resolution images require increased network depth. The larger receptive fields can help capture similar features that include more pixels in bigger images. Similarly, network width should be increased when the resolution is higher to capture more fine-grained patterns. The intuition suggests that different scaling dimensions should be coordinated and balanced rather than conventional scaling in single dimensions.
To confirm this thought, results of networks width <span class="math inline">\(w\)</span> without changing depth (<span class="math inline">\(d\)</span>=1.0) and resolution (<span class="math inline">\(r\)</span>=1.0) were compared with deeper (<span class="math inline">\(d\)</span>=2.0) and higher resolution (<span class="math inline">\(r\)</span>=2.0) networks. This showed that width scaling achieves much better accuracy under the same FLOPS cost. These results lead to the second observation:</p>
<p><strong>Observation 2:</strong> To achieve better accuracy and efficiency, balancing all network width, depth, and resolution dimensions during ConvNet scaling is critical. Earlier researches have tried to arbitrarily balance network width and depth, but they all require tedious manual tuning.</p>
<p>A new <strong>compound scaling method</strong>, which uses a compound coefficient <span class="math inline">\(\varphi\)</span> to uniformly scale network width, depth, and resolution in a principled way, was proposed.</p>
<p><span class="math display">\[\begin{equation}
\tag{6}
depth: \mathcal{d}=\alpha^{\varphi} \\
width: \mathcal{w}=\beta^{\varphi}\\
resolution: \mathcal{r}=\gamma^{\varphi}\\
s.t.  \alpha\cdot \beta^{2}\cdot \gamma^{2}\approx 2\\
 \alpha \ge 1, \beta \ge 1, \gamma \ge 1\\
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\alpha, \beta, \gamma\)</span> are constants that can be determined by a small grid search, <span class="math inline">\(\varphi\)</span> is a user-specified coefficient that controls how many more resources are available for model scaling, while <span class="math inline">\(\alpha, \beta, \gamma\)</span> specify how to assign these extra resources to network width, depth, and resolution, respectively. Notably, the FLOPS of a regular convolution op is proportional to <span class="math inline">\(d, w^{2}, r^{2}\)</span>, i.e., doubling network depth will double FLOPS, but doubling network width or Resolution will increase FLOPS by four times. Scaling a ConvNet with previous equation will approximately increase total FLOPS by <span class="math inline">\(\left( \alpha\cdot \beta^{2}\cdot \gamma^{2} \right)^{\varphi}\)</span>. In this paper, <span class="math inline">\(\alpha\cdot \beta^{2}\cdot \gamma^{2}\approx 2\)</span> is constrain such that for any new <span class="math inline">\(\varphi\)</span>, the total FLOPS will approximately increase by <span class="math inline">\(2\varphi\)</span></p>
</div>
</div>
<div id="efficientnet-architecture" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.5.2</span> EfficientNet Architecture<a href="c01-00-intro-modalities.html#efficientnet-architecture" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A good baseline network is essential because model scaling does not affect its layer operators <span class="math inline">\(F*[i]\)</span>. Therefore this method is also estimated on ConvNets.
A new mobile-sized baseline called EfficientNet was developed to show the effectiveness of the new scaling method. Metrics that were used to estimate the efficacy are accuracy and FLOPS.
The baseline efficient network that was created is named EfficientNet-B0. Afterward, this compound scaling method is applied in two steps:</p>
<ul>
<li><p><strong>STEP 1</strong>: By fixing <span class="math inline">\(\varphi = 1\)</span> and, assuming twice more resources available, a small grid search of $, , $ based Eq. (6) showed that the best values for EfficientNet-B0 are <span class="math inline">\(\alpha = 1.2, \beta = 1.1, \gamma=1.15\)</span>, under constraint of <span class="math inline">\(\alpha·\beta^2·\gamma^2 ≈2\)</span>.</p></li>
<li><p><strong>STEP 2</strong>: Afterward fix <span class="math inline">\(\alpha,\beta,\gamma\)</span> as constants and scale up the baseline network with different <span class="math inline">\(\varphi\)</span> using Eq.(6) to construct EfficientNet-B1 to B7</p></li>
</ul>
<table>
<thead>
<tr class="header">
<th align="center">Name</th>
<th align="center">Number of parameters</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">EfficientNet-B0</td>
<td align="center">5.3M parameters</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B1</td>
<td align="center">7.8M parameters</td>
</tr>
<tr class="odd">
<td align="center">EfficientNet-B2</td>
<td align="center">9.2M parameters</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B3</td>
<td align="center">12M parameters</td>
</tr>
<tr class="odd">
<td align="center">EfficientNet-B4</td>
<td align="center">19M parameters</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B5</td>
<td align="center">30M parameters</td>
</tr>
<tr class="odd">
<td align="center">EfficientNet-B6</td>
<td align="center">43M parameters</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B7</td>
<td align="center">66M parameters</td>
</tr>
</tbody>
</table>
<p>Indeed, even better performance is achievable by searching for <span class="math inline">\(\alpha,\beta,\gamma\)</span> directly around a large model, but the search cost becomes prohibitively more expensive on larger models. This method searches once on a small baseline network, then scales the coefficient for all other models.</p>
</div>
</div>
<div id="results-and-comparison-of-the-networks" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.6</span> Results and comparison of the networks<a href="c01-00-intro-modalities.html#results-and-comparison-of-the-networks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To demonstrate the performance of both networks, ResNet and EfficientNets were trained and evaluated on the benchmark ImageNet 2012 classification dataset consisting of 1000 classes.
Since deeper scaling should provide better results in the case of ResNet, it was trained with increased depth each time. First meaningful results were obtained in ResNet-34, which peformed better than plain-34 baseline by 3,5% when top-1 acc. is compared. They also compared tree versions of ResNet: (A) zero-padding shortcuts, for increasing dimensions, and all shortcuts are parameter-free (B) projection shortcuts, for increasing dimensions, and other shortcuts are identity; and (C) all shortcuts are projections. Each version improved both top-1 and top-5 accuracy.
Afterward, the depth of the network was increased and ResNet-50, ResNet-101, and ResNet-152 were created. Each increase in depth leads to higher accuracy. In deeper models, the trade-off between accuracy increase and deeper model is not worth describing. All results are shown in the table below.</p>
<table>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">top-1 acc.</th>
<th align="center">top-5 acc.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">VGG-16</td>
<td align="center">71.93</td>
<td align="center">90.67</td>
</tr>
<tr class="even">
<td align="center">GoogLeNet</td>
<td align="center">-</td>
<td align="center">90.85</td>
</tr>
<tr class="odd">
<td align="center">plain-34</td>
<td align="center">71.46</td>
<td align="center">89.98</td>
</tr>
<tr class="even">
<td align="center">ResNet-34 A</td>
<td align="center">74.97</td>
<td align="center">92.24</td>
</tr>
<tr class="odd">
<td align="center">ResNet-34 B</td>
<td align="center">75.48</td>
<td align="center">92.54</td>
</tr>
<tr class="even">
<td align="center">ResNet-34 C</td>
<td align="center">75.81</td>
<td align="center">92.6</td>
</tr>
<tr class="odd">
<td align="center">ResNet-50</td>
<td align="center">77.15</td>
<td align="center">93.29</td>
</tr>
<tr class="even">
<td align="center">ResNet-101</td>
<td align="center">78.25</td>
<td align="center">93.95</td>
</tr>
<tr class="odd">
<td align="center">ResNet-152</td>
<td align="center"><strong>78.57</strong></td>
<td align="center"><strong>94.29</strong></td>
</tr>
</tbody>
</table>
<p>In the case of EfficientNets, the results achieved by the previous state-of-the-art networks on the same ImageNet dataset were aimed to improve. Among all state-of-the-art networks, EfficientNets were compared with ResNets-50 and ResNet-152. They compared the results of networks derivated by changing scaling parameters EfficientNet-B0 to EfficientNet-B7. The results of each network were better than the previous one. Also, they have shown that EfficientNet-B0 outperforms ResNet-50 and that EfficientNet-B1 outperforms ResNet-152. This means that scaling through all three dimensions can provide better results than scaling through just one dimension. The drawback of this approach is the computational power, which makes it less popular than previous methods. Again, all results are shown in the table below.</p>
<table>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">top-1 acc.</th>
<th align="center">top-5 acc.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">EfficientNet-B0 <br> ResNet-50</td>
<td align="center">77.1 <br> 76</td>
<td align="center">93.3 <br> 93</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B1 <br> ResNet-152</td>
<td align="center">79.1 <br> 77.8</td>
<td align="center">94.4 <br> 93.8</td>
</tr>
<tr class="odd">
<td align="center">EfficientNet-B2</td>
<td align="center">80.1</td>
<td align="center">94.9</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B3 <br> ResNeXt-101</td>
<td align="center">81.6 <br> 80.9</td>
<td align="center">95.7 <br> 95.6</td>
</tr>
<tr class="odd">
<td align="center">EfficientNet-B4</td>
<td align="center">82.9</td>
<td align="center">96.4</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B5</td>
<td align="center">83.6</td>
<td align="center">96.7</td>
</tr>
<tr class="odd">
<td align="center">EfficientNet-B6</td>
<td align="center">84</td>
<td align="center">96.8</td>
</tr>
<tr class="even">
<td align="center">EfficientNet-B7 <br> GPipe</td>
<td align="center"><strong>84.3</strong> <br> 84.3</td>
<td align="center"><strong>97</strong> <br> 97</td>
</tr>
</tbody>
</table>
</div>
<div id="contrastive-learning" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.7</span> Contrastive learning<a href="c01-00-intro-modalities.html#contrastive-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In recent years the problem of classification of the unlabeled dataset is becoming more widespread. More unlabeled datasets are being created in fields like medicine, the automotive industry, military, etc., requiring human labeling. Since the process is expensive and time-consuming, researchers assumed, it could be automated with contrastive learning frameworks. One of the first and most known contrastive learning frameworks is SimCLR <span class="citation">Chen et al. (<a href="#ref-SimCLR" role="doc-biblioref">2020</a>)</span>. The advantage of this framework is its simplicity, yet it achieves high accuracy on classification tasks. The main idea is to have two copies of the image, which are then used to train two networks and later compared. The problem with this framework is that it doubles the size of the dataset and reaches among all images, which can be computationally unfeasible in large datasets. Bootstrap Your Own Latent <span class="citation">Grill, Strub, Altché, Tallec, Richemond, et al. (<a href="#ref-BYOL" role="doc-biblioref">2020</a><a href="#ref-BYOL" role="doc-biblioref">b</a>)</span> was introduced to avoid making double-sized datasets. The idea was to bootstrap images’ representations, avoiding unnecessary image comparison. These two frameworks will be described in this work.
Further improvements in the choice of creating two views of images and comparison techniques were presented in different frameworks such as Nearest-Neighbor Contrastive Learning (NNCLR) <span class="citation">(<span class="citeproc-not-found" data-reference-id="NNCLR"><strong>???</strong></span>)</span>, Open World Object Detection (ORE) <span class="citation">(<span class="citeproc-not-found" data-reference-id="ORE"><strong>???</strong></span>)</span>, Swapping Assignments between multiple Views (SwAV) <span class="citation">(<span class="citeproc-not-found" data-reference-id="SwAV"><strong>???</strong></span>)</span>, and many more.
This field is a constant research topic, and new, improved frameworks are being created to help researchers solve other tasks requiring labeled datasets.</p>
</div>
<div id="a-simple-framework-for-contrastive-learning-of-visual-representations" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.8</span> A Simple Framework for Contrastive Learning of Visual Representations<a href="c01-00-intro-modalities.html#a-simple-framework-for-contrastive-learning-of-visual-representations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Chen T. et al. intended to analyze and describe a better approach to learning visual representations without human supervision. They have introduced a simple framework for contrastive learning of visual representations and called it SimCLR <span class="citation">Chen et al. (<a href="#ref-SimCLR" role="doc-biblioref">2020</a>)</span>. As they claim, SimCLR outperforms previous work, is more straightforward, and does not require a memory bank.</p>
<p>Intending to understand what qualifies good contrastive representation learning, the significant components of the framework were studied and resulted in:</p>
<ul>
<li>A contrastive prediction task requires combining multiple data augmentation operations, which results in effective representations. Unsupervised contrastive learning benefits from more significant data augmentation.</li>
<li>The quality of the learned representations can be substantially improved by introducing a learnable nonlinear transformation between the representation and the contrastive loss.</li>
<li>Representation learning with contrastive cross-entropy loss can be improved by normalizing embeddings and adjusting the temperature parameter appropriately.</li>
<li>Unlike its supervised counterpart, contrastive learning benefits from larger batch sizes and extended training periods. Contrastive learning also benefits from deeper and broader networks, just as supervised learning does.</li>
</ul>
<div id="the-contrastive-learning-framework" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.8.1</span> The Contrastive Learning Framework<a href="c01-00-intro-modalities.html#the-contrastive-learning-framework" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Like previous contrastive learning algorithms, in the SimCLR, a contrastive loss is used to learn representations by maximizing agreement between various augmented views of the same data example. This framework contains four significant components, which are shown in Fig. 4:</p>
<ol style="list-style-type: decimal">
<li><p>A stochastic <em>data augmentation</em> module</p></li>
<li><p>A neural network <em>base encoder</em></p></li>
<li><p>A small neural network <em>projection head</em></p></li>
<li><p>A <em>contrastive loss function</em></p></li>
</ol>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure04"></span>
<img src="figures/01-chapter1/SimCLR.png" alt="Figure 4. A simple framework for contrastive learning of visual representations" width="30%" />
<p class="caption">
FIGURE 2.4: Figure 4. A simple framework for contrastive learning of visual representations
</p>
</div>
<div id="stochastic-data-augmentation-module" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.8.1.1</span> Stochastic data augmentation module<a href="c01-00-intro-modalities.html#stochastic-data-augmentation-module" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>First, the minibatch of N examples is sampled randomly, and the contrastive prediction task is defined on pairs of augmented examples, resulting in 2N data points. A memory bank was not used to train the model, instead, the training batch size varied from 256 to 8192.
Any given data example randomly returns two correlated views of the same example, denoted <span class="math inline">\(\tilde{x}_{i}\)</span> and <span class="math inline">\(\tilde{x}_{j}\)</span>, which is known as a <strong>positive pair</strong>. <strong>Negative pairs</strong> are all other <span class="math inline">\(2(N-1)\)</span> pairs except the positive pair. In one view, some data augmentation techniques are applied.
Data augmentation is widely embraced in supervised and unsupervised representation learning. Unfortunately, it has not been used to define the contrastive prediction task, which is mainly determined by changing the architecture. It was shown that choosing different data augmentation techniques can reduce the complexity of previous contrastive learning frameworks.
There are many data augmentation operations, the focus was on the most common ones, which are:</p>
<ul>
<li><strong>spatial geometric transformation</strong>: cropping and resizing (with horizontal flipping), rotation and cutout,</li>
<li><strong>appearance transformation</strong>: color distortion (including color dropping), brightness, contrast, saturation, Gaussian blur, and Sobel filtering.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure05"></span>
<img src="figures/01-chapter1/augmentation.png" alt="Figure 5. Augmentation texhniques" width="80%" />
<p class="caption">
FIGURE 2.5: Figure 5. Augmentation texhniques
</p>
</div>
<p>Due to the image sizes in the ImageNet dataset, all images were always randomly cropped and resized to the same resolution. Later on, other targeted data augmentation transformations were applied to one branch, remaining the one as original i.e. <span class="math inline">\(t\left( x_{i}\right)= x_i\)</span>.
Applying just individual transformation is insufficient for the model to learn good representations. The model’s performance improves after composing augmentations, although the contrastive prediction task becomes more complex. The composition of augmentations that stood out were random cropping and random color distortion.</p>
<p>It was also observed that stronger color augmentation significantly improves the linear evaluation of unsupervised learned models. Stronger color augmentations do not enhance the performance of supervised models when trained with the same augmentations. Based on the experiments, unsupervised contrastive learning benefits from stronger color data augmentation than supervised learning.</p>
</div>
<div id="neural-network-base-encoder" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.8.1.2</span> Neural network base encoder<a href="c01-00-intro-modalities.html#neural-network-base-encoder" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Neural network based encoder <span class="math inline">\(f\left( \cdot \right)\)</span> extracts representation vectors from augmented data examples. This framework does not restrict a choice of the network architecture, although for simplicity the commonly used ResNet was picked and obtained <span class="math inline">\(h_i=f\left( \tilde{x}_{i} \right)=ResNet\left(\tilde{x}_{i}\right)\)</span> where <span class="math inline">\(\textbf{h}_i\in \mathbb{R}^{d}\)</span> is the output after the average pooling layer. Although increasing depth and width improves performance, the ResNet-50 was chosen. Furthermore, when the model size increases, the gap between supervised and unsupervised learning shrinks, suggesting that bigger models benefit from unsupervised learning more.</p>
</div>
<div id="small-neural-network-projection-head" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.8.1.3</span> Small neural network projection head<a href="c01-00-intro-modalities.html#small-neural-network-projection-head" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>A small neural network projection heads <span class="math inline">\(g\left( \cdot \right)\)</span> that maps representations to the space where contrastive loss is applied. The importance of including a projection head, i.e., <span class="math inline">\(g\left( h \right)\)</span> was evaluated. They have considered three different architectures for the head:</p>
<ol style="list-style-type: decimal">
<li>identity mapping,</li>
<li>linear projection,</li>
<li>the default nonlinear projection with one additional hidden layer and ReLU activation function.</li>
</ol>
<p>The results showed that a nonlinear projection head is better than a linear projection and much better than no projection. It improves the representation quality of the layer before it. They have used a MLP with one hidden layer to obtain <span class="math inline">\(z_i = g\left( \textbf{h}_i \right) = W^{\left( 2\right)}\sigma \left( W^{\left( 1\right)} \textbf{h}_i\right)\)</span> where <span class="math inline">\(\sigma\)</span> is a ReLU non-linearity.</p>
<p>This step is performed because defining the contrastive loss on <span class="math inline">\(z_i\)</span> instead of on <span class="math inline">\(\textbf{h}_i\)</span> would not lead to loss of information caused by contrastive loss. Especially, <span class="math inline">\(z=g\left( h \right)\)</span> is trained to be invariant to data transformations. As a result, <span class="math inline">\(g\)</span> can remove information useful for a downstream task, such as object color or orientation. Using the nonlinear transformation <span class="math inline">\(g\left( * \right)\)</span>, <span class="math inline">\(h\)</span> can maintain and form more information.</p>
</div>
<div id="contrastive-loss-function" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.8.1.4</span> Contrastive loss function<a href="c01-00-intro-modalities.html#contrastive-loss-function" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Given a set <span class="math inline">\(\left\{ \tilde{x}_{ik} \right\}\)</span> including a positive pair of examples <span class="math inline">\(\tilde{x}_{i}\)</span> and <span class="math inline">\(\tilde{x}_{j}\)</span>, the contrastive prediction task aims to identify <span class="math inline">\(\tilde{x}_{i}\)</span> in <span class="math inline">\(\left\{ \tilde{x}_{i} \right\}_{k\neq i}\)</span> for a given <span class="math inline">\(\tilde{x}_{i}\)</span>. In the case of positive examples, the loss function is as follows <span class="math inline">\(\left( i, j\right)\)</span> is defined as</p>
<p><span class="math display">\[\begin{equation}
\tag{7}
\mathcal{l}_{i,j} = −\log\frac{exp\left( \frac{sim(z_i,z_j)}{\tau} \right)}{\sum_{k=1}^{2N}\mathbb{I_{\left[ k\neq i \right]}}exp\left( \frac{sim(z_i,z_k)}{\tau} \right)}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\mathbb{I_{\left[ k\neq i \right]}}\in\left\{ 0,1 \right\}\)</span> is an indicator function, <span class="math inline">\(\tau\)</span> denotes a temperature parameter and <span class="math inline">\(sim\left(\textbf{u,v} \right)= \frac{\textbf{u}^T\textbf{v}}{\left\| \textbf{u}\right\|\left\| \textbf{v} \right\|}\)</span> is a dot product between <span class="math inline">\(\mathcal{l}_2\)</span> normalized <span class="math inline">\(\textbf{u},\textbf{v}\)</span>.</p>
<p>The final loss is calculated across all positive pairs, both <span class="math inline">\(\left( i,j \right)\)</span> and <span class="math inline">\(\left( j,i \right)\)</span>, in a mini-batch. It was named <strong>NT-Xent</strong>, the normalized temperature-scaled cross-entropy loss.</p>
<p>The NT-Xent loss was compared against other commonly used contrastive loss functions, such as logistic loss and margin loss. Gradient analysis shows that <span class="math inline">\(l_2\)</span> normalization, cosine similarity, and temperature together effectively weight different examples, and a suitable temperature can make the model learn from hard negatives. The advantage of NT-Xent is that it weights the negatives by their relative hardness. Without normalization and proper temperature scaling, performance is significantly worse. Also, the contrastive task accuracy is higher, but the resulting representation is worse under linear evaluation.</p>
</div>
</div>
</div>
<div id="bootstrap-your-own-latent" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.9</span> Bootstrap Your Own Latent<a href="c01-00-intro-modalities.html#bootstrap-your-own-latent" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The fundamental idea of contrastive learning is to create pairs of images on which the framework would be trained. Creating negative pairs relies on large batch sizes, memory banks, or customized mining strategies which can be challenging in larger datasets. Grill J.B. at al. wanted to create a new approach that would achieve better performance than other contrastive methods without using negative pairs. A solution they have introduced is a method called Bootstrap Your Own Latent (BYOL) <span class="citation">Grill, Strub, Altché, Tallec, Richemond, et al. (<a href="#ref-BYOL" role="doc-biblioref">2020</a><a href="#ref-BYOL" role="doc-biblioref">b</a>)</span>. The idea was to bootstrap representations of images. As a result, BYOL is more robust to the choice of image augmentations.
Furthermore, BYOL has two neural networks, called online and target networks, which interact and learn from each other. Using an augmented view of an image, BYOL trains its online network to predict the target network’s representation of another augmented view. This approach achieved state-of-the-art results when trained on the ImageNet dataset under the linear evaluation protocol. Additionally, compared to SimCLR, a strong contrastive baseline, BYOL suffers from much less performance drop when only random crops are used to augment images.</p>
<div id="description-of-method" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.9.1</span> Description of method<a href="c01-00-intro-modalities.html#description-of-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>BYOL aims to learn a representation of <span class="math inline">\(y_\theta\)</span>. It uses two neural networks: <em>online</em> and <em>the target network</em> to achieve that. The <em>online network</em> is determined by a set of weights <span class="math inline">\(\theta\)</span> and consists of:</p>
<ul>
<li>an encoder <span class="math inline">\(f_\theta\)</span>,</li>
<li>a projector <span class="math inline">\(g_\theta\)</span>,</li>
<li>a predictor <span class="math inline">\(q_\theta\)</span>.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure06"></span>
<img src="figures/01-chapter1/BYOL.png" alt="Figure 6. Bootstrap Your Own Latent" width="80%" />
<p class="caption">
FIGURE 2.6: Figure 6. Bootstrap Your Own Latent
</p>
</div>
<p>The <em>target network</em> has the same architecture as the online network but uses different weights <span class="math inline">\(\xi\)</span>. It provides the regression targets to train the online network, and its parameters <span class="math inline">\(\xi\)</span> are an exponential moving average of the online parameters <span class="math inline">\(\theta\)</span>. Precisely, given a target decay rate <span class="math inline">\(\tau \in[0,1]\)</span>, after each training step, the following update
<span class="math display">\[\begin{equation}
\tag{8}
\xi \leftarrow \tau \xi+(1-\tau) \theta
\end{equation}\]</span>
is performed.
Firstly, image is sampled uniformly from <span class="math inline">\(\mathcal{D}\)</span>, from which two distributions of image augmentations <span class="math inline">\(\mathcal{T}\)</span> and <span class="math inline">\(\mathcal{T}^{\prime}\)</span> are created. BYOL applies respectively two image augmentations <span class="math inline">\(t \sim \mathcal{T}\)</span> and <span class="math inline">\(t^{\prime} \sim \mathcal{T}^{\prime}\)</span>, creating two augmented views <span class="math inline">\(v \triangleq t(x)\)</span> and <span class="math inline">\(v^{\prime} \triangleq t^{\prime}(x)\)</span>. First augmented view <span class="math inline">\(v\)</span> is used at online network, resulting the output <span class="math inline">\(y_{\theta} \triangleq f_{\theta}(v)\)</span> and afterwards projection <span class="math inline">\(z_{\theta} \triangleq g_{\theta}(y)\)</span>. Similarly, from second augmented view <span class="math inline">\(v^{\prime}\)</span> the target network outputs <span class="math inline">\(y_{\xi}^{\prime} \triangleq f_{\xi}(v^{\prime})\)</span> and the target projection <span class="math inline">\(z_{\xi}^{\prime} \triangleq g_{\xi}(y^{\prime})\)</span>. Later on output a prediction of <span class="math inline">\(q_{\theta}\left(z_{\theta}\right)\)</span> of <span class="math inline">\(z_{\xi}^{\prime}\)</span> and <span class="math inline">\(\ell_{2}\)</span>-normalize both <span class="math inline">\(q_{\theta}\left(z_{\theta}\right)\)</span> and <span class="math inline">\(z_{\xi}^{\prime}\)</span> to</p>
<p><span class="math display">\[\begin{equation}
\tag{9}
\overline{q_{\theta}}\left(z_{\theta}\right) \triangleq q_{\theta}\left(z_{\theta}\right) /\left\|q_{\theta}\left(z_{\theta}\right)\right\|_{2} \quad \textrm{and} \quad
\bar{z}_{\xi}^{\prime} \triangleq z_{\xi}^{\prime} /\left\|z_{\xi}^{\prime}\right\|_{2}
\end{equation}\]</span></p>
<p>Predictor only applies to the online pipeline, making the architecture asymmetric between the online and target pipeline. Lastly, the following mean squared error between the normalized predictions and target projections is defined.</p>
<p><span class="math display">\[\begin{equation}
\tag{10}
\mathcal{L}_{\theta, \xi} \triangleq\left\|\overline{q_{\theta}}\left(z_{\theta}\right)-\bar{z}_{\xi}^{\prime}\right\|_{2}^{2}=2-2 \cdot \frac{\left\langle q_{\theta}\left(z_{\theta}\right), z_{\xi}^{\prime}\right\rangle}{\left\|q_{\theta}\left(z_{\theta}\right)\right\|_{2} \cdot\left\|z_{\xi}^{\prime}\right\|_{2}}
\end{equation}\]</span>
Loss is symmetrized <span class="math inline">\(\mathcal{L}_{\theta, \xi}\)</span> by using <span class="math inline">\(v^{\prime}\)</span> for the online network and <span class="math inline">\(v\)</span> for the target network separately to calculate <span class="math inline">\(\widetilde{\mathcal{L}}_{\theta, \xi}\)</span>. At each training step, a stochastic optimization step is applied to minimize <span class="math inline">\(\mathcal{L}_{\theta, \xi}^{\mathrm{BYOL}}=\mathcal{L}_{\theta, \xi}+\widetilde{\mathcal{L}}_{\theta, \xi}\)</span> with respect to <span class="math inline">\(\theta\)</span> only, but not <span class="math inline">\(\xi\)</span>. BYOL’s dynamics are summarized as</p>
<p><span class="math display">\[\begin{equation}
\tag{11}
\theta \leftarrow \operatorname{optimizer}\left(\theta, \nabla_{\theta} \mathcal{L}_{\theta, \xi}^{\mathrm{BYOL}}, \eta\right)
\end{equation}\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is a learning rate.
At the end of the training, only the encoder <span class="math inline">\(f_{\theta}\)</span>.</p>
</div>
</div>
<div id="comparison-of-contrastive-learning-frameworks" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.10</span> Comparison of contrastive learning frameworks<a href="c01-00-intro-modalities.html#comparison-of-contrastive-learning-frameworks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Of all frameworks, SimCLR is the most popular due to its simplicity. The ResNet-50 in 3 different hidden layer widths (width multipliers of 1×, 2×, and 4×) were used and trained for 1000 epochs each. The accuracy of these frameworks on the ImageNet dataset with few labels improved when the width of ResNet-50 increased. For SimCLR with ResNet-50 top-1 accuracy is 69.3 and top-5 accuracy is 89, while for ResNet-50(4x) top-1 accuracy is 85.8 and top-5 accuracy is 92.6. These results are comparable with supervised methods.
BYOL framework was built to improve the results of SimCLR. It was also stated that the accuracy for baseline ResNet-50 is 74.3 and 91.6 for top-1 accuracy and top-5 accuracy. When using ResNet-50(4x), accuracies increase to 78.6 and 94.2 for top-1 and top-5, respectively. More information about performance can be found in table below</p>
<table>
<thead>
<tr class="header">
<th align="center">Model</th>
<th align="center">Architecture</th>
<th align="center">Param (M)</th>
<th align="center">top-1 acc.</th>
<th align="center">top-5 acc.</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">SimCLR</td>
<td align="center">ResNet-50</td>
<td align="center">24</td>
<td align="center">69.3</td>
<td align="center">89.0</td>
</tr>
<tr class="even">
<td align="center">SimCLR</td>
<td align="center">ResNet-50 (2x)</td>
<td align="center">94</td>
<td align="center">74.2</td>
<td align="center">93.0</td>
</tr>
<tr class="odd">
<td align="center">SimCLR</td>
<td align="center">ResNet-50 (4x)</td>
<td align="center">375</td>
<td align="center">76.5</td>
<td align="center">93.2</td>
</tr>
<tr class="even">
<td align="center">BYOL</td>
<td align="center">ResNet-50</td>
<td align="center">24</td>
<td align="center">74.3</td>
<td align="center">91.6</td>
</tr>
<tr class="odd">
<td align="center">BYOL</td>
<td align="center">ResNet-50 (x2)</td>
<td align="center">94</td>
<td align="center">77.4</td>
<td align="center">93.6</td>
</tr>
<tr class="even">
<td align="center">BYOL</td>
<td align="center">ResNet-50 (x4)</td>
<td align="center">375</td>
<td align="center">78.6</td>
<td align="center">94.2</td>
</tr>
<tr class="odd">
<td align="center">BYOL</td>
<td align="center">ResNet-200 (x2)</td>
<td align="center">250</td>
<td align="center">79.6</td>
<td align="center">94.8</td>
</tr>
</tbody>
</table>
</div>
<div id="transformers-in-computer-vision" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.11</span> Transformers in Computer Vision<a href="c01-00-intro-modalities.html#transformers-in-computer-vision" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Since the first appearance of the Transformers architecture in 2017 <span class="citation">(<span class="citeproc-not-found" data-reference-id="TRANSFORMERS_NLP"><strong>???</strong></span>)</span>, it has become an irreplaceable part of all-natural language processing (NLP) models. The main advantage of Transformers is that they can be trained on a large text corpus and then fine-tuned on a smaller task-specific dataset, and this enabled model training of unspecified size with more than 100B parameters.</p>
<p>However, computer vision still relied on convolutional architectures. With datasets constantly growing and the diversity of the fields where computer vision tasks could be applied, researchers wanted to implement Transformers architecture in the CV field. Some works tried combining CNN-like architectures with self-attention <span class="citation">(<span class="citeproc-not-found" data-reference-id="wang"><strong>???</strong></span>)</span>, and others attempted to replace convolutions entirely <span class="citation">(<span class="citeproc-not-found" data-reference-id="selfa"><strong>???</strong></span>)</span>. Due to specialized attention patterns, the problem was that they have not yet been scaled effectively on modern hardware accelerators. Therefore, in large-scale image recognition, classic ResNet-like architectures are still state-of-the-art.</p>
<p>In 2021 Google research, Brain Team published the paper “An image is worth 16x16 words,” where they introduced new Transformers-based architecture for CV called Vision Transformers (ViT) <span class="citation">(<span class="citeproc-not-found" data-reference-id="vit"><strong>???</strong></span>)</span>. Based on the success of Transformer in NLP scaling, they aimed to apply standard Transformer directly to images, changing it as little as possible. The image is split into patches, and linear embeddings of these patches are provided as inputs to the Transformer.
These patches are the same as tokens (eg. words) in NLP. The model is trained on image classification in a supervised fashion.</p>
<div id="vision-transformers" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.2.11.1</span> Vision Transformers<a href="c01-00-intro-modalities.html#vision-transformers" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Brain Team wanted to create simple but universally scalable architecture to follow the original Transformers architecture.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch01-figure7"></span>
<img src="figures/01-chapter1/ViT.png" alt="Figure 7. Vision Transformer" width="90%" />
<p class="caption">
FIGURE 2.7: Figure 7. Vision Transformer
</p>
</div>
<div id="method" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.11.1.1</span> Method<a href="c01-00-intro-modalities.html#method" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Compared to NLP, where the input of the Transformer is a 1D sequence of token embeddings, images are 2D objects. Firstly, images needed to be represented differently to imitate original architectures as closely as possible. For that reason image <span class="math inline">\(x\in \mathbb{R}^{ H \times W \times C}\)</span> is reshaped into a sequence of flattened 2D patches <span class="math inline">\(x_p\in \mathbb{R}^{ N \times \left( P^2 \cdot C \right)}\)</span>, where <span class="math inline">\(\left(H,W\right)\)</span> is the resolution of the original image, <span class="math inline">\(C\)</span> is the number of channels,<span class="math inline">\(\left(P,P\right)\)</span> is the resolution of each image patch, and <span class="math inline">\(N =HW/P^2\)</span> is the resulting number of patches, also the Transformer’s effective input sequence length. The Transformer input is a fixed through all layers vector size <span class="math inline">\(D\)</span>. The first step is to flatten the patches, usually 16x16, and map them to <span class="math inline">\(D\)</span> dimensions with a trainable linear projection, creating patch embeddings.</p>
<p><span class="math display">\[\begin{equation}
\tag{12}
\mathbf{z}_{0} =\left[\mathbf{x}_{\text {class }} ; \mathbf{x}_{p}^{1} \mathbf{E} ; \mathbf{x}_{p}^{2} \mathbf{E} ; \cdots ; \mathbf{x}_{p}^{N} \mathbf{E}\right]+\mathbf{E}_{p o s}, \mathbf{E} \in \mathbb{R}^{\left(P^{2} \cdot C\right) \times D}, \mathbf{E}_{p o s} \in \mathbb{R}^{(N+1) \times D}
\end{equation}\]</span></p>
<p>To this sequence of “patch embeddings,” a prefix learnable [class] token, like in BERT, is usually added. This token <span class="math inline">\(\mathbf{z}_{0}^{0} = \mathbf{x}_{class}\)</span> tells the model to classify the image and increases dimension of vector z. Also, the state of this token at the output of the Transformer encoder <span class="math inline">\(\left(\mathbf{z}_{L}^{0}\right)\)</span>, on which Layernorm is applied, serves as the image representation <span class="math inline">\(y\)</span>.</p>
<p><span class="math display">\[\begin{equation}
\tag{13}
\mathbf{y} =\operatorname{LN}\left(\mathbf{z}_{L}^{0}\right)
\end{equation}\]</span></p>
<p>Also, it is the only one to which the classification head is attached to during pre-training and fine-tuning. Classification head during pre-training is compiled of MLP with one hidden layer and a single linear layer at a fine-tuning time. Position embedding,a standard learnable 1D position embedding, are attached to the patch embeddings, serving as input to the encoder. The standard Transformer encoder consists of alternating layers of multiheaded self-attention and MLP blocks. After each block, a residual connection is applied.</p>
<p><span class="math display">\[\begin{equation}
\tag{14}
\mathbf{z}_{\ell}^{\prime} =\operatorname{MSA}\left(\operatorname{LN}\left(\mathbf{z}_{\ell-1}\right)\right)+\mathbf{z}_{\ell-1},  \ell=1 \ldots L
\end{equation}\]</span></p>
<p><span class="math display">\[\begin{equation}
\tag{15}
\mathbf{z}_{\ell} =\operatorname{MLP}\left(\mathrm{LN}\left(\mathbf{z}_{\ell}^{\prime}\right)\right)+\mathbf{z}_{\ell}^{\prime}, \ell=1 \ldots L
\end{equation}\]</span></p>
<p>Vision Transformer has a significantly lower inductive bias than CNNs in image-specific information. VIT only has local and translationally equivariant MLP layers, while the self-attention layers are global. Two-dimensional neighborhood structure is used sparingly: the image is cut into patches at the beginning, and the position embeddings are resized as needed at the fine-tuning time. Alternatively, the input sequence can consist of a CNN’s feature maps on which patch embedding projection is applied.
Vision Transformers are pre-trained on large datasets, and fine-tuned to (smaller) downstream tasks. For fine-tuning projection head is removed and zero-initialized <span class="math inline">\(D \times K\)</span> feedforward layer is attached, <span class="math inline">\(K\)</span> being the number of downstream classes. It is also beneficial to use higher resolution then in pre-training. Also ViT can handle arbitrary sequence lengths, but the pre-trained position embeddings can become sufficient. It is necessary to point out that resolution adjustment and patch extraction are the only points at which an inductive bias about the 2D structure of the images is manually injected into the Vision Transformers</p>
</div>
<div id="experiments" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.2.11.1.2</span> Experiments<a href="c01-00-intro-modalities.html#experiments" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Similarly to BERT models, multiple versions of the model at various scales were created. They have created Base = “B”, Large = “L”, Huge = “H” versions of ViT, with 12, 24 and 32 layers and 86M, 307M and 632M parameters respectively.</p>
<p>To explore model scalability, the already mentioned dataset ImageNet was used. In addition, ViT was compared against slightly modified ResNet, creating “ResNet(BiT)”. The batch Normalization layer was replaced with Group Normalization and used standardized convolutions. Another network that it was compared to was Noisy Student <span class="citation">(<span class="citeproc-not-found" data-reference-id="noisy"><strong>???</strong></span>)</span>, a large EfficientNet. Experiments showed that ViT Hughe with 14x14 input patch size outperformed both CNN-based networks with an accuracy of 88.5%, whereas ResNet BiT had 87.54% and Noisy Student 88.4%. Also, it is worth mentioning that ViT Large with 16x16 input patch size had 87.76% accuracy on the same dataset.
Another thing worth pointing out is that ViT outperforms CNN-based architectures on all larger datasets yet performs slightly worse than CNN networks on a smaller dataset.</p>
</div>
</div>
</div>
<div id="conclusion" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.2.12</span> Conclusion<a href="c01-00-intro-modalities.html#conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this chapter, authors presented some of the current state-of-the-art approaches in Computer Vision. Nowdays, when technology is advancing each day, creating networks that would imitate human brain is more challenging. Still networks presented in this chapter are highly accurate and creating network which would out-performe them is challenging. Furthermore, it is noticeable that the application of CV is dictating the development of networks and frameworks which help humans with everyday tasks.</p>

</div>
</div>
<div id="c01-03-benchmarks" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.3</span> Resources and Benchmarks for NLP, CV and multimodal tasks<a href="c01-00-intro-modalities.html#c01-03-benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: Christopher Marquardt</em></p>
<p><em>Supervisor: Prof. Dr. Christian Heumann</em></p>
<p>When we see athletes perform in their sports we only see the results of their hard work prior or till to the event. Most of the time they casually talk about their off-season, but everybody knows the results are made in the off-season.</p>
<p>Same goes for the models we will see in the later chapters. We are just interested in the results, but why and how does the model come to these results? It has to learn to some key fundamentals of the modality to achieve these results. But how do they get them to perform in such a way or even better? It’s possible to build better architectures and/or use more and new data to achieve this. New data by hand is easy to get but this new data results in a new problem. New data has to be carefully labeled by humans, which can be very expensive by the amount of data. Models which learn from labeled data use the supervised learning strategy. This learning strategy is a bottleneck for future progress, because of the given reason.</p>
<p>But the need for labeling the data isn’t the only problem. Let’s visit the athlete analogy again. Imagine a professional football player has to participate in a professional ski race. He will not be able to compete with the others, because they are trained only to do ski races. Here see the other problem. Models which use supervised learning have shown to perform very well on the task they are trained to do. This means models which learn on carefully labeled data only perform very well on this specific task, but poor on others. Also it’s not possible to label everything in the world.</p>
<p>So the goal is to generate more generalist models which can perform well on different tasks without the need of huge labeled data. Humans are able to perform well on different tasks in a short amount of time. Humans, for example, only need a small amount of hours to learn how to drive a car, even without supervision. On the other hand fully automated driving AI need thousand of hours of data to drive a car. Why do humans learn so fast compared to machines?
Humans don’t rely on labeled data, because most of the time humans learn by observation. By this humans generate a basic knowledge of how the world works, which also called common sense. This enables us to learn so much faster compared to machines.
Meta AI <span class="citation">(Yann and Ishan <a href="#ref-darkMatter" role="doc-biblioref">2021</a>)</span> believes that self-supervised learning is one of the most promising ways to generate background knowledge and some sort of common sense in AI systems. By self-supervised learning one means a supervised learning algorithm, but it doesn’t need an external supervisor. Self-supervised pre-training differs between the modalities, which means there is not an approach which works in all modalities.
The following chapter will inspect on the one hand pre-training resources and the use of them and on the other hand also the benchmarks which are used for Natural Language Processing (NLP), Computer Vision (CV) and ,the combination of both, vision language pre-trained models (VL-PTM).</p>
<div id="datasets" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.3.1</span> Datasets<a href="c01-00-intro-modalities.html#datasets" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After pointing out that pre-training is very important, one might ask how do the datasets look and how do the different modalities pre-train? At first we will inspect the former one and focus afterwards on the use of the resources. As one might expect NLP models pre-train on text, CV models pre-train on images and VL-PTM pre-train on text image pairs, which can somehow be seen as a combination of NLP and CV. But CV models mostly used labeled data like a picture of a dog with the corresponding single label “dog”. MML datasets can contain several sentences of text which correspond to the given image.</p>
<p>Even if the datasets might be completely different, the procedure to get the data is mostly the same for all of them, because the data is crafted from the internet. This can lead to a problem, since by using this method the resulting dataset might be noisy. One approach for the VL-PTM, for example, is to use CommonCrawl and extract the image plus the alt of an image. The alt is an alternate text for an image, if the image cannot be displayed or for visual impaired people. This seems like a reasonable approach, but the alt is often not very informative about what’s in the image.</p>
<p>Another difference between the modalities is the cardinality of the pre-training data. It’s easy to realize that text is by far easiest to crawl from the internet. This results in huge high-quality massive text data. Some magnitudes smaller are the datasets for CV. Since VL-PTM are pretty new compared to the other modalities it still relatively small, but growing fast. A small downer is that some of the datasets are not public available. The big companies like to keep their models and used datasets private, which hinders the reproducibility, but there are also real open AI competitors like LAION and Eleuther in the field. The next chapter will provide some of the most used pre-training datasets.</p>
<div id="natural-language-processing-datasets" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.1.1</span> Natural Language Processing Datasets<a href="c01-00-intro-modalities.html#natural-language-processing-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="common-crawl" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.1.1</span> Common Crawl<a href="c01-00-intro-modalities.html#common-crawl" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>As already mentioned, extracting text from the internet is rather easy. More precisely there is a non-profit organization, called <a href="https://commoncrawl.org">Common Crawl</a>, which does exactly this. They provide copies of the internet to researchers, companies and individuals at no cost for the purpose of research and analysis. The Common Crawl corpus contains petabytes of data collected since 2008. Every month, Common Crawl releases a snapshot of the web obtained by randomly exploring and sampling URLs. It contains raw web page data, extracted metadata and text extractions. The advantages of Common Crawl come along with their disadvantages. The text is from diverse domains but with varying quality of data. To handle the raw nature of the datasets one often has to use a well-designed extraction and filter to use the datasets appropriately <span class="citation">(<span class="citeproc-not-found" data-reference-id="gao2020pile"><strong>???</strong></span>)</span>. GPT-3 ,for example, uses a filtered version of Common Crawl, which consists of 410 billion tokens <span class="citation">(T. Brown et al. <a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>. So data for NLP is freely available but one needs to use well-designed extraction and filtering to really use the dataset.</p>
</div>
<div id="the-pile" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.1.2</span> The Pile<a href="c01-00-intro-modalities.html#the-pile" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Recent work <span class="citation">(<span class="citeproc-not-found" data-reference-id="rosset2020turing"><strong>???</strong></span>)</span> showed that diversity in training datasets improves general cross-domain knowledge and downstream generalization capability for language models. The Pile <span class="citation">(<span class="citeproc-not-found" data-reference-id="gao2020pile"><strong>???</strong></span>)</span> was introduced to address exactly these results. The Pile contains <span class="math inline">\(22\)</span> sub-datasets, including established NLP datasets, but also several newly introduced ones. The size of the <span class="math inline">\(22\)</span> sub-datasets, which can be categorized roughly into five categories, pile up to around <span class="math inline">\(825\)</span> GB of data.
The following treemap shows the distribution of the dataset.</p>
<p><img src="figures/01-chapter1/thePile.png" /></p>
<p>While only 13% of the world’s population speaks English, the vast majority of NLP research is done on English. <span class="citation">(<span class="citeproc-not-found" data-reference-id="gao2020pile"><strong>???</strong></span>)</span> followed this trend, but did not explicitly filtered out other languages when collecting our the data. This leads to the fact that roughly 95% of the Pile is English. Also EuroParl <span class="citation">(Koehn <a href="#ref-koehn2005europarl" role="doc-biblioref">2005</a>)</span>, a multilingual parallel corpus introduced for machine translation, is included in the Pile. To train GPT-2 Open AI collected data from WebText. WebText is an internet dataset created by scraping URLs extracted from Reddit submissions with a minimum score for quality, but sadly it was never released to the public. Independent researchers reproduced the pipeline and released the resulting dataset, called OpenWebTextCorpus <span class="citation">(Gokaslan and Cohen <a href="#ref-Gokaslan2019OpenWeb" role="doc-biblioref">2019</a>)</span> (OWT). Eleuther created an enhanced version of the original OWT Corpus called OpenWebText2. It covers all Reddit submissions from 2005 up until April 2020. It covers content from multiple languages, document metadata, multiple dataset versions, and open source replication code.</p>
<p>They also explicitly included a dataset of mathematical problems (DeepMind Mathematics) to improve the mathematical ability of language models trained on the Pile. An ArXiv dataset was in included in the hopes that it will be a source of high quality text and math knowledge, and benefit potential downstream applications to research in these areas and also because arXiv papers are written in LaTeX. Training a language model to be able to generate papers written in LaTeX could be a huge benefit to the research community.</p>
<p>Since CC needs further steps, due to the raw nature of CC, to really use is. Pile-CC is Common Crawl-based dataset, which can be used directly. It yields higher quality output than directly using the WET files.
These were only some of the 22 included datasets. A more detailed description of the sub-dataset and the reasons why these were included can be found in the corresponding paper <span class="citation">(<span class="citeproc-not-found" data-reference-id="gao2020pile"><strong>???</strong></span>)</span>.</p>
</div>
<div id="multilingual-datasets" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.1.3</span> Multilingual Datasets<a href="c01-00-intro-modalities.html#multilingual-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Another pre-cleaned version of CC is CC-100<span class="citation">(Wenzek et al. <a href="#ref-wenzek2019ccnet" role="doc-biblioref">2019</a>)</span>. They present a pipeline to create curated monolingual corpora in more than 100 languages. A filter, which covers the data based on their distance to Wikipedia, is used and this improves the quality of the resulting dataset. However, its English portion is much smaller than the Pile. But a multilingual dataset might help a low-resource language acquire extra knowledge from other languages.
Perhaps the most multilingual corpus publicly available, containing 30k sentences in over 900 languages, is the Bible corpus <span class="citation">(Mayer and Cysouw <a href="#ref-mayer2014creating" role="doc-biblioref">2014</a>)</span>.
Till now all datasets were freely available and almost directly usable. The next one is not public available for some reasons.</p>
<p>To provide mT5 <span class="citation">(Xue et al. <a href="#ref-xue2020mt5" role="doc-biblioref">2020</a>)</span>, which is multilingual pre-trained text-to-text transformer, a suitable pre-training dataset, Google Research designed a dataset including more than 100 languages. The dataset is called mC4 <span class="citation">(Xue et al. <a href="#ref-xue2020mt5" role="doc-biblioref">2020</a>)</span>. Since some languages are relatively scarce on the internet, they used all of the 71 monthly web scrapes released so far by Common Crawl. It contains 6.6 billion pages and 6.3 trillion tokens. A smaller version of the mC4 is also used by Google Research. The smaller dataset C4 (Colossal Clean Common Crawl) was explicitly designed to be English only. The C4 dataset is a collection of about <span class="math inline">\(750\)</span>GB of English-language text sourced from the public Common Crawl web.</p>
<p>Most of the datasets used in NLP are derived entirely from Common Crawl and <span class="citation">(<span class="citeproc-not-found" data-reference-id="rosset2020turing"><strong>???</strong></span>)</span> came to the result, that the current best practice in training large-scale language models involve using both large web scrapes and more targeted, higher-quality datasets, which the Pile directly addresses.</p>
</div>
<div id="bookscorpus" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.1.4</span> BooksCorpus<a href="c01-00-intro-modalities.html#bookscorpus" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The last dataset for NLP is the BooksCorpus dataset <span class="citation">(<span class="citeproc-not-found" data-reference-id="zhu2015aligning"><strong>???</strong></span>)</span>. The BooksCorpus uses books from yet unplished authors from the web. Only books with more than 20k words were included to filter out shorter, noisier stories. This results in around 11k books from 16 different genres. So more than 74 million sentences can be used in pre-training. BooksCorpus contains a sample of books from <a href="https://www.smashwords.com">a distributor of indie ebooks</a>. Sadly a datasheet about the BooksCorpus was not releasd with the corresponing paper.</p>
<p>Frankly there was just an paragraph about the content and the extraction inside the paper <span class="citation">(<span class="citeproc-not-found" data-reference-id="zhu2015aligning"><strong>???</strong></span>)</span>. <span class="citation">Bandy and Vincent (<a href="#ref-bandy2021addressing" role="doc-biblioref">2021</a>)</span> addressed exactly this short coming. They provided a retrospective datasheet about the BooksCorpus. Some of their major concerns were copyright violations, duplicate books, skewed genre representation, potentially skewed religious representation and also problematic content (18+ content). Little harm can be expected if an informed adults reads books with these concers, but how does a language model contribute to for example well-documented gender discrimination if it trains on these books.</p>
<p>Since BookCorpus is no longer distributed, one has to visit the distributor of the <a href="https://www.smashwords.com">indie ebooks</a> and collect a own version of the BookCorpus. This is one of the user-based dataset, besides to the datasets of the Pile.</p>
</div>
</div>
<div id="computer-vision-dataset" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.1.2</span> Computer Vision Dataset<a href="c01-00-intro-modalities.html#computer-vision-dataset" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="imagenet" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.2.1</span> ImageNet<a href="c01-00-intro-modalities.html#imagenet" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The next inspected modality is CV. Almost every state-of-the-art CV model uses a classifier pre-trained on an ImageNet based dataset. ImageNet uses the hierarchical structure of WordNet <span class="citation">(<span class="citeproc-not-found" data-reference-id="fellbaum2010wordnet"><strong>???</strong></span>)</span>. At the release of ImageNet-1k the amount of classes was unheard at this time point. Datasets like CIFAR-10 <span class="citation">(<span class="citeproc-not-found" data-reference-id="krizhevsky2009learning"><strong>???</strong></span>)</span> and CIFAR-100 <span class="citation">(<span class="citeproc-not-found" data-reference-id="krizhevsky2009learning"><strong>???</strong></span>)</span> had 10 or 100 classes, but ImageNet1k had 1000 different classes and this was not the only major improvement. They also increased the resolution from <span class="math inline">\(32 \times 32\)</span> to <span class="math inline">\(256 \times 256\)</span>. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. The ImageNet-1k dataset is a subset of the ImageNet dataset <span class="citation">(Deng et al. <a href="#ref-deng2009imagenet" role="doc-biblioref">2009</a>)</span>. The full ImageNet dataset is also called ImageNet-21k. It consists of more than 14 million images, divided in almost 22k classes. Because of this some paper described it as ImageNet-22k.</p>
<p>Those two dataset do not only differ by the amount of classes, but also by the type of labels. The labels of ImageNet-21k are not mutually exclusive. Because of this the pre-training wiht ImageNet-1k is far more popular. Also the ImageNet-21k dataset lacks an official train-validation split, which is just another reason why ImageNet-1k is more popular. The raw dataset ImageNet-21k is around 1.3 terabyte (TB). It’s also nice, that the the dataset of ImageNet are open available. The next dataset is in contrast to this, because it’s not freely available.</p>
</div>
<div id="joint-foto-tree-jft-entity-foto-tree-eft" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.2.2</span> Joint-Foto-Tree (JFT) &amp; Entity-Foto-Tree (EFT)<a href="c01-00-intro-modalities.html#joint-foto-tree-jft-entity-foto-tree-eft" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The Joint-Foto-Tree (JFT) 300M is one of the follow up version of the JFT dataset <span class="citation">(<span class="citeproc-not-found" data-reference-id="hinton2015distilling"><strong>???</strong></span>)</span>. Given the name it consists of 300 million images and on average each image has 1.26 labels. The whole datasets has around 375 million labels. These labels can be divided into 18291 classes. These categories form a rich hierarchy with the maximum depth of hierarchy being 12 and maximum number of child for parent node being 2876 <span class="citation">(<span class="citeproc-not-found" data-reference-id="sun2017revisiting"><strong>???</strong></span>)</span>. For example there are labels for 1165 types of animals and 5720 types of vehicles. The work states that approximately 20% of the labels in this dataset are noisy <span class="citation">(<span class="citeproc-not-found" data-reference-id="sun2017revisiting"><strong>???</strong></span>)</span>, because the labels are generated automatically.</p>
<p>It also provides the fact, that the distribution is heavily long-tailed, which means that some of the classes have less than 100 images. There is also an extendend version of the JFT dataset.</p>
<p>It’s called Entity-Foto-Tree (EFT), because the class labels are physical entities organized in a tree-like hierarchy, which contains 20 diversified verticals and consists of 100k classes. It’s even rarely used in practice by Google because of the intolerable large model size and the slow training speed <span class="citation">(Gao et al. <a href="#ref-gao2017knowledge" role="doc-biblioref">2017</a>)</span>. Honestly nobody really knows what is inside these datasets, except Google and they never published a datasheet about it.</p>
<p>These datasets are often used for image classification, but localization-sensitive tasks like object detection and semantic segmentation are also of interest in CV.</p>
</div>
<div id="objects365" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.2.3</span> Objects365<a href="c01-00-intro-modalities.html#objects365" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Objects365 <span class="citation">(Shao et al. <a href="#ref-shao2019objects365" role="doc-biblioref">2019</a>)</span> is a large-scale object detection and semantic segmentation freely available dataset. It contains 365 object categories with over 600K training images. More than 10 million, high-quality bounding boxes are manually labeled through a three-step, carefully designed annotation pipeline. The ImageNet datasets also contain bounding boxes, but compared Object365 dataset the number of boxes per image is about 15.8 vs 1.1 <span class="citation">(Deng et al. <a href="#ref-deng2009imagenet" role="doc-biblioref">2009</a>)</span>. They collected images mainly from Flicker to make the image sources more diverse. All the images conform to licensing for research purposes. The dataset also builds on a tree-like hierarchy with eleven super-categories (human and related accessories, living room, clothes, kitchen, instrument, transportation, bathroom, electronics, food (vegetables), office supplies, and animal). Further they proposed 442 categories which widely exists in daily lives. As some of the object categories are rarely found, they first annotate all 442 categories in the first 100K images and then they selected the most frequent 365 object categories as their target objects.</p>
<p>To enable compatibility with the existing object detection benchmarks, the 365 categories include the categories defined in Microsoft Common Objects in Context (COCO) <span class="citation">(T.-Y. Lin, Maire, Belongie, Hays, Perona, Ramanan, Dollár, and Zitnick <a href="#ref-lin2014microsoft" role="doc-biblioref">2014</a><a href="#ref-lin2014microsoft" role="doc-biblioref">a</a>)</span>, which is described in the next paragraph.</p>
</div>
<div id="microsoft-common-objects-in-context-coco" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.2.4</span> Microsoft Common Objects in Context (COCO)<a href="c01-00-intro-modalities.html#microsoft-common-objects-in-context-coco" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Microsoft decided to employed a novel pipeline for gathering data with extensive use of Amazon Mechanical Turk. Their goal was to create a non-iconic image collection. Iconic-object images have a single large object in the centered of the image. By this they provide high quality object instances, but they also lack information of contextual important and non-canonical viewpoints <span class="citation">(T.-Y. Lin, Maire, Belongie, Hays, Perona, Ramanan, Dollár, and Zitnick <a href="#ref-lin2014microsoft" role="doc-biblioref">2014</a><a href="#ref-lin2014microsoft" role="doc-biblioref">a</a>)</span>. Recent work showed that non-iconic images are better at generalizing <span class="citation">(<span class="citeproc-not-found" data-reference-id="torralba2011unbiased"><strong>???</strong></span>)</span>. They mostly used Flickr images, because they tend to have fewer iconic images. This results in a collection of 328,000 images. After getting the images they used workers on Amazon’s Mechanical Turk for the annotation. The workers got a list with 91 categories and 11 super-categories. At first a worker had to decide if a super-category (e.g. animal) was present or not. If it was present he had to class the animal into the appropriate subordinate category (dog, cat, mouse). This greatly reduces the time needed to classify the various categories and took the workers about 20k hours to complete. After this the workers had also to do instance spotting and instance segmentation. For the instance segmentation the workers had to complete a training task until their segmentation adequately matched the ground truth. Only 1 in 3 workers passed this training stage. At the end they added five written captions to each image in the dataset, which is called Microsoft Common Objects in Context.</p>
<p>At the end they utilized more than 70,000 worker hours to collect a amount of annotated object instances, which were gathered to drive the advancement of segmentation algorithms and others tasks. COCO is a dataset which can be used in CV and also in multi-modal models, because of the image-text pairs.</p>
</div>
</div>
<div id="multi-modal-datasets" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.1.3</span> Multi Modal Datasets<a href="c01-00-intro-modalities.html#multi-modal-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Pile is an attempt from Eleuther to mimic the dataset used for GPT-3 and LAION wants to achieve something similiar. Open AI collected more than 250 million text-images pairs from the internet to train CLIP and DALL-E. This dataset does include parts of COCO, Conceptual Captions and a filtered subset of the Yahoo Flickr Creative Commons 100 Million Dataset (YFCC100M). YFCC100M contains of a total of 100 million media objects. The collection provides a comprehensive snapshot of how photos and videos were taken, described, and shared over the years, from the inception of Flickr in 2004 until early 2014. Also this dataset was never published, even though the used data is freely available. To address this shortcoming, LAION created the LAION-400M.</p>
<div id="laion-400m-5b" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.3.1</span> LAION 400M &amp; 5B<a href="c01-00-intro-modalities.html#laion-400m-5b" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>LAION-400M <span class="citation">(<span class="citeproc-not-found" data-reference-id="schuhmann2021laion"><strong>???</strong></span>)</span> consists of 400 million image-text pairs. They used Common Crawl and parsed out all HTML IMG tags containing an alt-text attribute. As already mentioned these alt-texts can sometimes be very uninformative. So they used CLIP to compute embeddings of the image and alt-text and droped all samples with a similarity below 0.3. The dataset also contains the CLIP embedding and kNN indices. <span class="citation">(<span class="citeproc-not-found" data-reference-id="schuhmann2021laion"><strong>???</strong></span>)</span> describes the procedure to create the dataset in an open manner. They also ran DALLE-pytroch, an open-source replication of DALL-E, on a subset of LAION-400M and produced samples of sufficient quality. This opens the road for large-scale training and research of language-vision models, which was previously not possible for everyone. It still is difficult, because of the large amount of data, but at least it’s theoretically possible for everyone. LAION-400M is also known as crawling@home (C@H), because they started as a small group and used only their own computers at the beginning, which is like the fight of David versus Goliath.</p>
<p>End of March 2022 the team of LAION released a <span class="math inline">\(14 \times\)</span> bigger than LAION-400M dataset called LAION-5B. It consists of 5.85 billion CLIP-filtered image-text pairs. A paper about the dataset is right now in progress, but the dataset is already available to download if you have enough space. The size of the dataset is about <span class="math inline">\(240\)</span> TB in <span class="math inline">\(384\)</span> or 80 TB in <span class="math inline">\(224\)</span>. Due to the nature of the extraction 2,3 billion contain English language, 2,2 billion samples from 100+ other languages and they also provide a <a href="https://rom1504.github.io/clip-retrieval/?back=https%3A%2F%2Fknn5.laion.ai&amp;index=laion5B&amp;useMclip=false">search demo</a>. At the moment LAION-5B is the biggest openly accessible image-text dataset.</p>
<p>The amount of image-text pairs in LAION-400M or LAION-5B seems incomparable to COCO, but one has to keep in mind, that the text in the COCO dataset is gathered in a high-quality manner. The COCO dataset is still used, because of the high quality, even though it was created 2014.</p>
</div>
<div id="localized-narratives" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.3.2</span> Localized Narratives<a href="c01-00-intro-modalities.html#localized-narratives" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Localized Narratives choose a new form of connecting vision and language in multi-modal image annotations <span class="citation">(<span class="citeproc-not-found" data-reference-id="pont2020connecting"><strong>???</strong></span>)</span>. They asked annotators to describe an image with their voice while simultaneously hovering their mouse over the region they are describing. This synchronized approach enable them to determine the image location of every single word in the description. Since the automatic speech recognition still results in imperfect transcription, an additional transcription of the voice stream is needed to get the written word. The manual transcription step might be skipped in the future if automatic speech recognition improves and this would result in an even more effective approach. They collected Localized Narratives for, the earlier introduced, COCO <span class="citation">(T.-Y. Lin, Maire, Belongie, Hays, Perona, Ramanan, Dollár, and Zitnick <a href="#ref-lin2014microsoft" role="doc-biblioref">2014</a><a href="#ref-lin2014microsoft" role="doc-biblioref">a</a>)</span> dataset, ADE20K <span class="citation">(<span class="citeproc-not-found" data-reference-id="zhou2017scene"><strong>???</strong></span>)</span>, Flickr30k &amp; 32k datasets <span class="citation">(<span class="citeproc-not-found" data-reference-id="young2014image"><strong>???</strong></span>)</span> and 671k images of Open Images<span class="citation">(<span class="citeproc-not-found" data-reference-id="kuznetsova2020open"><strong>???</strong></span>)</span>.</p>
<p>Localized Narratives can be used in many different multi-modal tasks, since it incorporates four synchronized modalities (Image, Text, Speech, Grounding). Another difference is that the captions are longer than in most previous datasets <span class="citation">(<span class="citeproc-not-found" data-reference-id="krishna2017visual"><strong>???</strong></span>; <span class="citeproc-not-found" data-reference-id="kuznetsova2020open"><strong>???</strong></span>; T.-Y. Lin, Maire, Belongie, Hays, Perona, Ramanan, Dollár, and Zitnick <a href="#ref-lin2014microsoft" role="doc-biblioref">2014</a><a href="#ref-lin2014microsoft" role="doc-biblioref">a</a>)</span> and models like Imagen <span class="citation">(<span class="citeproc-not-found" data-reference-id="saharia2022photorealistic"><strong>???</strong></span>)</span> and Parti <span class="citation">(Yu et al. <a href="#ref-parti" role="doc-biblioref">2022</a>)</span> work well with long prompts. Beside to that the 849k images with Localized Narratives are publicly available <span class="citation">(<span class="citeproc-not-found" data-reference-id="LocNarWeb"><strong>???</strong></span>)</span>.</p>
</div>
<div id="wudaomm" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.3.3</span> WuDaoMM<a href="c01-00-intro-modalities.html#wudaomm" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>English is the most spoken language on the world, but Mandarin Chinese is on the second place and also increasing steadily. So we will also present a large-scale Chinese multi-modal dataset WuDaoMM <span class="citation">(Yuan et al. <a href="#ref-yuan2022wudaomm" role="doc-biblioref">2022</a>)</span>. Totally it consists of 650 million image-text pair samples but, they released a base version dataset containing about 5 million image-text pairs. WuDaoMM base includes 19 categories and 5 million high-quality images, which can be used for most of Chinese vision-language model pre-training. They designed two acquisition strategies according to the correlation types between text and image. Their collection included data with weak relations, by this they mean that the texts don’t have tp precisely describe their corresponding images to be retained, and data with strong relations. These strong relation image-text pairs were found on professional websites. Most of these images are reviewed for relevance, content, and sensitivity when they are uploaded. The WuDaoMM-base dataset is a balanced sub-dataset composed of each major category of the strong-correlated dataset, which is sufficient to support the research and use of current mainstream pre-training models.</p>
</div>
<div id="wikipedia-image-text-wit" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.1.3.4</span> Wikipedia Image Text (WIT)<a href="c01-00-intro-modalities.html#wikipedia-image-text-wit" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The Wikipedia Image Text (WIT) dataset ends this chapter. Most dataset are only in English and this lack of language coverage also impedes research in the multilingual mult-imodal space. To address these challenges and to advance in research on multilingual, multimodal learning they presented WIT <span class="citation">(K. Srinivasan et al. <a href="#ref-srinivasan2021wit" role="doc-biblioref">2021</a>)</span>. They used Wikipedia articles and Wikimedia image link to extract multiple different texts associated with an image. Additionally a rigorous filtering was used to retain high quality image-text associations.</p>
<p>This results in a dataset, which contains more than 37.6 million image-text sets and spans 11.5 million unique images. Due to the multi-modal coverage of Wikipedia, they provide unique multilingual coverage – with more than 12K examples in each of the 108 languages and 53 languages have more than 100K image-text pairs.</p>
<p>Another thing which is worth pointing out, is that they could leverage Wikipedia’s editing, verification and correction mechanism,to ensure a high- quality bar. This curation can be seen an huge difference compared to the web crawls used to create other existing datasets. At the end they even verified the curated quality of the WIT dataset via an extensive human-annotation process with an overwhelming majority of 98.5% judging the randomly sampled image-text associations favorably.</p>
<p>These datasets were just some of the more used dataset. Some of them are public available while some others are not public available. Normally each dataset comes with a paper, which describes the procedure way more detailed than this chapter. This chapter gives just a small insight into the different datasets and wants to raise the interest into the corresponding papers. <a href="https://paperswithcode.com/">Papers with code</a> delivers research papers with code implementations by the authors or community. One can get information about the State-of-the-Art model for every modality and down-task. They also provide available datasets for all possible tasks.</p>
<p>Datasets are crucial for research and exploration as, rather obviously, data is required for performing experiments, analyzing designs, and building applications. A particular problem is that the collected data is often not made publicly available. While this sometimes is out of necessity due to the proprietary or sensitive nature of the data, this is certainly not always the case. A public dataset with clearly marked licenses that do not overly impose restrictions on how the data is used, such as those offered by CC, would therefore be suitable for use by both academia and industry. But one has to keep in mind that an effective dataset is a catalyst and accelerator for technological development <span class="citation">(Yuan et al. <a href="#ref-yuan2022wudaomm" role="doc-biblioref">2022</a>)</span>. This may be a reason, why the big companies don’t share their datasets, but there are also some other reasons. Another reason might be the bias which is included in the datastes.</p>
</div>
</div>
<div id="bias-in-datasets" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.1.4</span> Bias In Datasets<a href="c01-00-intro-modalities.html#bias-in-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Internet access itself is not evenly distributed, which results in a narrow Internet participation. So internet data overrepresents younger users and those from developed countries. User-generated content sites present themselves as open to anyone, but there are factors including moderation practices which make them less welcoming to specific sub-populations.
Take the training data of GPT-2 as an example. It is sourced by scraping outbound links from Reddit, and Pew Internet Research’s 2016 <span class="citation">(MICHAEL BARTHEL and MITCHELL <a href="#ref-redditUsers" role="doc-biblioref">2016</a>)</span> survey reveals 67% of Reddit users in the United States are men, and 64% between ages 18 and 29.
<span class="citation">Sheng et al. (<a href="#ref-sheng2019woman" role="doc-biblioref">2019</a>)</span> shedded lights on some of the gender bias. They used OpenAI’s GPT-2 to generate text given different prompts. Some of the examples can be seen in the next table.</p>
<table>
<colgroup>
<col width="26%" />
<col width="73%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"><strong>Prompt</strong></th>
<th align="center"><strong>Generated text</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">The man worked as</td>
<td align="center">a car salesman at the local Wal-Mart</td>
</tr>
<tr class="even">
<td align="center">The woman worked as</td>
<td align="center">a prostitute under the name of Hariya</td>
</tr>
<tr class="odd">
<td align="center">The Black man worked as</td>
<td align="center">a pimp for 15 years</td>
</tr>
<tr class="even">
<td align="center">The White man worked as</td>
<td align="center">a police officer, a judge, a prosecutor, a prosecutor, and the president of the United States</td>
</tr>
<tr class="odd">
<td align="center">The gay person was known for</td>
<td align="center">his love of dancing, but he also did drugs</td>
</tr>
<tr class="even">
<td align="center">The straight person was known for</td>
<td align="center">his ability to find his own voice and to speak clearly</td>
</tr>
</tbody>
</table>
<p>Datasets obviously encode the social bias that surrounds us, and models trained on that data may expose the bias in their decisions. The predictions of the models are based on what the model learned from so we habe to be aware of this bias.</p>
<p><span class="citation">Dhamala et al. (<a href="#ref-dhamala2021bold" role="doc-biblioref">2021</a>)</span> introduced the Bias in Open-Ended Language Generation Dataset (BOLD), a large-scale dataset that consists of 23,679 English text generation prompts for bias benchmarking across five domains: profession, gender, race, religion, and political ideology. They also proposed new automated metrics for toxicity, psycholinguistic norms, and text gender polarity to measure social biases in open-ended text generation from multiple angles. An examination of text generated from three popular language models (BERT, GPT-2, CTRL) revealed that the majority of these models exhibit a large social bias across all domains. It was also shown that GPT-2 conform more to social biases than BERT and GPT-3 was trained on filtered version of the Common Crawl dataset, developed by training a classifier to pick out those documents that are most similar to the ones used in GPT-2’s training data. So very likely the same goes for GPT-3. These biases don’t only persist in the NLP datasets, they can also be found in other modalites.</p>
<p>There exists the so called WordNet Effect which leads to some bias in the CV datasets. This effects emerges because WordNet includes words that can be perceived as pejorative or offensive. N*****r and wh**e are just two examples which can be found in WordNet. <span class="citation">Prabhu and Birhane (<a href="#ref-prabhu2020large" role="doc-biblioref">2020</a>)</span> investigated problematic practices and the consequences of large scale vision datasets. Broad issues such as the question of consent and justice as well as specific concerns such as the inclusion of verifiably pornographic images in datasets were revealed. Two days after the publication of the paper <span class="citation">(Prabhu and Birhane <a href="#ref-prabhu2020large" role="doc-biblioref">2020</a>)</span>, the TinyImages was <a href="https://groups.csail.mit.edu/vision/TinyImages/">withdrawn</a>, because of their findings. <a href="https://groups.csail.mit.edu/vision/TinyImages/">Torralba, Fergus, Freeman</a>, the creator of TinyImages, also argued that the offensive images were a consequence of the automated data collection procedure that relied on nouns from WordNet. MS-Celeb <span class="citation">(Guo et al. <a href="#ref-guo2016ms" role="doc-biblioref">2016</a>)</span> was also retracted for the same reasons. It would be very surprising if these kinds of problems where not present in other databases for this kind of research, especially as we get to extremely dataset sizes. Despite retractions, datasets like TinyImages and MS-Celeb remain widely available through file sharing websites.</p>
<p>Even if LAION-400M opened the road for large-scale training and research of language-vision models for everyone, their curation pipeline involves CLIP. One might argue, that this approach will potentially generate CLIP-like models and it is known that CLIP inherits various biases <span class="citation">(Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, and others <a href="#ref-radford2021learning" role="doc-biblioref">2021</a>)</span>. <span class="citation">Birhane, Prabhu, and Kahembwe (<a href="#ref-birhane2021multimodal" role="doc-biblioref">2021</a>)</span> found that the LAION-400M dataset contains, troublesome and explicit images and text pairs of rape, pornography, malign stereotypes, racist and ethnic slurs, and other extremely problematic content and you can be pretty sure that the same holds for LAION-5B, as it uses the same curation pipeline. This shows even more that large institutions should open up their datasets to both internal and external audits in a thoughtful manner. We have to fully understand the risks of using such datasets and this is not achievable by the used approach. Despite all these concerns, the next chapters will demonstrate how the different datasets are used, but it is important to keep these concerns in mind.</p>
</div>
</div>
<div id="pre-training-tasks" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.3.2</span> Pre-Training Tasks<a href="c01-00-intro-modalities.html#pre-training-tasks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Yann LeCun and Ishan Misra suggest in their <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">blogpost</a> that supervised pre-training is gone because of the already mentioned reasons at the beginning and the future will be self-supervised pre-training <span class="citation">(Yann and Ishan <a href="#ref-darkMatter" role="doc-biblioref">2021</a>)</span>. Meta AI wants to create a background knowledge in the models that can approximate the common sense of humans. This suggestion is even more reasonable, because recent work <span class="citation">(Mineault <a href="#ref-unsupBrain" role="doc-biblioref">2021</a>)</span> also showed that a self-supervised or a unsupervised pre-training approach is biologically more plausible than supervised methods. This why neuroscientists are taking interest in unsupervised and self-supervised deep neural networks in order to explain how the brain works <span class="citation">(Zhuang et al. <a href="#ref-zhuang2021unsupervised" role="doc-biblioref">2021</a>)</span>.</p>
<p>Self-supervised learning (SSL) is also called predictive learning. This comes by the nature of the process. The general technique of self-supervised learning is to predict any unobserved or hidden part (or property) of the input from any observed or unhidden part of the input <span class="citation">(Yann and Ishan <a href="#ref-darkMatter" role="doc-biblioref">2021</a>)</span>. Models like BERT try to predict between known intervals and GPT-3 predicts the future, given the past. A part of a sentence is hidden and the model tries to predict the hidden words from the remaining ones. Predicting missing parts of the input is one of the more standard tasks for SSL pre-training. To complete a sentence with missing parts the system has to learn how to represent the meaning of words, the syntactic role of words, and the meaning of entire texts.</p>
<p>These missing parts tasks are easy to implement in NLP compared to CV. In NLP the solution space is finite, because one estimates a distribution from, a before specified, dictionary. In CV the solution space is infinite, because it is not possible to explicitly represent all the possible frames and associate a prediction score to them <span class="citation">(Yann and Ishan <a href="#ref-darkMatter" role="doc-biblioref">2021</a>)</span>.</p>
<p>Meta AI proposed an unified view of self-supervised method. They say an energy-based model (EBM) is a system that, given two inputs, x and y, tells us how incompatible they are with each other <span class="citation">(Yann and Ishan <a href="#ref-darkMatter" role="doc-biblioref">2021</a>)</span>. If the energy is high, x and y are deemed incompatible; if it is low, they are deemed compatible.</p>
<p>The idea sounds simple, but it is difficult to achieve this. An usual approach is to take an image and create an augmented version of the image. By this approach the energy has to be low, because it’s from save picture. For example one can gray scale the image. By this we say the model the color does not matter. <span class="citation">Bromley et al. (<a href="#ref-bromley1993signature" role="doc-biblioref">1993</a>)</span> proposed this kind of approach under the name Siamese networks. The difficulty is to make sure that the networks produce high energy, i.e. different embedding vectors, when x and y are different images. The problem is that these Siamese networks tend to collapse. When a collapse occurs, the energy is not higher for nonmatching x and y than it is for matching x and y. So the networks ignore their input and produce the same embeddings.</p>
<p>This lead to so called contrastive methods. The method used to train NLP systems by masking or substituting some input words belongs to the category of contrastive methods. Contrastive methods are based on the simple idea of constructing pairs of x and y that are not compatible, and adjusting the parameters of the model so that the corresponding output energy is large. The problem is that they are very inefficient to train. For a contrastive methods one needs so called hard negatives. These are images that are similar to image x but different enough to still produce a high energy. This is a major issue of contrastive methods. So Self-supervised representation learning relies on negative samples to prevent collapsing to trivial solutions.</p>
<p>So the best idea is to get rid of the hard negatives and BYOL <span class="citation">(Grill, Strub, Altché, Tallec, Richemond, et al. <a href="#ref-grill2020bootstrap" role="doc-biblioref">2020</a><a href="#ref-grill2020bootstrap" role="doc-biblioref">a</a>)</span> is one approach that achieved exactly this. They create two slightly different variants of an image by applying two random augmentations, like a random crop, a horizontal flip, a color jitter or a blur. A big difference to the Siamese network is that they use different parameters in the encoder. They use so called online and target parameters. The target parameters are never learned, they are just copied over from the online parameters, but they use an exponential moving average. So it’s some kind of a lagged version of the online parameters. BYOL achieves to learn a representation of an image, without using negative pairs, just by predicting previous versions of its outputs.</p>
<p>Still they say, that BYOL remains dependent on existing sets of augmentations and these augmentations require human intention and automating the search for these augmentations would be an important next step, if this is even possible <span class="citation">(Grill, Strub, Altché, Tallec, Richemond, et al. <a href="#ref-grill2020bootstrap" role="doc-biblioref">2020</a><a href="#ref-grill2020bootstrap" role="doc-biblioref">a</a>)</span>.</p>
<p><span class="citation">He et al. (<a href="#ref-he2022masked" role="doc-biblioref">2022</a>)</span> recently came very close to the MLM pre-training used in BERT with their masked autoencoder (MAE). They leveraged transformers and autoencoders for self-supervised pre-training. An autoencoder is an encoder that maps the observed signal to a latent representation, and a decoder that reconstructs the original signal from the latent representation. The MAE is a form of denoising autoencoding exactly like the MLM. Their approach is to divide an image into, for example, 16 <span class="math inline">\(\times\)</span> 16 patches. Then remove 75% of the patches and just use the remaining 25% in their huge encoder. Important to add is that the position embeddings are also used in the encoder. The input of the decoder is again the full set of tokens consisting of the unmasked and the masked tokens. So the MAE has to reconstruct the input by predicting the pixel values for each masked patch. Autoencoding pursues a conceptually different direction compared to BYOl or DINO, which are based on augmentation.</p>
<p>Still their reconstructions look kind of blury, but the learned representations are already very rich. Interesting to note is also that BERT removes only 15% of the data where MAE removes 75% of the data.</p>
<p>Dual encoder models like CLIP <span class="citation">(Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, and others <a href="#ref-radford2021learning" role="doc-biblioref">2021</a>)</span> and ALIGN <span class="citation">(C. Jia, Yang, Xia, Chen, et al. <a href="#ref-jia2021scaling" role="doc-biblioref">2021</a><a href="#ref-jia2021scaling" role="doc-biblioref">a</a>)</span> demonstrated in the past that contrastive objectives on noisy image-text pairs can lead to strong image and text representations. One thing to mention is, that contrastive objectives are easier to implement in vision-language models (VLM) than in CV. This comes from the fact that VLM use image-text pairs. As a dual encoder CLIP encodes the image and text and by construction the text which corresponds to the image or vice versa achieves the highest similarity and the other texts will have a lower similarity. So one already has some hard negatives already available and don’t has to search for some.</p>
<p>Through the SSL the models already learned a good representation of the given input, but fine-tuning models leads to even better results. This chapter will just provide an rough sketch, since fine-tuning heavily depends on the model and the down-stream task. Also fine-tuning will be shown in later chapters. Fine-tuning means updating the weights of a pre-trained model by training on a supervised (labeled) dataset to a specific down-task. A huge amount of data is needed to fine-tune a model. This is also the main disadvantage of fine-tuning, because one needs new large dataset for every possible down-task.</p>
<p>After pre-training and fine-tuning the models there is a need to compare the models, because one always seeks to find the best model among all competitors. This need lead to the creation of datasets for test purposes which are often called benchmarks.</p>
</div>
<div id="benchmarks" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.3.3</span> Benchmarks<a href="c01-00-intro-modalities.html#benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As models got better over time, because of bigger datasets or better pre-training tasks, it’s important to create and use new benchmarks. Interestingly there are also benchmark, which rely only on Zero-Shot performance. Zero-shot learning (ZSL) is a problem in machine learning, where during test time, a model observes samples from classes not observed during training. So it has to complete a task without having received any training examples. By this the model has to generalize on a novel category of samples.</p>
<p>But the most common approach is to use a part of the datasets which was not used to train the model. To make this possible the pre-training datasets are divided into training, test and validation sets. It’s clear that the models must not be tested on the training data.</p>
<p>This splitting results in so called held-out data, but <span class="citation">Rajpurkar, Jia, and Liang (<a href="#ref-rajpurkar2018know" role="doc-biblioref">2018</a>)</span> showed, that this held-out datasets are often not comprehensive, and contain the same biases as the training data. <span class="citation">Recht et al. (<a href="#ref-recht2019imagenet" role="doc-biblioref">2019</a>)</span> also proposed that these held-out datasets may overestimate the real-world performance.</p>
<p>Something to consider is also that pre-training on large internet datasets may lead to the unintentional overlap of pre-training and down-tasks. Because of this studies <span class="citation">(Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, and others <a href="#ref-radford2021learning" role="doc-biblioref">2021</a>, <span class="citation">@parti</span>, <span class="citation">@brown2020language</span>)</span> conducted a de-duplication analysis. CLIP analysis resulted in a median overlap of 2.2% and an average overlap of 3.2%, but they also observed that the overall accuracy is rarely shifted by more than 0.1% <span class="citation">(Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, and others <a href="#ref-radford2021learning" role="doc-biblioref">2021</a>)</span>. <span class="citation">Mahajan et al. (<a href="#ref-mahajan2018exploring" role="doc-biblioref">2018</a>)</span>, <span class="citation">Kolesnikov et al. (<a href="#ref-kolesnikov2019large" role="doc-biblioref">2019</a>)</span> also came to the similar results, but it’s still something to keep in mind.</p>
<p>Some of the already mentioned datasets like COCO and the ImageNet versions are often used for CV or VLM. Almost every state-of-the-art CV model uses a classifier pre-trained on an ImageNet based dataset and benchmarked on the validation sets of the dataset. A another small downer is that the models of the big companies are usually trained on different datasets, but at least compared on the same benchmarks. So the comparison seems a bit odd. Maybe the better performance of the models comes from the different pre-training datasets.</p>
<div id="natural-language-processing-benchmarks" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.3.1</span> Natural Language Processing Benchmarks<a href="c01-00-intro-modalities.html#natural-language-processing-benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div id="superglue" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.1.1</span> (Super)GLUE<a href="c01-00-intro-modalities.html#superglue" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The goal of NLP is the development of general and robust natural language understanding systems. Through SSL models gain a good “understanding” of language in general. To benchmark this good “understanding” General Language Understanding Evaluation (GLUE) was created. It’s a collection of nine different task datasets. These datasets can be divided into the Single-Sentence Tasks, Similarity and Paraphrase Tasks and Inference Tasks.</p>
<p>The Single-Sentence Tasks consist of the Corpus of Linguistic Acceptability (CoLA) and The Stanford Sentiment Treebank (SST-2). Each example in the CoLA is a sequence of words annotated with whether it is a grammatical English sentence. SST-2 uses sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence.</p>
<p>For the Similarity and Paraphrase Tasks the Microsoft Research Paraphrase Corpus (MRPC), Quora Question Pairs (QQP) and the Semantic Textual Similarity Benchmark (STS-B) are used. MRPC is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent. The model has to predict if sentence B is a paraphrase of sentence A. The STS-B sub-task dataset consist of a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5. The task for the model is to predict these similarity scores. QQP is a collection of question pairs from the community question-answering website Quora. Here the model has to predict if a pair of questions are semantically equivalent.</p>
<p>Lastly The Multi-Genre Natural Language Inference Corpus (MNLI), the Stanford Question Answering Dataset (QNLI), The Recognizing Textual Entailment (RTE) dataset and the Winograd Schema Challenge (WNLI) are used in the Inference Tasks. WNLI is a crowdsourced collection of sentence pairs with textual entailment annotations. The task is to predict whether the premise entails the hypothesis (entailment), contradicts the hypothesis (contradiction), or neither (neutral). QNLI is a question-answering dataset consisting of question-paragraph pairs, where one of the sentences in the paragraph contains the answer to the corresponding question. The task is to determine whether the context sentence contains the answer to the question. RTE comes from a series of annual textual entailment challenges. WNLI is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun from a list of choices. In the following table is a short summary of all GLUE tasks.
<img src="figures/01-chapter1/glue_table_condensed.png" alt="taken from https://mccormickml.com" />
A nice topping is that GLUE also provides a leaderboard with a human benchmark. So the models can compete against each other and a human benchmark. After a short period of time the models started to surpass the human benchmark, which lead to creation of SuperGLUE.</p>
<p>SuperGLUE also consists of a public leaderboard built around eight language understanding tasks, drawing on existing data, accompanied by a single-number performance metric, and an analysis toolkit. SuperGLUE surpassed GLUE because of more challenging tasks, more diverse task formats, comprehensive human baslines, improved code support and refinded usage rules.
The following figure gives a short summary of the SuperGLUE tasks.</p>
<div class="figure">
<img src="figures/01-chapter1/SuperGLUE.png" alt="" />
<p class="caption">taken from <a href="https://mccormickml.com" class="uri">https://mccormickml.com</a></p>
</div>
<p>The GLUE and SuperGLUE tasks are more or less reduced to a classification problem. One might argue if this is really General Language Understanding, but we will see other benchmarks which try evaluate that in an other way.</p>
<p>However it’s also of interest to check if the models understand what they are reading. The act of understanding what you are reading is called reading comprehension (RC). RC requires both understanding of natural language and knowledge about the world.</p>
</div>
<div id="stanford-question-answering-dataset-squad-1.0-2.0" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.1.2</span> Stanford Question Answering Dataset (SQuAD) (1.0 &amp; 2.0)<a href="c01-00-intro-modalities.html#stanford-question-answering-dataset-squad-1.0-2.0" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="citation">Rajpurkar et al. (<a href="#ref-rajpurkar2016squad" role="doc-biblioref">2016</a>)</span> introduced the Stanford Question Answering Dataset (SQuAD), a large reading comprehension dataset on Wikipedia articles with human annotated question-answer pairs. SQuAD contains 107,785 question-answer pairs on 536 articles and it does not provide a list of answer choices for each question. The model must select the answer from all possible spans in the passage, thus needing to cope with a fairly large number of candidates. The problem is that the it’s guaranteed that the answer exist in the context document.</p>
<p>To address this weakness <span class="citation">Rajpurkar, Jia, and Liang (<a href="#ref-rajpurkar2018know" role="doc-biblioref">2018</a>)</span> presented SQuAD 2.0, the latest version of SQuAD. SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones.</p>
<p><span class="citation">Rajpurkar, Jia, and Liang (<a href="#ref-rajpurkar2018know" role="doc-biblioref">2018</a>)</span> contribution to NLP is not that they provide a deeper glimpse into the workings of QA systems, they also facilitated the creation of more non-English datasets. Korean, Russian, Italian, Spanish, French and Arabic versions of SQuAD exist around the world. XQuAD, MLQA and TyDi are multilingual question-answering datasets. XQuAD is a subset of SQuAD translated into 10 different language by professional translators. These kinds of resources are crucial in ensuring that the societal benefits of NLP can also be felt by speakers of lower resourced languages.</p>
</div>
<div id="beyond-the-imitation-game-benchmark-big-bench" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.1.3</span> Beyond the Imitation Game Benchmark (BIG-bench)<a href="c01-00-intro-modalities.html#beyond-the-imitation-game-benchmark-big-bench" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>The mentioned ones are rather old compared to Beyond the Imitation Game Benchmark (BIG-bench) <span class="citation">(Srivastava et al. <a href="#ref-srivastava2022beyond" role="doc-biblioref">2022</a>)</span>. It’s a collaborative benchmark intended to probe large language models and extrapolate their future capabilities. BIG-bench already contains more than 200 tasks. They claim that current language-modeling benchmarks are insufficient to satisfy our need to understand the behavior of language models and to predict their future behavior. They mainly provide three reasons for that. One of them is the short useful lifespans. When human-equivalent performance is reached for these benchmarks, they are often either discontinued. One might call this “challenge-solve-and-replace” evaluation dynamic.</p>
<p>To prevent this they encourage new task submissions and literally everybody can submit a task to BIG-Bench. So they call BIG-bench a living benchmark. The review of the tasks is based on ten criteria. It includes for example “Justification”. One has to give background motivating why this is an important capability of large language models to quantify. With the inclusion of small tasks they want to improve the diversity of topics covered and enable domain experts to contribute tasks without the difficulties of distributed human labeling.</p>
<p>Another reason for the insufficients is because the others benachmarks are narrowly targeted, and because their targets are often ones that language models are already known to perform. So it’s not possible to identify new and unexpected capabilities that language models may develop with increased scale, or to characterize the breadth of current capabilities.</p>
<p>Finally, many current benchmarks use data collected through human labeling that is not performed by experts or by the task authors. Their benchmark tasks are primarily intended to evaluate pre-trained models, without task-specific fine-tuning. By focusing on such tasks in the zero- and few-shot evaluation setting, it becomes possible to provide meaningful scores for even those tasks with a very small number of examples.</p>
<p>The “everybody can submit” strategy also leads to inclusion a variety of tasks covering non-English languages. Till now the large language models, like GPT-3 and PaLM, perform poorly on BIG-bench relative to expert humans, which is maybe a good sign for the future. But superhuman performance on SuperGLUE benchmark was achieved in less than 18 months after it was produced.</p>
</div>
<div id="wmt" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.1.4</span> WMT<a href="c01-00-intro-modalities.html#wmt" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>There is a family of datasets which is the most popular datasets used to benchmark machine translation systems. <a href="https://machinetranslate.org/wmt">Workshop on Machine Translation (WMT)</a> is the main event for machine translation and machine translation research. This conference is held annually. WMT includes competitions on different aspects of machine translation. These competitions are known as shared tasks. Typically, the task organisers provide datasets and instructions. Then teams can submit their output of their models. The submissions are ranked with human evaluation.</p>
<p>Most of the models are evaluated on bi-lingual translation like English-to-German, but there are also tri-linguar tasks like using English to improve Russian-to-Chinese machine translation. One of the most popular NLP metrics is called the Bleu Score and this metric is also used in the WMT tasks. It is based on the idea that the closer the predicted sentence is to the human-generated target sentence, the better it is. Bleu Scores are between 0 and 1, but a score of 0.6 or 0.7 is considered the best you can achieve.</p>
<p>Problematic is that <span class="citation">Bowman and Dahl (<a href="#ref-bowman2021will" role="doc-biblioref">2021</a>)</span> claim that the evaluation for many natural language understanding (NLU) tasks are broken. They claim that unreliable and biased systems score so highly on standard benchmarks that there is little room for researchers who develop better systems to demonstrate their improvements.
They provide four criteria to handle this:</p>
<ol style="list-style-type: decimal">
<li>Good performance on the benchmark should imply robust in-domain performance on the task</li>
<li>Benchmark examples should be accurately and unambiguously annotated</li>
<li>Benchmarks should offer adequate statistical power</li>
<li>Benchmarks should reveal plausibly harmful social biases in systems, and should not incentivize the creation of biased systems</li>
</ol>
<p>Building new benchmarks that improve upon these four axes is likely to be quite difficult.</p>
</div>
<div id="checklist" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.1.5</span> CheckList<a href="c01-00-intro-modalities.html#checklist" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Inspired by principles of behavioral testing in software engineering, <span class="citation">Ribeiro et al. (<a href="#ref-ribeiro2020beyond" role="doc-biblioref">2020</a>)</span> introduced CheckList, a model-agnostic and task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideas, as well as a software tool to generate a large and diverse number of test cases quickly. To break down potential capability failures into specific behaviors, CheckList introduces three different test types. A Minimum Functionality test (MFT), inspired by unit tests in software engineering, is a collection of simple examples to check a behavior within a capability. An Invariance test (INV) is when label-preserving perturbations to inputs are applied and the model prediction are expected to remain the same. A Directional Expectation test (DIR) is similar, except that the label is expected to change in a certain way.</p>
<p>Tests created with CheckList can be applied to any model, making it easy to incorporate in current benchmarks or evaluation pipelines and CheckList is open source. Their goal was to create a benchmark which goes beyond just accuracy on held-out data.</p>
</div>
</div>
<div id="computer-vision-benchmarks" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.3.2</span> Computer Vision Benchmarks<a href="c01-00-intro-modalities.html#computer-vision-benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>CV models try to answer visual tasks. A visual task is a task which can be solved only by visual input. Often visual task can be solved as a binary classification problem, which is called image classification, but there are also numerous other applications for CV. This chapter will focus on image classification, semantic segmentation and object detection with their usual benchmarks datasets.</p>
<div id="imagenet-versions" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.2.1</span> ImageNet Versions<a href="c01-00-intro-modalities.html#imagenet-versions" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>It’s not only common to pre-train your model on ImageNet datasets it’s also common to benchmark the models on them. There are many different variants of ImageNet. There is ImageNet-R, a version with non-natural images such as art, cartoons and sketches, or ImageNet-A, which is a a more challenging version because they use adversarial images <span class="citation">(Ian J Goodfellow, Shlens, and Szegedy <a href="#ref-goodfellow2014explaining" role="doc-biblioref">2014</a>)</span>, or ImageNet-V2 <span class="citation">(Recht et al. <a href="#ref-recht2019imagenet" role="doc-biblioref">2019</a>)</span>. The last was created to check whether there is an over-fitting on the classic pre-training ImageNet dataset. They followed the creation process of the original dataset and tested to what extent current classification models generalize to new data. <span class="citation">Recht et al. (<a href="#ref-recht2019imagenet" role="doc-biblioref">2019</a>)</span> found accuracy drops for all models and suggested that these drops are not caused by adaptivity, but by the models’ inability to generalize to slightly “harder” images than those found in the original test sets.</p>
<p>The goal of image classification is to classify the image by assigning a label. Typically, Image Classification refers to images in which only one object appears. To asses the performance one mainly uses Top-1 accuracy, the model’s answer with highest probability must be exactly the expected answer, or Top-5 accuracy. Top-5 accuracy means that any of five highest probability answers must match the expected answer. <span class="citation">Beyer et al. (<a href="#ref-beyer2020we" role="doc-biblioref">2020</a>)</span> tried to answer the question “Are we done with ImageNet?” in their paper. Many images of the ImageNet dataset contain a clear view on a single object of interest: for these, a single label is an appropriate description of their content. However many other images contain multiple, similarly prominent objects, limiting the relevance of a single label <span class="citation">(Beyer et al. <a href="#ref-beyer2020we" role="doc-biblioref">2020</a>)</span>. In these cases, the ImageNet label is just one of many equally valid descriptions of the image and as a result an image classifier can be penalized for producing a correct description that happens to not coincide with that chosen by the ImageNet label.</p>
<p>In short a single label per image is not sufficient in many cases. They concluded yes and no as an answert to the question “Are we done with ImageNet?”. The shortcomings of ImageNet labels and their accuracy were identified and they provided a new ImageNet validation set ReaL <span class="citation">(Beyer et al. <a href="#ref-beyer2020we" role="doc-biblioref">2020</a>)</span> (“Reassessed Labels”) and also a new metric, called ReaL accuracy <span class="citation">(Beyer et al. <a href="#ref-beyer2020we" role="doc-biblioref">2020</a>)</span>. The ReaL accuracy measures the precision of the model’s top-1 prediction, which is deemed correct if it is included in the set of labels. these findings suggested that although the original set of labels may be nearing the end of their useful life, ImageNet and its ReaL labels can readily benchmark progress in visual recognition for the foreseeable future.</p>
<p>An addition of a localization tasks to the classification tasks results into object detection. It is used to analyze more realistic cases, like mentioned above, in which multiple objects may or may not exist in an image. The location of an object is typically represented by a bounding box.</p>
</div>
<div id="ms-coco-object365" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.2.2</span> MS-COCO &amp; Object365<a href="c01-00-intro-modalities.html#ms-coco-object365" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In the recent years, the Microsoft COCO dataset or the Object365 data have become the standards to evaluate object detection algorithms, but it’s also possible to use a ImageNet dataset. The primary challenge metric is called mean Average Precision (mAP) at Intersection over Union (IoU) <span class="math inline">\(=\)</span>.50:.05:.95. The IoU is the intersection of the predicted and ground truth boxes divided by the union of the predicted and ground truth boxes. IoU, also called Jaccard Index, values range from 0 to 1. Where 0 means no overlap and 1 means perfect overlap. But how is precision captured in the context of object detection? Precision is known as the ratio of <span class="math inline">\(True~Positive/(True~Positive+False~Positive)\)</span>. With the help of the IoU threshold, it’s possible to decide whether the prediction is True Positive(TP), False Positive(FP), or False Negative(FN). The example below shows predictions with IoU threshold ɑ set at 0.5.</p>
<p><img src="figures/01-chapter1/4-birds-prediction-types-1.png" alt="taken from https://learnopencv.com" />
The .50:.05:.95 means that one uses 10 IoU thresholds of <span class="math inline">\(\{0.50, 0.55, 0.60, \dots ,0.95\}\)</span>. COCO uses this as primary metric, because it rewards detectors with better localization <span class="citation">(Mircosoft <a href="#ref-coco_eval" role="doc-biblioref">2019</a>)</span>.</p>
<p>Object detection and image segmentation are both tasks which are concerned with localizing objects of interest in an image, but in contrast to object detection image segmentation focuses on pixel-level grouping of different semantics.</p>
<p>Image segmentation can be splitted into various tasks including instance segmentation, panoptic segmentation, and semantic segmentation. Instance segmentation is a task that requires the identification and segmentation of individual instance in an image. Semantic segmentation is a task that requires segmenting all the pixels in the image based on their class label. Panoptic segmentation is a combination of semantic and instance segmentation. The task is to classify all the pixels belonging to a class label, but also identify what instance of class they belong to. Panoptic and instance segmentation is often done on COCO.</p>
</div>
<div id="ade20k" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.2.3</span> ADE20k<a href="c01-00-intro-modalities.html#ade20k" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Semantic segmentation can be done one ADE20K<span class="citation">(<span class="citeproc-not-found" data-reference-id="zhou2017scene"><strong>???</strong></span>)</span>. ADE are the first three letters of the name Adela Barriuso, who single handedly annotated the entire dataset and 20K is a reference to being roughly 20,000 images in the dataset. This dataset shows a high annotation complexity, because any image in ADE20K contains at least five objects, and the maximum number of object instances per image reaches 273. To asses the performance of a model on the ADE20K dataset one uses the mean IoU. It indicates the IoU between the predicted and ground-truth pixels, averaged over all the classes.</p>
<p>In contrast to the object detection task, the definition of TP, FP, and FN is slightly different as it is not based on a predefined threshold. TP is now the area of intersection between Ground Truth and segmentation mask. FP is the predicted area outside the Ground Truth. FN is the number of pixels in the Ground Truth area that the model failed to predict. The calculation of IoU is the same as in object detection tasks. It’s the intersection of the predicted and ground truth boxes aka. TP divided by the union of the predicted and ground truth boxes, which is essentially <span class="math inline">\(TP + FN + FP\)</span>.
A example is shown down below.</p>
<div class="figure">
<img src="figures/01-chapter1/5-segmentation-iou.png" alt="" />
<p class="caption">taken from <a href="https://learnopencv.com" class="uri">https://learnopencv.com</a></p>
</div>
</div>
</div>
<div id="multi-modal-benchmarks" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.3.3</span> Multi-Modal Benchmarks<a href="c01-00-intro-modalities.html#multi-modal-benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Visual understanding goes well beyond object recognition or semantic segmentation. With one glance at an image, a human can effortlessly imagine the world beyond the pixels. This is emphasized by the quote “a picture says more then a thousand words”. High-order of cognition and commonsense reasoning about the world is required to infer people’s actions, goals, and mental states. To answer visual understanding tasks a models needs to leverage more than one modality.</p>
<div id="visual-commonsense-reasoning-vcr" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.3.1</span> Visual Commonsense Reasoning (VCR)<a href="c01-00-intro-modalities.html#visual-commonsense-reasoning-vcr" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Visual understanding tasks require seamless integration between recognition and cognition and this task can be formalize as Visual Commonsense Reasoning (VCR). <span class="citation">Zellers et al. (<a href="#ref-zellers2019recognition" role="doc-biblioref">2019</a>)</span> introduce a new dataset called VCR. It consists of 290k multiple choice QA problems derived from 110k movie scenes. The key recipe for generating non-trivial and high-quality problems at scale is Adversarial Matching. Incorrect choices are obtained via maximum-weight bipartite matching between queries and responses. This matching transforms rich annotations into multiple choice questions with minimal bias. VCR casted as a four-way multiple choice task.</p>
<p>The underlying scenes come from the Large Scale Movie Description Challenge and YouTube movie clips and they searched for interesting an diverse situations to ensure this they trained and applied an “interestingnes filter”. The most interesting images were passed to Workers of Amazon Mechanical Turk. Additional context in form of video caption was given to the worker. After reading this they had to propose one to three questions about the image. For each question, they had to provide a reasonable answer and a rationale. This results is an underlying dataset with high agreement and diversity of reasoning. Almost every answer and rationale is unique. To make these cognition-level questions simple to ask, and to avoid the clunkiness of referring expressions, VCR’s language integrates object tags ([person2]) and explicitly excludes referring expressions (‘the woman on the right.’). These object tags are detected from Mask-RCNN. The following types of questions are in the benchmarks: 38% Explanation (‘Why is [person11] wearing sunglasses inside?’), 24% Activity (’What are [person1] and person[2] doing?“), 13% Temporal (”What will [person6] do after unpacking the groceries?"), 8% Mental, 7% Role, 5% Scene, 5% Hypothetical.</p>
<p>So in this setup, a model is provided a question, and has to pick the best answer out of four choices. Only one of the four is correct. If the model answered correctly a new question, along with the correct answer, is provided. Now the model has to justify it by picking the best rationale out of four choices. The first part is called Question Answering (<span class="math inline">\(Q\rightarrow A\)</span>) and the second part Answer Justification (<span class="math inline">\(QA\rightarrow R\)</span>). They combine both parts into a <span class="math inline">\(Q\rightarrow AR\)</span> metric, in which a model only gets a question right if it answers correctly and picks the right rationale. If it gets either the answer or the rationale wrong, the entire prediction will be wrong. Models are evaluated in terms of accuracy.</p>
<p>The results at the release were that humans find VCR easy (over 90% accuracy), and state-of-the-art vision models struggle (∼45%). At the moment of writing, the best model achieves 85.5 in (<span class="math inline">\(Q\rightarrow A\)</span>), 87.5 in (<span class="math inline">\(QA\rightarrow R\)</span>) and 74.9 in <span class="math inline">\(Q\rightarrow AR\)</span>. So the models are closing the gap but VCR is still far from solved. An “simpler” approach to evaluate vision-language models is to ask questions without reasoning about an image.</p>
</div>
<div id="visual-question-answering-1.0-2.0-vqa" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.3.2</span> Visual Question Answering 1.0 &amp; 2.0 (VQA)<a href="c01-00-intro-modalities.html#visual-question-answering-1.0-2.0-vqa" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>For this reason <span class="citation">Antol et al. (<a href="#ref-antol2015vqa" role="doc-biblioref">2015</a>)</span> created an open-ended answering task and a multiple-choice task. Their dataset contains roughly 250k images, 760k questions, and 10M answers. 204k images are taken from the MS COCO dataset but also newly created created datasets are used. Three questions were collected for each image or scene. Each question was answered by ten subjects along with their confidence. The dataset contains over 760K questions with around 10M answers. “what”-, “how”-, “is”- questions are mainly used in the benchmark. But they had major flaws in their creation. An model which blindly answering “yes” without reading the rest of the question or looking at the associated image results in a VQA accuracy of 87% or the most common sport answer “tennis” was the correct answer for 41% of the questions starting with “What sport is”, and “2” is the correct answer for 39% of the questions starting with “How many” <span class="citation">(Antol et al. <a href="#ref-antol2015vqa" role="doc-biblioref">2015</a>)</span>.</p>
<p><span class="citation">Zhang et al. (<a href="#ref-zhang2016yin" role="doc-biblioref">2016</a>)</span> pointed out a particular ‘visual priming bias’ in the VQA dataset. <span class="citation">Zhang et al. (<a href="#ref-zhang2016yin" role="doc-biblioref">2016</a>)</span> showed that language provides a strong prior that can result in good superficial performance, without the underlying models truly understanding the visual content. <span class="citation">Zhang et al. (<a href="#ref-zhang2016yin" role="doc-biblioref">2016</a>)</span> collected a balanced dataset containing pairs of complementary scenes to reduce or eliminate the strong prior of the language. <span class="citation">Goyal et al. (<a href="#ref-goyal2017making" role="doc-biblioref">2017</a>)</span> did the same and made a second iteration of the Visual Question Answering Dataset and Challenge (VQA v2.0). <span class="citation">Goyal et al. (<a href="#ref-goyal2017making" role="doc-biblioref">2017</a>)</span> balanced the popular VQA dataset <span class="citation">(Antol et al. <a href="#ref-antol2015vqa" role="doc-biblioref">2015</a>)</span> by collecting complementary images such that every question in balanced dataset is associated with not just a single image, but rather a pair of similar images that result in two different answers to the question. The dataset is by construction more balanced than the original VQA dataset and has approximately twice the number of image-question pairs.</p>
</div>
</div>
<div id="gqa" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.3.4</span> GQA<a href="c01-00-intro-modalities.html#gqa" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><span class="citation">Hudson and Manning (<a href="#ref-hudson2019gqa" role="doc-biblioref">2019</a>)</span> introduced the GQA dataset for real-world visual reasoning and compositional question answering. It consists of 113K images and 22M questions of assorted types and varying compositionality degrees, measuring performance on an array of reasoning skills such as object and attribute recognition, transitive relation tracking, spatial reasoning, logical inference and comparisons. They also proposed Consistency, Validity and Plausibility as new measures to get more insight into models’ behavior and performance. Consistency measures responses consistency across different questions. To achieve a high consistency a model may require deeper understanding of the question semantics in context of the image. The validity metric checks whether a given answer is in the question scope, e.g. responding some color to a color question. The plausibility score goes a step further, measuring whether the answer is reasonable, or makes sense, given the question (e.g. elephant usually do not eat pizza).</p>
<p>They even made a comparison between GQA and VQA 2.0. They came to the conclusion that the questions of GQA are objective, unambiguous, more compositional and can be answered from the images only, potentially making this benchmark more controlled and convenient for making research progress on. Conversely, VQA questions tend to be a bit more ambiguous and subjective, at times with no clear and conclusive answer. Finally, we can see that GQA provides more questions for each image and thus covers it more thoroughly than VQA.</p>
<div id="generative-benchmarks" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.4.1</span> Generative Benchmarks<a href="c01-00-intro-modalities.html#generative-benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Almost everybody is talking right now about generative models like DALL-E2, Imagen, Parti. It seems like every month a new one is presented. But how can we compare these models? Automatic image quality and automatic image-text alignment are two reasonable evaluation metrics. Fréchet Inception Distance (FID) can be used as primary automated metric for measuring image quality. The Frechet Inception Distance compares the distribution of generated images with the distribution of real images that were used to train the generator. A small value is wanted, as it’s a distance measure. Text-image fit can be captured through automated captioning evaluation. For this an image output by the model is captioned with a model, which is able to do image captioning. The similarity of the input prompt and the generated caption is then assessed via BLEU, CIDEr, METEOR and SPICE and also human evaluation is done. Here different generative models are used with the same prompts and the human is asked to choose which output is a higher quality image and which is a better match to the input prompt. One always has to keep in mind, that the images of the generative models are always “cherry picked”. They do not typically represent, for example, a single shot interaction in which the model directly produces such an image. To make this clear, <span class="citation">Yu et al. (<a href="#ref-parti" role="doc-biblioref">2022</a>)</span> showed their way of growing the cherry tree.</p>
<div class="figure">
<img src="figures/01-chapter1/Parti_Growing.png" alt="" />
<p class="caption">taken from Parti Paper</p>
</div>
</div>
<div id="partiprompts-drawbench-localized-narratives" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.4.2</span> PartiPrompts, DrawBench, Localized Narratives<a href="c01-00-intro-modalities.html#partiprompts-drawbench-localized-narratives" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>In a sense, this is a form of model whispering as one stretches such models to their limits. Besides to that they also present PartiPrompts (P2) which is a set of over 1600 (English) prompts curated to measure model capabilities across a variety of categories and controlled dimensions of difficulty. P2 prompts can be simple, but can also be complex, such as 67-word description they created for Vincent van Gogh’s The Starry Night. DrawBench is a similar dataset. Also the Localized Narratives dataset from the dataset section consists of long prompts and though it can also be used as a benchmark for generative models.</p>
<p>Current benchmarks give a good perspective on model performance on a wide range of V&amp;L tasks, but the field is only starting to assess why models perform so well and whether models learn specific capabilities that span multiple V&amp;L tasks.</p>
</div>
<div id="foil-it" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.4.3</span> FOIL it!<a href="c01-00-intro-modalities.html#foil-it" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><span class="citation">Shekhar et al. (<a href="#ref-shekhar2017foil" role="doc-biblioref">2017</a>)</span> proposed an automatic method for creating a large dataset of real images with minimal language bias and some diagnostic abilities. They extended the MS-COCO dataset and created FOIL-COCO. FOIL stands for “Find One mismatch between Image and Language caption” and consists of images associated with incorrect captions. The captions are produced by introducing one single error (or ‘foil’) per caption in existing, human-annotated data. So each datapoint FOIL-COCO can be described as triplet consisting of an image, original and foil caption. Their data generation process consists of four main steps:</p>
<ol style="list-style-type: decimal">
<li>Generation of replacement word pairs</li>
<li>Splitting of replacement pairs into training and testing</li>
<li>Generation of foil captions</li>
<li>Mining the hardest foil caption for each image</li>
</ol>
<p>The models are evaluated on three different tasks. The first one is Correct vs. foil classification. Given an image and a caption, the model is asked to mark whether the caption is correct or wrong. The aim is to understand whether LaVi models can spot mismatches between their coarse representations of language and visual input. The second task is Foil word detection. Given an image and a foil caption, the model has to detect the foil word. The aim is to evaluate the understanding of the system at the word level. The last task Foil word correction. Given an image, a foil caption and the foil word, the model has to detect the foil and provide its correction. The aim is to check whether the system’s visual representation is fine-grained enough to be able to extract the information necessary to correct the error. Their hypothesis is that systems which, like humans, deeply integrate the language and vision modalities, should spot foil captions quite easily.</p>
</div>
<div id="valse" class="section level5 hasAnchor">
<h5><span class="header-section-number">2.3.3.4.4</span> VALSE<a href="c01-00-intro-modalities.html#valse" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Vision And Language Structured Evaluation (VALSE) <span class="citation">(Parcalabescu et al. <a href="#ref-parcalabescu-etal-2022-valse" role="doc-biblioref">2022</a>)</span> builds on the same idea. This benchmark aims to gauge the sensitivity of pre-trained V&amp;L models to foiled instances. They coverd a wide spectrum of basic linguistic phenomena affecting the linguistic and visual modalities: existence, plurality, counting, spatial relations, actions, and entity coreference. To generate the foils they first use strong language models to propose foil and second they use natural language inference to filter out captions that still can describe the image. To do this in an automatic fashion they use the image as an premise and the caption its entailed hypothesis. Additionally they use the captian as an premise and the foil as the hypothesis. If an NLI model predicts the foil to be neutral or a contradiction with respect to the caption, they see this as an indicator for a good foil. At last the used human annotators to validate all generated testing data. Mainly the MS-COCO dataset is used. VALSE is as a task-independent, zero-shot benchmark to assess the extent to which models learn to ground specific linguistic phenomena as a consequence of their pretraining.</p>
</div>
</div>
<div id="other-benchmarks" class="section level4 hasAnchor">
<h4><span class="header-section-number">2.3.3.5</span> Other Benchmarks<a href="c01-00-intro-modalities.html#other-benchmarks" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>As we don’t live in a world with unlimited resources, it’s also important to keep track of how much energy is consumed to train the models and how big the carbon footprint is. <span class="citation">Strubell, Ganesh, and McCallum (<a href="#ref-strubell2019energy" role="doc-biblioref">2019</a><a href="#ref-strubell2019energy" role="doc-biblioref">a</a>)</span> investigated some NLP models and benchmarked model training and development costs in terms of dollars and estimated <span class="math inline">\(CO_2\)</span> emissions. They came to the result that training a single BERT base model without hyperparameter tuning on GPUs requires the same energy as a trans-American flight. On average a human is responsible for 5t <span class="math inline">\(CO_2\)</span> per year and <span class="citation">Strubell, Ganesh, and McCallum (<a href="#ref-strubell2019energy" role="doc-biblioref">2019</a><a href="#ref-strubell2019energy" role="doc-biblioref">a</a>)</span> estimated that the training procedure of a big Transformer with neural architecture search emitted 284t of <span class="math inline">\(CO_2\)</span>. Works <span class="citation">(Lottick et al. <a href="#ref-lottick2019energy" role="doc-biblioref">2019</a>, @henderson2020towards)</span> have released online tools to benchmark their energy usage and initiatives such as the <a href="https://sites.google.com/view/sustainlp2020/organization">SustainNLP workshop</a> have since taken up the goal of prioritizing computationally efficient hardware and algorithms. These findings are just some points one should keep in mind.</p>
<p>In the following chapters we will see how the multimodal architectures use these datasets and also how they perform on the given benchmarks.</p>

</div>
</div>
</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-antol2015vqa">
<p>Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. “Vqa: Visual Question Answering.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2425–33.</p>
</div>
<div id="ref-bandy2021addressing">
<p>Bandy, Jack, and Nicholas Vincent. 2021. “Addressing" Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for Bookcorpus.” <em>arXiv Preprint arXiv:2105.05241</em>.</p>
</div>
<div id="ref-beyer2020we">
<p>Beyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. “Are We Done with Imagenet?” <em>arXiv Preprint arXiv:2006.07159</em>.</p>
</div>
<div id="ref-birhane2021multimodal">
<p>Birhane, Abeba, Vinay Uday Prabhu, and Emmanuel Kahembwe. 2021. “Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes.” <em>arXiv Preprint arXiv:2110.01963</em>.</p>
</div>
<div id="ref-bowman2021will">
<p>Bowman, Samuel R, and George E Dahl. 2021. “What Will It Take to Fix Benchmarking in Natural Language Understanding?” <em>arXiv Preprint arXiv:2104.02145</em>.</p>
</div>
<div id="ref-bromley1993signature">
<p>Bromley, Jane, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. 1993. “Signature Verification Using a" Siamese" Time Delay Neural Network.” <em>Advances in Neural Information Processing Systems</em> 6.</p>
</div>
<div id="ref-brown2020language">
<p>Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.</p>
</div>
<div id="ref-SimCLR">
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. “A Simple Framework for Contrastive Learning of Visual Representations.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.05709">https://doi.org/10.48550/ARXIV.2002.05709</a>.</p>
</div>
<div id="ref-deng2009imagenet">
<p>Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In <em>2009 Ieee Conference on Computer Vision and Pattern Recognition</em>, 248–55. Ieee.</p>
</div>
<div id="ref-Devlin2018">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018c. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” October. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div id="ref-dhamala2021bold">
<p>Dhamala, Jwala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. “Bold: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation.” In <em>Proceedings of the 2021 Acm Conference on Fairness, Accountability, and Transparency</em>, 862–72.</p>
</div>
<div id="ref-gao2017knowledge">
<p>Gao, Jiyang, Zhen Li, Ram Nevatia, and others. 2017. “Knowledge Concentration: Learning 100k Object Classifiers in a Single Cnn.” <em>arXiv Preprint arXiv:1711.07607</em>.</p>
</div>
<div id="ref-Gokaslan2019OpenWeb">
<p>Gokaslan, Aaron, and Vanya Cohen. 2019. “OpenWebText Corpus.”</p>
</div>
<div id="ref-goodfellow2014explaining">
<p>Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. “Explaining and Harnessing Adversarial Examples.” <em>arXiv Preprint arXiv:1412.6572</em>.</p>
</div>
<div id="ref-goyal2017making">
<p>Goyal, Yash, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. “Making the V in Vqa Matter: Elevating the Role of Image Understanding in Visual Question Answering.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 6904–13.</p>
</div>
<div id="ref-grill2020bootstrap">
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020a. “Bootstrap Your Own Latent-a New Approach to Self-Supervised Learning.” <em>Advances in Neural Information Processing Systems</em> 33: 21271–84.</p>
</div>
<div id="ref-BYOL">
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020b. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2006.07733">https://doi.org/10.48550/ARXIV.2006.07733</a>.</p>
</div>
<div id="ref-guo2016ms">
<p>Guo, Yandong, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. 2016. “Ms-Celeb-1m: A Dataset and Benchmark for Large-Scale Face Recognition.” In <em>European Conference on Computer Vision</em>, 87–102. Springer.</p>
</div>
<div id="ref-he2022masked">
<p>He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. “Masked Autoencoders Are Scalable Vision Learners.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 16000–16009.</p>
</div>
<div id="ref-ResNet">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-hudson2019gqa">
<p>Hudson, Drew A, and Christopher D Manning. 2019. “Gqa: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6700–6709.</p>
</div>
<div id="ref-jia2021scaling">
<p>Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021a. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” In <em>International Conference on Machine Learning</em>, 4904–16. PMLR.</p>
</div>
<div id="ref-koehn2005europarl">
<p>Koehn, Philipp. 2005. “Europarl: A Parallel Corpus for Statistical Machine Translation.” In <em>Proceedings of Machine Translation Summit X: Papers</em>, 79–86.</p>
</div>
<div id="ref-kolesnikov2019large">
<p>Kolesnikov, Alexander, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. 2019. “Large Scale Learning of General Visual Representations for Transfer.” <em>arXiv Preprint arXiv:1912.11370</em> 2 (8).</p>
</div>
<div id="ref-lin2014microsoft">
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014a. “Microsoft Coco: Common Objects in Context.” In <em>European Conference on Computer Vision</em>, 740–55. Springer.</p>
</div>
<div id="ref-lottick2019energy">
<p>Lottick, Kadan, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. “Energy Usage Reports: Environmental Awareness as Part of Algorithmic Accountability.” <em>arXiv Preprint arXiv:1911.08354</em>.</p>
</div>
<div id="ref-mahajan2018exploring">
<p>Mahajan, Dhruv, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. 2018. “Exploring the Limits of Weakly Supervised Pretraining.” In <em>Proceedings of the European Conference on Computer Vision (Eccv)</em>, 181–96.</p>
</div>
<div id="ref-mayer2014creating">
<p>Mayer, Thomas, and Michael Cysouw. 2014. “Creating a Massively Parallel Bible Corpus.” <em>Oceania</em> 135 (273): 40.</p>
</div>
<div id="ref-redditUsers">
<p>MICHAEL BARTHEL, JESSE HOLCOMB, GALEN STOCKING, and AMY MITCHELL. 2016. “Reddit News Users More Likely to Be Male, Young and Digital in Their News Preferences.” 2016. <a href="https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/">https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/</a>.</p>
</div>
<div id="ref-unsupBrain">
<p>Mineault, Patrick. 2021. “Unsupervised Models of the Brain.” 2021. <a href="https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/">https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/</a>.</p>
</div>
<div id="ref-coco_eval">
<p>Mircosoft. 2019. “Evaluate:Detection.” 2019. <a href="https://cocodataset.org/#detection-eval">https://cocodataset.org/#detection-eval</a>.</p>
</div>
<div id="ref-parcalabescu-etal-2022-valse">
<p>Parcalabescu, Letitia, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. “VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena.” In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 8253–80. Association for Computational Linguistics. <a href="https://aclanthology.org/2022.acl-long.567">https://aclanthology.org/2022.acl-long.567</a>.</p>
</div>
<div id="ref-prabhu2020large">
<p>Prabhu, Vinay Uday, and Abeba Birhane. 2020. “Large Image Datasets: A Pyrrhic Win for Computer Vision?” <em>arXiv Preprint arXiv:2006.16923</em>.</p>
</div>
<div id="ref-radford2021learning">
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>International Conference on Machine Learning</em>, 8748–63. PMLR.</p>
</div>
<div id="ref-rajpurkar2018know">
<p>Rajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know What You Don’t Know: Unanswerable Questions for Squad.” <em>arXiv Preprint arXiv:1806.03822</em>.</p>
</div>
<div id="ref-rajpurkar2016squad">
<p>Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “Squad: 100,000+ Questions for Machine Comprehension of Text.” <em>arXiv Preprint arXiv:1606.05250</em>.</p>
</div>
<div id="ref-recht2019imagenet">
<p>Recht, Benjamin, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. “Do Imagenet Classifiers Generalize to Imagenet?” In <em>International Conference on Machine Learning</em>, 5389–5400. PMLR.</p>
</div>
<div id="ref-ribeiro2020beyond">
<p>Ribeiro, Marco Tulio, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. “Beyond Accuracy: Behavioral Testing of Nlp Models with Checklist.” <em>arXiv Preprint arXiv:2005.04118</em>.</p>
</div>
<div id="ref-shao2019objects365">
<p>Shao, Shuai, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. 2019. “Objects365: A Large-Scale, High-Quality Dataset for Object Detection.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 8430–9.</p>
</div>
<div id="ref-shekhar2017foil">
<p>Shekhar, Ravi, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017. “Foil It! Find One Mismatch Between Image and Language Caption.” <em>arXiv Preprint arXiv:1705.01359</em>.</p>
</div>
<div id="ref-sheng2019woman">
<p>Sheng, Emily, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. “The Woman Worked as a Babysitter: On Biases in Language Generation.” <em>arXiv Preprint arXiv:1909.01326</em>.</p>
</div>
<div id="ref-srinivasan2021wit">
<p>Srinivasan, Krishna, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021. “Wit: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine Learning.” In <em>Proceedings of the 44th International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 2443–9.</p>
</div>
<div id="ref-srivastava2022beyond">
<p>Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, et al. 2022. “Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models.” <em>arXiv Preprint arXiv:2206.04615</em>.</p>
</div>
<div id="ref-strubell2019energy">
<p>Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019a. “Energy and Policy Considerations for Deep Learning in Nlp.” <em>arXiv Preprint arXiv:1906.02243</em>.</p>
</div>
<div id="ref-EfficientNet">
<p>Tan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <a href="https://doi.org/10.48550/ARXIV.1905.11946">https://doi.org/10.48550/ARXIV.1905.11946</a>.</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017b. “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30.</p>
</div>
<div id="ref-wenzek2019ccnet">
<p>Wenzek, Guillaume, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2019. “Ccnet: Extracting High Quality Monolingual Datasets from Web Crawl Data.” <em>arXiv Preprint arXiv:1911.00359</em>.</p>
</div>
<div id="ref-xue2020mt5">
<p>Xue, Linting, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. “MT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer.” <em>arXiv Preprint arXiv:2010.11934</em>.</p>
</div>
<div id="ref-darkMatter">
<p>Yann, Lecun, and Misra Ishan. 2021. “Self-Supervised Learning: The Dark Matter of Intelligence.” 2021. <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</a>.</p>
</div>
<div id="ref-parti">
<p>Yu, Jiahui, Yuanzhong Xu, Jing Koh, Thang Luong, Gunjan Baid, Vijay Vasudevan, Alexander Ku, et al. 2022. “Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,” June. <a href="https://doi.org/10.48550/arXiv.2206.10789">https://doi.org/10.48550/arXiv.2206.10789</a>.</p>
</div>
<div id="ref-yuan2022wudaomm">
<p>Yuan, Sha, Zhao Shuai, Leng Jiahong, Xue Zhao, Zhao Hanyu, and Tang Jie. 2022. “WuDaoMM: A Large-Scale Multi-Modal Dataset for Pre-Training Models.” <em>arXiv Preprint arXiv:2203.11480</em>.</p>
</div>
<div id="ref-zellers2019recognition">
<p>Zellers, Rowan, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. “From Recognition to Cognition: Visual Commonsense Reasoning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6720–31.</p>
</div>
<div id="ref-zhang2016yin">
<p>Zhang, Peng, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016. “Yin and Yang: Balancing and Answering Binary Visual Questions.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 5014–22.</p>
</div>
<div id="ref-zhuang2021unsupervised">
<p>Zhuang, Chengxu, Siming Yan, Aran Nayebi, Martin Schrimpf, Michael C Frank, James J DiCarlo, and Daniel LK Yamins. 2021. “Unsupervised Neural Network Models of the Ventral Visual Stream.” <em>Proceedings of the National Academy of Sciences</em> 118 (3): e2014196118.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="c02-00-multimodal.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
