<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Conclusion | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Conclusion | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Conclusion | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="c03-00-further.html"/>
<link rel="next" href="epilogue.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Multimodal Deep Learning</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-to-multimodal-deep-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Multimodal Deep Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-01-sota-nlp"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in NLP</a></li>
<li class="chapter" data-level="2.2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-02-sota-cv"><i class="fa fa-check"></i><b>2.2</b> State-of-the-art in Computer Vision</a></li>
<li class="chapter" data-level="2.3" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-03-benchmarks"><i class="fa fa-check"></i><b>2.3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-01-img2text"><i class="fa fa-check"></i><b>3.1</b> Image2Text</a></li>
<li class="chapter" data-level="3.2" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-02-text2img"><i class="fa fa-check"></i><b>3.2</b> Text-2-image</a></li>
<li class="chapter" data-level="3.3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-03-img-support-text"><i class="fa fa-check"></i><b>3.3</b> Images supporting Language Models</a></li>
<li class="chapter" data-level="3.4" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-04-text-support-img"><i class="fa fa-check"></i><b>3.4</b> Text supporting computer vision models</a></li>
<li class="chapter" data-level="3.5" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-05-text-plus-img"><i class="fa fa-check"></i><b>3.5</b> Text + Image</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c03-00-further.html"><a href="c03-00-further.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-01-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a></li>
<li class="chapter" data-level="4.2" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-02-structured-unstructured"><i class="fa fa-check"></i><b>4.2</b> Structured + Unstructured Data</a></li>
<li class="chapter" data-level="4.3" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-03-multi-purpose"><i class="fa fa-check"></i><b>4.3</b> Multi-Purpose Models</a></li>
<li class="chapter" data-level="4.4" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-04-usecase"><i class="fa fa-check"></i><b>4.4</b> Generative Art</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>6</b> Epilogue</a></li>
<li class="chapter" data-level="7" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>7</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conclusion-1" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 5</span> Conclusion<a href="conclusion-1.html#conclusion-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Author: Nadja Sauter</em></p>
<p><em>Supervisor: Matthias Assenmacher</em></p>
<p>It is very impressive how multimodal architectures developed especially over the course of the last two years. Particularly, methods to generate pictures based on text prompts like DALL-E became incredibly good in their job. A lot of people are fascinated by the stunning results and a huge hype about these AI generated images evolved in the internet, especially on twitter. In this way, the models were not only investigated by researchers but also by the AI online Community (e.g. Katherine Crowson alias <a href="https://twitter.com/RiversHaveWings">Rivers Have Wings</a>). Even in the art scene these methods attracted a lot of attention as shown in our use case “Generative Arts” (subsection <a href="c03-00-further.html#c03-04-usecase">4.4</a>). Apart from that, it is possible to deploy these methods commercially, for instance in the film production or gaming industry (e.g. creating characters for games). However, this also results into problems of copyright.</p>
<p>It is impressive how realistic and precise outputs are achieved by such architectures. On the other hand, these methods can also be abused to spread misleading information as it is often very difficult to distinguish between a fake or a real picture by only looking at it. This can be systematically used to manipulate the public opinion by spreading AI manipulated media, also called deep fakes. That’s why researchers like <span class="citation">Joshi, Walambe, and Kotecha (<a href="#ref-explainaility" role="doc-biblioref">2021</a>)</span> demand automated tools which are capable of detecting these fabrications. Apart from taht, like most Deep Learning models, multimodal architectures are not free from bias which also needs to be investigated further <span class="citation">(Esser, Rombach, and Ommer <a href="#ref-bias" role="doc-biblioref">2020</a>)</span>. Besides, the algorithms are very complex which is why they are called “black-box” models, meaning that one cannot directly retrace how the model came to a certain solution or decision. This may limit their social acceptance and usability as the underlying process is not credible and transparent enough <span class="citation">(Joshi, Walambe, and Kotecha <a href="#ref-explainaility" role="doc-biblioref">2021</a>)</span>. For instance, in medical applications like e.g. predicting the presence or absence of cancer, apart from the decision of the AI the reasoning and the certainty are highly relevant for doctors and patients.</p>
<p>Furthermore, there is a clear trend in recent years to build more and more complex architectures in order to achieve higher performance. For instance OpenAI’s language model GPT-2 has about 1.5 Billion parameter <span class="citation">(Radford, Wu, et al. <a href="#ref-Radford2019LanguageMA" role="doc-biblioref">2019</a><a href="#ref-Radford2019LanguageMA" role="doc-biblioref">a</a>)</span> whereas its successor GPT-3 has about 175 Billion parameters <span class="citation">(T. B. Brown et al. <a href="#ref-GPT3" role="doc-biblioref">2020</a>)</span>. Increasing the number of parameters often helps to improve model performance, but all of these parameters need to be trained and stored which takes a lot of time, enormous computational power and storage. For example, training GPT-2 took about one week (168 hours) of training on 32 TPUv3 chips <span class="citation">(Strubell, Ganesh, and McCallum <a href="#ref-environment" role="doc-biblioref">2019</a><a href="#ref-environment" role="doc-biblioref">b</a>)</span>. The researchers <span class="citation">Strubell, Ganesh, and McCallum (<a href="#ref-environment" role="doc-biblioref">2019</a><a href="#ref-environment" role="doc-biblioref">b</a>)</span> estimated that the cloud compute costs for training GPT-2 add up to about $12,902–$43,008. Apart from the enormous expenses, this also contributes to our environmental burden as this process is really energy intensive. Due to missing power draw data on GPT-2’s training hardware, the researchers weren’t able to calculate the CO<sub>2</sub> emission. However, for the popular BERT architecture with 110M parameters they calculated cloud compute costs of $3,751-$12,571, energy consumption of 1,507 kWh and a Carbon footprint of 1,438 lbs of CO<sub>2</sub>. In comparison, the footprint of flying from New York to San Francisco by plane for one passenger is about 1,984 lbs of CO<sub>2</sub>. In conclusion training BERT once results in almost the same footprint as this long-haul flight. On top of this, these numbers are only for one training run. Developing a new model or adapting it often takes several fitting and tuning phases.</p>
<p>Moreover, the computational power as well as the necessary hardware, technology and financial means to run these models can only be provided by big technology companies like e.g. Google, Facebook or Open AI. This results in a disparate access between researchers in academia versus industry. Furthermore, the companies often do not publish their models as they are their ‘product’ and intellectual property. In this way it is not possible to reproduce their work and findings independently. Besides, from an economic point of view, this may be the foundation of a monopoly which can be dangerous for economic competition and could abused by the Tech Giants.</p>

</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-GPT3">
<p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2005.14165">https://doi.org/10.48550/ARXIV.2005.14165</a>.</p>
</div>
<div id="ref-bias">
<p>Esser, Patrick, Robin Rombach, and Björn Ommer. 2020. “A Note on Data Biases in Generative Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2012.02516">https://doi.org/10.48550/ARXIV.2012.02516</a>.</p>
</div>
<div id="ref-explainaility">
<p>Joshi, Gargi, Rahee Walambe, and Ketan Kotecha. 2021. “A Review on Explainability in Multimodal Deep Neural Nets.” <em>IEEE Access</em> 9: 59800–59821. <a href="https://doi.org/10.1109/ACCESS.2021.3070212">https://doi.org/10.1109/ACCESS.2021.3070212</a>.</p>
</div>
<div id="ref-Radford2019LanguageMA">
<p>Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. “Language Models Are Unsupervised Multitask Learners.” In.</p>
</div>
<div id="ref-environment">
<p>Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019b. “Energy and Policy Considerations for Deep Learning in Nlp.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1906.02243">https://doi.org/10.48550/ARXIV.1906.02243</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="c03-00-further.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="epilogue.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
