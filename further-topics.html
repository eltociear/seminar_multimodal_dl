<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Further Topics | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Further Topics | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Further Topics | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="multimodal-architectures.html"/>
<link rel="next" href="title.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multimodal Deep Learning></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="0.1" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i><b>0.1</b> Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#state-of-the-art-in-computer-vision"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in computer vision</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#history"><i class="fa fa-check"></i><b>2.1.1</b> History</a></li>
<li class="chapter" data-level="2.1.2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#supervised-and-unsupervised-learning"><i class="fa fa-check"></i><b>2.1.2</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#resnet"><i class="fa fa-check"></i><b>2.1.3</b> ResNet</a></li>
<li class="chapter" data-level="2.1.4" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#efficientnet"><i class="fa fa-check"></i><b>2.1.4</b> EfficientNet</a></li>
<li class="chapter" data-level="2.1.5" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#simclr"><i class="fa fa-check"></i><b>2.1.5</b> SimCLR</a></li>
<li class="chapter" data-level="2.1.6" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#bootstrap-your-own-latent-byol"><i class="fa fa-check"></i><b>2.1.6</b> Bootstrap Your Own Latent (BYOL)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks"><i class="fa fa-check"></i><b>2.2</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#datasets"><i class="fa fa-check"></i><b>2.2.1</b> Datasets</a></li>
<li class="chapter" data-level="2.2.2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#pre-training-tasks"><i class="fa fa-check"></i><b>2.2.2</b> Pre-Training Tasks</a></li>
<li class="chapter" data-level="2.2.3" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#benchmarks"><i class="fa fa-check"></i><b>2.2.3</b> Benchmarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#img2text"><i class="fa fa-check"></i><b>3.1</b> img2text</a><ul>
<li class="chapter" data-level="3.1.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#microsoft-coco-common-objects-in-context"><i class="fa fa-check"></i><b>3.1.1</b> 2.1.1 Microsoft COCO: Common Objects in Context</a></li>
<li class="chapter" data-level="3.1.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#meshed-memory-transformer-for-image-captioning-m2"><i class="fa fa-check"></i><b>3.1.2</b> 2.1.2 Meshed-Memory Transformer for Image Captioning (<span class="math inline">\(M^2\)</span>)</a></li>
<li class="chapter" data-level="3.1.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#seeking-objectivity"><i class="fa fa-check"></i><b>3.1.3</b> Seeking objectivity</a></li>
<li class="chapter" data-level="3.1.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>3.1.4</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="3.1.5" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#dall-e-starting-post-gan-era"><i class="fa fa-check"></i><b>3.1.5</b> Dall-E starting post-GAN era</a></li>
<li class="chapter" data-level="3.1.6" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#glide"><i class="fa fa-check"></i><b>3.1.6</b> GLIDE</a></li>
<li class="chapter" data-level="3.1.7" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#dall-e-2"><i class="fa fa-check"></i><b>3.1.7</b> Dall-E 2</a></li>
<li class="chapter" data-level="3.1.8" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#imagen"><i class="fa fa-check"></i><b>3.1.8</b> Imagen</a></li>
<li class="chapter" data-level="3.1.9" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#parti"><i class="fa fa-check"></i><b>3.1.9</b> Parti</a></li>
<li class="chapter" data-level="3.1.10" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#open-source-community"><i class="fa fa-check"></i><b>3.1.10</b> Open-Source Community</a></li>
<li class="chapter" data-level="3.1.11" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#discussion"><i class="fa fa-check"></i><b>3.1.11</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#images-supporting-language-models"><i class="fa fa-check"></i><b>3.2</b> Images supporting language models</a><ul>
<li class="chapter" data-level="3.2.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#words-in-non-symbolic-contexts"><i class="fa fa-check"></i><b>3.2.1</b> Words In (Non-Symbolic) Contexts</a></li>
<li class="chapter" data-level="3.2.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#word-embeddings-survival-kit"><i class="fa fa-check"></i><b>3.2.2</b> Word-Embeddings: Survival-Kit</a></li>
<li class="chapter" data-level="3.2.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-beginning-sequential-multimodal-embeddings"><i class="fa fa-check"></i><b>3.2.3</b> The Beginning: Sequential Multimodal Embeddings</a></li>
<li class="chapter" data-level="3.2.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-grounded-space"><i class="fa fa-check"></i><b>3.2.4</b> The Grounded Space</a></li>
<li class="chapter" data-level="3.2.5" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-transformers-era"><i class="fa fa-check"></i><b>3.2.5</b> The Transformers Era</a></li>
<li class="chapter" data-level="3.2.6" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#was-it-worth"><i class="fa fa-check"></i><b>3.2.6</b> Was It Worth?</a></li>
<li class="chapter" data-level="3.2.7" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-end-of-this-story"><i class="fa fa-check"></i><b>3.2.7</b> The End Of This Story</a></li>
<li class="chapter" data-level="3.2.8" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#appendix-selected-models---summary"><i class="fa fa-check"></i><b>3.2.8</b> Appendix: Selected Models - Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#text-supporting-computer-vision-models"><i class="fa fa-check"></i><b>3.3</b> Text supporting computer vision models</a><ul>
<li class="chapter" data-level="3.3.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#intro"><i class="fa fa-check"></i><b>3.3.1</b> Intro</a></li>
<li class="chapter" data-level="3.3.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#concepts"><i class="fa fa-check"></i><b>3.3.2</b> Concepts</a></li>
<li class="chapter" data-level="3.3.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#contrastive-loss"><i class="fa fa-check"></i><b>3.3.3</b> Contrastive loss</a></li>
<li class="chapter" data-level="3.3.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#clip"><i class="fa fa-check"></i><b>3.3.4</b> CLIP</a></li>
<li class="chapter" data-level="3.3.5" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#align"><i class="fa fa-check"></i><b>3.3.5</b> ALIGN</a></li>
<li class="chapter" data-level="3.3.6" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#florence"><i class="fa fa-check"></i><b>3.3.6</b> Florence</a></li>
<li class="chapter" data-level="3.3.7" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#performance-comparison"><i class="fa fa-check"></i><b>3.3.7</b> Performance comparison</a></li>
<li class="chapter" data-level="3.3.8" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#resources"><i class="fa fa-check"></i><b>3.3.8</b> Resources</a></li>
<li class="chapter" data-level="3.3.9" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#outlook"><i class="fa fa-check"></i><b>3.3.9</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#text-image"><i class="fa fa-check"></i><b>3.4</b> Text + Image</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#todo"><i class="fa fa-check"></i><b>3.4.1</b> Todo</a></li>
<li class="chapter" data-level="3.4.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#challenges-in-ai"><i class="fa fa-check"></i><b>3.4.2</b> challenges in AI</a></li>
<li class="chapter" data-level="3.4.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#vilbert"><i class="fa fa-check"></i><b>3.4.3</b> vilbert</a></li>
<li class="chapter" data-level="3.4.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#flamingo-alayrac2022flamingo"><i class="fa fa-check"></i><b>3.4.4</b> flamingo <span class="citation">(<span class="citeproc-not-found" data-reference-id="alayrac2022flamingo"><strong>???</strong></span>)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="further-topics.html"><a href="further-topics.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="further-topics.html"><a href="further-topics.html#including-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a><ul>
<li class="chapter" data-level="4.1.1" data-path="further-topics.html"><a href="further-topics.html#intro-1"><i class="fa fa-check"></i><b>4.1.1</b> Intro</a></li>
<li class="chapter" data-level="4.1.2" data-path="further-topics.html"><a href="further-topics.html#motivation"><i class="fa fa-check"></i><b>4.1.2</b> Motivation</a></li>
<li class="chapter" data-level="4.1.3" data-path="further-topics.html"><a href="further-topics.html#taxonomy-of-multimodal-challenges"><i class="fa fa-check"></i><b>4.1.3</b> Taxonomy of Multimodal Challenges</a></li>
<li class="chapter" data-level="4.1.4" data-path="further-topics.html"><a href="further-topics.html#general-multimodal-architectures"><i class="fa fa-check"></i><b>4.1.4</b> General Multimodal Architectures</a></li>
<li class="chapter" data-level="4.1.5" data-path="further-topics.html"><a href="further-topics.html#multimodal-training-paradigms"><i class="fa fa-check"></i><b>4.1.5</b> Multimodal Training Paradigms</a></li>
<li class="chapter" data-level="4.1.6" data-path="further-topics.html"><a href="further-topics.html#combining-general-architectures-and-training-paradigms"><i class="fa fa-check"></i><b>4.1.6</b> Combining General Architectures and Training Paradigms</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="further-topics.html"><a href="further-topics.html#strucutered-unstrucutered-data"><i class="fa fa-check"></i><b>4.2</b> Strucutered + Unstrucutered Data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="further-topics.html"><a href="further-topics.html#intro-2"><i class="fa fa-check"></i><b>4.2.1</b> Intro</a></li>
<li class="chapter" data-level="4.2.2" data-path="further-topics.html"><a href="further-topics.html#taxonomy-structured-vs.-unstructured-data"><i class="fa fa-check"></i><b>4.2.2</b> Taxonomy: Structured vs. Unstructured Data</a></li>
<li class="chapter" data-level="4.2.3" data-path="further-topics.html"><a href="further-topics.html#fusion-strategies"><i class="fa fa-check"></i><b>4.2.3</b> Fusion Strategies</a></li>
<li class="chapter" data-level="4.2.4" data-path="further-topics.html"><a href="further-topics.html#applications-of-multimodal-dl"><i class="fa fa-check"></i><b>4.2.4</b> Applications of Multimodal DL</a></li>
<li class="chapter" data-level="4.2.5" data-path="further-topics.html"><a href="further-topics.html#multimodal-dl-in-survival"><i class="fa fa-check"></i><b>4.2.5</b> Multimodal DL in Survival</a></li>
<li class="chapter" data-level="4.2.6" data-path="further-topics.html"><a href="further-topics.html#traditional-survival-analysis-cox-proportional-hazard-model"><i class="fa fa-check"></i><b>4.2.6</b> Traditional Survival Analysis (Cox Proportional Hazard Model)</a></li>
<li class="chapter" data-level="4.2.7" data-path="further-topics.html"><a href="further-topics.html#deepconvsurvdeepcorrsurv"><i class="fa fa-check"></i><b>4.2.7</b> DeepConvSurv+DeepCorrSurv</a></li>
<li class="chapter" data-level="4.2.8" data-path="further-topics.html"><a href="further-topics.html#concat-cross-auto-encoders"><i class="fa fa-check"></i><b>4.2.8</b> Concat + Cross Auto Encoders</a></li>
<li class="chapter" data-level="4.2.9" data-path="further-topics.html"><a href="further-topics.html#cheerla-and-gevaert-2019"><i class="fa fa-check"></i><b>4.2.9</b> Cheerla and Gevaert (2019)</a></li>
<li class="chapter" data-level="4.2.10" data-path="further-topics.html"><a href="further-topics.html#multimodal-dl-in-economics"><i class="fa fa-check"></i><b>4.2.10</b> Multimodal DL in Economics</a></li>
<li class="chapter" data-level="4.2.11" data-path="further-topics.html"><a href="further-topics.html#critical-assessment"><i class="fa fa-check"></i><b>4.2.11</b> Critical Assessment</a></li>
<li class="chapter" data-level="4.2.12" data-path="further-topics.html"><a href="further-topics.html#conclusion-and-outlook"><i class="fa fa-check"></i><b>4.2.12</b> Conclusion and Outlook</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="further-topics.html"><a href="further-topics.html#multi-purpose-models"><i class="fa fa-check"></i><b>4.3</b> Multi-purpose Models</a><ul>
<li class="chapter" data-level="4.3.1" data-path="further-topics.html"><a href="further-topics.html#intro-3"><i class="fa fa-check"></i><b>4.3.1</b> Intro</a></li>
<li class="chapter" data-level="4.3.2" data-path="further-topics.html"><a href="further-topics.html#introduction-1"><i class="fa fa-check"></i><b>4.3.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3.3" data-path="further-topics.html"><a href="further-topics.html#todo-1"><i class="fa fa-check"></i><b>4.3.3</b> TODO</a></li>
<li class="chapter" data-level="4.3.4" data-path="further-topics.html"><a href="further-topics.html#previous-work"><i class="fa fa-check"></i><b>4.3.4</b> Previous Work</a></li>
<li class="chapter" data-level="4.3.5" data-path="further-topics.html"><a href="further-topics.html#todo-2"><i class="fa fa-check"></i><b>4.3.5</b> TODO</a></li>
<li class="chapter" data-level="4.3.6" data-path="further-topics.html"><a href="further-topics.html#pathway-proposal"><i class="fa fa-check"></i><b>4.3.6</b> Pathway Proposal</a></li>
<li class="chapter" data-level="4.3.7" data-path="further-topics.html"><a href="further-topics.html#todo-3"><i class="fa fa-check"></i><b>4.3.7</b> TODO</a></li>
<li class="chapter" data-level="4.3.8" data-path="further-topics.html"><a href="further-topics.html#discussion-1"><i class="fa fa-check"></i><b>4.3.8</b> Discussion</a></li>
<li class="chapter" data-level="4.3.9" data-path="further-topics.html"><a href="further-topics.html#todo-4"><i class="fa fa-check"></i><b>4.3.9</b> TODO</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="title.html"><a href="title.html"><i class="fa fa-check"></i><b>5</b> title</a></li>
<li class="chapter" data-level="6" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a></li>
<li class="chapter" data-level="7" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>7</b> Epilogue</a></li>
<li class="chapter" data-level="8" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>8</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="further-topics" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 4</span> Further Topics<a href="further-topics.html#further-topics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Authors: Marco Moldovan, Rickmer Schulte, Philipp Koch</em></p>
<p><em>Supervisor: Rasmus Hvingelby</em></p>
<p>So far we have learned about multimodal models for text and 2D images. Text and images can be seen as merely snapshots of the sensory stimulus that we humans perceive constantly. If we view the research field of multimodal deep learning as a means to approach human-level capabilities of perceiving and processing real-world signals then we have to consider lots of other modalities in a trainable model other than textual representation of language or static images. Besides introducing further modalities that are frequently encountered in muli-modal deep learning, the following chapter will also aim to bridge the gap between the two fundamental sources of data, namely structured and unstructured data. Investigating modeling approaches from both classical statistics and more recent deep learning we will examine the strengths and weaknesses of those and will discover that a combination of both may be a promising path for future research. Going from multi modalities to multi task, the last section will then broaden our view of multi-modal deep learning by examining multi purpose modals. Discussing cutting-edge research topics such as the newly developed Pathways, we will discuss current achievements and limitations of the new modeling and hardware approaches that might lead our way towards the ultimate goal of AGI in multi-modal deep learning.</p>

<div id="including-further-modalities" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.1</span> Including Further Modalities<a href="further-topics.html#including-further-modalities" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: Marco Moldovan</em></p>
<p><em>Supervisor: Rasmus Hvingelby</em></p>
<div id="intro-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.1</span> Intro<a href="further-topics.html#intro-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this chapter we will build up a taxonomy of different perceivable and interpretable types of signals that we as humans use to navigate the world and we will see how today’s state-of-the-art multimodal models are built and trained in order to process more and more modalities simultaneously in order to build more and more complete representations of world through available data. We will build up our taxonomy starting from the two most well-understood modalities - namely text and 2D images - and introduce models that learn relationships between increasingly many modalities at the same time and to map them to a cross-modal representation space in which we can apply distance functions to points in order to represent semantic relatedness between datapoint from these different modalities.
Given such a learned cross-modal representation space we will look at some of the most important multimodal downstream tasks and applications.</p>
<p>Towards the end of the chapter we will take a closer look at the two main types of model architectures and training paradigms: bi-encoders and “true” multimodal cross-encoders. The first kind of model can be seen as an ensemble of unimodal expert models that map into the same representation space while using some form of metric learning to relate representations of different modalities to one another. True multimodal models are essentially agnostic to their input (as long as it is preprocessed and featurized appropriately). We currently see the second kind of architecture as the more promising one for the case of approximating human-level perception. An example of a modality agnostic multimodal model is the Perceiver of which we will introduce a newer, even more efficient variant.</p>
<p>Up until recently each modality required their own specific self-supervised training paradigm: for text a common approach would be MLM while the same training paradigm wasn’t as effective for images or video. data2vec introduces a modality-agnostic SSL masked prediction setup which requires careful preprocessing but does not care about the source of the input. We see a model that marries a modality-agnostic model like Perceiver with a modality agnostic training paradigm like data2vec as a very promising path forward.
Topic 11 will build on this idea of modality-agnostic models by introducing Google’s Pathways: a concept for multimodal, multi-task, sparse world models.</p>
</div>
<div id="motivation" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.2</span> Motivation<a href="further-topics.html#motivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>World is inherently multimodal, images and text are just discrete snapshots while we as humans perceive lots of continuous multimodal signals.</li>
<li>We can extend the ideas and intuitions of image-text multimodal learning to include more modalities.</li>
<li>Listing research that includes continously more modalities would get out of hand quickly and seems unstructured: If we were to consider all possible learnable permutations of signal types we could go on forever.</li>
</ul>
</div>
<div id="taxonomy-of-multimodal-challenges" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.3</span> Taxonomy of Multimodal Challenges<a href="further-topics.html#taxonomy-of-multimodal-challenges" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Instead we want to build a taxonomy for multimodal machine learning that is based on challenges instead of modalities.</li>
<li>Viewing multimodal learning from the perspective of challenges is more generic and intuitive.</li>
<li>Once we understand the challenges we will see that real-world problems and their solutions will arise naturally.</li>
<li>The taxonomy will act as a blueprint for approaching multimodal learning challenges. For each category we will introduce some examples that apply a mixture of diverse modalities to a model.</li>
<li>We hope that the reader will understand that the different modalities are in priciple interchangable and that he/she will be able to apply the correct framework to their own multimodal problem.</li>
</ul>
<div id="multimodal-representation-learning" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.1.3.1</span> Multimodal Representation Learning<a href="further-topics.html#multimodal-representation-learning" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Representation learning lies at the base of solving most learning problems today, including for multimodal learning problems.</li>
<li>Dense representations are commonly learnt by deep neural networks like we’ve seen in the previous chapters.</li>
<li>If we want to generalize this notion to an arbitrary number of modalities we have to be clearn about the type of function that we want to learn and the kind of Representation we want to project to.</li>
</ul>
<div id="joint-representations" class="section level5 hasAnchor">
<h5><span class="header-section-number">4.1.3.1.1</span> Joint Representations<a href="further-topics.html#joint-representations" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Different modalities live in the same representation space.</li>
<li>Given a multimodal signal one needs to learna model that “fuses” these modalities in order to learn a joint representation of these input signals.</li>
<li>Typically modalities are somehow concatenated as an input and are then fed into a model that is constructed such as to learn a joint representation.</li>
</ul>
</div>
<div id="coordinated-representation" class="section level5 hasAnchor">
<h5><span class="header-section-number">4.1.3.1.2</span> Coordinated Representation<a href="further-topics.html#coordinated-representation" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>Given input signals of different modalities we can learn a class of models that each projects a single modality into its own space.</li>
<li>One model will typically receive one modality.</li>
<li>For learning joint representations we have to define a training paradigm that will learn to coordinate these different representation spaces by placeing semantically similar representations close to each other while maximizing the distance between semantically different representations.</li>
<li>Essentially one learns to align different representation spaces to each other.</li>
<li>Contrastive learning is a popular paradigm for learning joint representations.</li>
</ul>
</div>
</div>
<div id="multimodal-translation" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.1.3.2</span> Multimodal Translation<a href="further-topics.html#multimodal-translation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Given a signal in one modality we want to return a semantically equivalent output in a different modality.</li>
<li>E.g. give a text input and retrieve a speech segment.</li>
<li>E.g. provide a video and return a text description of the video</li>
<li>Translation can be retrieval-based or generative. I.e. either return an existing datapoint or sample and synthesize a new one.</li>
<li>Clip is classic example (already known)</li>
<li>DALL-E is generative translation model</li>
<li>NÜWA even more general across modalities</li>
<li>VideoCLIP for video retrieval</li>
<li>SpeechBERT for text to speech retrieval</li>
</ul>
</div>
<div id="multimodal-alignment" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.1.3.3</span> Multimodal Alignment<a href="further-topics.html#multimodal-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Multimodal alignment is the challenge of how exactly to learn coordinated representation spaces.</li>
<li>VATT learns coordinated space contrastively. Can even share weights between modalities to serve as one-model-fits-all for multimodal alignment.</li>
<li>Alternative: masked multimodal autoencoding with MultiMAE.</li>
</ul>
</div>
<div id="multimodal-fusion" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.1.3.4</span> Multimodal Fusion<a href="further-topics.html#multimodal-fusion" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Multimodal fusion is the challenge of how to learn a joint representation space.</li>
<li>Where in the model does fusion happen? Early, late or hybrid fusion possible. What are the advantages and disadvantages of each approach?</li>
<li>Introduce Multimodal Bottleneck Transformer (MBT)</li>
</ul>
</div>
</div>
<div id="general-multimodal-architectures" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.4</span> General Multimodal Architectures<a href="further-topics.html#general-multimodal-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Introduce architectures that are general enough to be applied to most/any multimodal problem</li>
<li>NÜWA 3D Nearby Attention can take text, audio, images and video as input: data has to first be encoded into this format batch size x time x height x width x embedding size. Not necesserally suitable for joint representations but can serve as a general modality-agnostic encoder for coordinated representations. Time dimension can be replaced with channel or depth dimensions if one wants to encode more exotic modalities. Preserves locality in data.</li>
<li>Perceiver and Perceiver IO require almost no preprocessing and can read extremely long sequences of data by cross-attending between data and modality specific learnable latent array.</li>
<li>Hiearchical Perceiver is follow-up that can preserve locality and compositionality in data (very important).</li>
</ul>
</div>
<div id="multimodal-training-paradigms" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.5</span> Multimodal Training Paradigms<a href="further-topics.html#multimodal-training-paradigms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Present training paradigms of how to train multimodal modal with SSL.</li>
</ul>
<div id="modality-agnostic-uni-modal-ssl" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.1.5.1</span> Modality-Agnostic Uni-Modal SSL<a href="further-topics.html#modality-agnostic-uni-modal-ssl" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>data2vec is unimodal SSL paradigm but it’s completely modality agnostic.</li>
</ul>
</div>
<div id="generalized-cross-modal-ssl" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.1.5.2</span> Generalized Cross-Modal SSL<a href="further-topics.html#generalized-cross-modal-ssl" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Here we introduce methods for true multimodal SSL.</li>
<li>Have to seperate into contrastive and non-contrastive methods</li>
<li>Generalize as much as possible: are there any approaches that solve alignment and fusion at the same time?</li>
</ul>
<div id="contrastive-methods" class="section level5 hasAnchor">
<h5><span class="header-section-number">4.1.5.2.1</span> Contrastive Methods<a href="further-topics.html#contrastive-methods" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>VATT -&gt; look for similar</li>
</ul>
</div>
<div id="non-contrastive-methods" class="section level5 hasAnchor">
<h5><span class="header-section-number">4.1.5.2.2</span> Non-Contrastive Methods<a href="further-topics.html#non-contrastive-methods" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<ul>
<li>MultiMAE -&gt; look for similar</li>
<li>Can data2vec work with multiple modalities?</li>
</ul>
</div>
</div>
</div>
<div id="combining-general-architectures-and-training-paradigms" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.6</span> Combining General Architectures and Training Paradigms<a href="further-topics.html#combining-general-architectures-and-training-paradigms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Future research: combining general architectures like Perceiver with contrastive methods like VATT, data2vec or MultiMAE.</li>
</ul>

</div>
</div>
<div id="strucutered-unstrucutered-data" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.2</span> Strucutered + Unstrucutered Data<a href="further-topics.html#strucutered-unstrucutered-data" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: Rickmer Schulte</em></p>
<p><em>Supervisor: Daniel Schalk</em></p>
<div id="intro-2" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.1</span> Intro<a href="further-topics.html#intro-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While the previous chapter has extended the range of modalities considered in multimodal deep learning beyond image and text data, the focus remained on other sorts of unstructured data. This has neglected the broad class of structured data, which has been the basis for research in pre deep learning eras and which has given rise to many fundamental modeling approaches in statistics and classical machine learning. Hence, the following chapter will aim to give an overview of both data sources and outline the respective ways these have been used for modeling purposes as well as more recent attempts to model them jointly.</p>
<p>Generally, structured and unstructured data substantially differ in certain aspects such as dimensionality and interpretability which have led to various modeling approaches that are particularly designed for the special characteristics of the data types, respectively. As shown in previous chapters, deep learning models such as neural networks are known to work well on unstructured data due to their ability to extract latent representation and to learn complex dependencies from unstructured data sources to achieve state-of-the art performance on many classification and prediction tasks. By contrast, classical statistical models are mostly applied to tabular data due the advantage of interpretability inherent to these models, which is commonly of great interest in many research fields. However, as more and more data has become available to researchers today, they often do not only have one sort of data modality at hand but both structured and unstructured data at the same time. Discarding one or the other data modality makes it likely to miss out on valuable insights and potential performance improvements.</p>
<p>Therefore, the following chapter will mainly investigate different proposed methods to model both data types jointly and examine similarities and differences between those. Besides classical methods such as feature engineering to integrate unstructured data via expert knowledge into the classical model framework, end-to-end learning techniques as well as different fusion procedures to integrate both types of modalities into common deep learning architectures are analyzed and evaluated. Especially the latter will be explored in detail by referring to numerous examples from survival analysis, finance and economics.
Finally, the chapter will conclude with a critical assessment of recent research for combining structured and unstructured data in multimodal DL, highlighting lacking steps that are required by following research as well as giving an outlook on future developments in the field.</p>
</div>
<div id="taxonomy-structured-vs.-unstructured-data" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.2</span> Taxonomy: Structured vs. Unstructured Data<a href="further-topics.html#taxonomy-structured-vs.-unstructured-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In order to have a clear setup for the remaining chapter, we will start off with a brief taxonomy of data types that will be encountered. Structured data, normally stored in some tabular form, has been the main research object in classical scientific fields. Whenever there was unstructured data involved, this was normally transformed into a structured form in a informed manner. Typically, this was done via expert-knowledge or data reduction techniques such as PCA prior to further statistical analysis. However, DL has enabled unsupervised extraction of features from unstructured data and thus to incorporate this kind of data in the models directly. Classical examples of unstructured data are image, text, video, and audio data as shown in Figure 1. Of these, the use of image and textual data together with tabular data will be examined along various examples later in the chapter. While the previous data types allowed for a clear distinction, lines can become increasingly blurred. For example, the record of few selected biomarkers or genes from patients would be regarded as structured data and normally be analyzed with classical statistical models. On the contrary, having the records of multiple thousand biomarkers or genes would rather be regarded as unstructured data and usually be analyzed via DL techniques. Thus, the distinction between structured and unstructured data does not only follow along the line of dimensionality but also concerns regarding the interpretability of single features within the data.</p>
<p>&lt;Figure1: Structured vs. Unstructured Data &gt;</p>
</div>
<div id="fusion-strategies" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.3</span> Fusion Strategies<a href="further-topics.html#fusion-strategies" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After we have classified the different data types that we will be dealing with, we will now proceed with fusion strategies that are used to merge data modalities into a single model. While there are potentially many ways to fuse data modalities, a distinction between three different strategies, namely early, joint and late fusion has been made in the literature. Here we follow along the taxonomy laid out by Huang et al. (2020) with a few generalizations as this is sufficient in our context.</p>
<p>Early fusion refers to the procedure of merging data modalities into a feature vector prior to feeding it into the model. The data that is being fused can be raw or preprocessed data. The step of preprocessing usually involves dimensionality reduction to algin dimensions of the model input data. This can be done by training a separate DNN, using data driven transformations such as PCA or directly via expert knowledge. Besides using domain expertise to feed only regions of interest of e.g an image to the model, sampling from these regions is another common approach to further decrease dimensionality.</p>
<p>Joint fusion offers the flexibility to merge the modalities at different depths of the model and thereby can learn feature representations from the input data (within the model) before fusing the different modalities into a common layer. Thus, the important difference to early fusion is that latent feature representation learning is not separated from the subsequent model and hence the loss can be backpropagated to the process of extracting features from raw data. This process is also called end-to-end learning. Depending on the task, CNNs or LSTMs are usually acquired to learn latent feature representations. As depicted in Figure 2, learning feature representations do not have to be applied to all modalities and is often not done for structured data. A further distinction can be made between models that facilitate another FCNN or a classical statistical model (linear, logistic, GAM, etc.) as model head. While the former can be desirable to capture possible interactions between modalities, the latter is frequently used as it preserves interpretability.</p>
<p>Late fusion or sometimes also called decision level fusion is the procedure of fusing the predictions of multiple models that have been trained on each modality separately. The idea comes from ensemble classifiers, where each model is assumed to inform the final prediction separately. Outcomes from the models can be aggregated in various ways such as averaging or majority voting.</p>
<p>While numerous examples from various fields for both early and joint fusion will be discussed in this chapter, late fusion has not been applied in many publications due to its separate training modes and thus is not further investigated here.</p>
<p>&lt;Figure2: Data modality fusion strategies&gt;</p>
</div>
<div id="applications-of-multimodal-dl" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.4</span> Applications of Multimodal DL<a href="further-topics.html#applications-of-multimodal-dl" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following section will discuss various examples of multimodal DL by referring to different publications and their proposed methods. The publications come from very different scientific fields and methods are target for their respective use case. Hence, allowing the user to follow along the development of methods as well as the progress in the field of multimodal DL (Struc. + Unstruc.) and obtaining a good overview of current and potential areas of applications. As there are various publications related to the topic of multimodal DL, the investigation was narrowed down to publications which introduce new methodical approaches or did pioneering work in their field by facilitating multimodal DL.
The last part of this section will also allude to applications of multimodal DL in settings where costly collected structured data was predominately used but freely available unstructured data sources were shown to be reasonable alternatives.</p>
</div>
<div id="multimodal-dl-in-survival" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.5</span> Multimodal DL in Survival<a href="further-topics.html#multimodal-dl-in-survival" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Especially in the field of survival analysis, many interesting ideas were proposed with regards to multimodal deep learning which also incorporates structured data. While clinical patient data such as electronic health records (EHR) were traditionally used for modelling risks and hazards in survival analysis, recent research has started to incorporate image data such as body scans and other modalities such as gene expression or RNA data in the modelling framework. Before examining these procedures in detail, we will briefly revisit the classical modelling setup of survival analysis by referring to the well-known Cox Proportional Hazard Model (CPH).</p>
</div>
<div id="traditional-survival-analysis-cox-proportional-hazard-model" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.6</span> Traditional Survival Analysis (Cox Proportional Hazard Model)<a href="further-topics.html#traditional-survival-analysis-cox-proportional-hazard-model" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="deepconvsurvdeepcorrsurv" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.7</span> DeepConvSurv+DeepCorrSurv<a href="further-topics.html#deepconvsurvdeepcorrsurv" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Briefly mention the advancements from DeepConv to DeepConvSurv over to DeepCorrSurv</p>
</div>
<div id="concat-cross-auto-encoders" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.8</span> Concat + Cross Auto Encoders<a href="further-topics.html#concat-cross-auto-encoders" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Explain the new ideas regarding Autoencoders that Tong et al. (2020) in the setup of multi-modal DL. Stemming from the fact, that different modalities have complementary and consensus information that can be utilized differently.
- not end-to-end learning
- mention the simulation they did on MNIST and the idea to control the complementary and consensus information for model evaluation purposes</p>
</div>
<div id="cheerla-and-gevaert-2019" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.9</span> Cheerla and Gevaert (2019)<a href="further-topics.html#cheerla-and-gevaert-2019" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to Tong et al. (2020), they also try to make use of the common information that is shared by all modalities. However, they learn similar feature representation by means of end-to-end learning and incorporating a similarity loss additional to the survival loss. Also in contrast to Tong et al. (2020), they specifically try to incorporate the missingness of some data and do not discard those features entirely. Instead, they propose a variation of regular dropout, which they refer to as multimodal dropout. Hence, they dropout entire modalities while training in order to make the trained models less dependent on one single data source and to better handle missing data during inference time.
-Mention t-SNE learned feature maps which surprisingly show</p>
</div>
<div id="multimodal-dl-in-economics" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.10</span> Multimodal DL in Economics<a href="further-topics.html#multimodal-dl-in-economics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Law, Paige and Russell (2019)
-end-to-end training to avoid labeling images
-combining aerial images and street view images with classical features to predict house prices
(compared to others, they dont use interior images)
-they actually want to make the effects of images orthogonal to the ones of the strucutred ones -&gt; they do that by fitting it in a two-stage process (regressing on the residuals)</p>
<p>-showed that visual attributes actually improve prediction compared to struc features only (however struc are still the most important single modality)
-showed that (linear) and GAM (interpretable models, perform similarly well) just perform slightly worse than full non linear model</p>
<p>&lt;Figure2: Results table – comparing different models&gt;</p>
<p>Jean et al (2016) – in detail</p>
<p>Briefly mention the pioneering work the following publications did in their related field
(Steele et al., 2017), (Sirko et al., 2021) and maybe (You et al., 2017), (Gebru et al., 2017)</p>
</div>
<div id="critical-assessment" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.11</span> Critical Assessment<a href="further-topics.html#critical-assessment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="conclusion-and-outlook" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.12</span> Conclusion and Outlook<a href="further-topics.html#conclusion-and-outlook" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>-Achievements: Different ways to incorporate multi modal data using DL
-General Observations:
tabular data often carries the most important information (noisy image data and small sample size)
end-to-end learning may improve performance of predictions
Joint fusion with head classical statistical model can preserve interpretability
-Major challenges: Small sample sizes particularly for images from patients,
making it hard for DL to extract valuable information from images so that structured data sources mostly carry the most relevant information,
insufficient benchmarking between proposed models as well as with the most important benchmark of single modality models (especially tabular data only models), DL has many tunable parameters, which makes it easy to achieve small improvements for some configurations
not clear on which data and which fields multi-modal works best, not clear which DL archtiectures as well as fusion strategies work best (joint fusion with interpretable or NN as head)
strong publication bias
-Outlook: Do we need multi-modal deep learning in regular scientific context (outside classical Computer vision tasks) where good and interpretable structured data is available? - In current setup it might seem questionable, but with increasing data sizes
However: Missing Data might be more easily handled if different data sources contain not only complementary but also consensus information</p>
<!-- I think this is obsolete? Uncomment if not true.

Outline of Chapter:

\begin{enumerate}
\item Structured vs. Unstructured Data
\begin{itemize}
  \item Taxonomy of Structured vs. Unstructured Data (Referring to classical examples and the respective modeling approaches traditionally used for them)
  \item Shortly discussing the advantages/disadvantages of such modeling approaches (prediction performance vs. interpretability)
  \item Discussing the fact that the line between Structured vs. Unstructured Data can become increasingly blurred if e.g. one feature can be interpreted on its own but only the combination of many yield the whole picture needed for prediction (e.g. measurement of a weather station)
  \item Before the rise of DL methods, manually creating features (feature engineering) was the common way to integrate unstructured data into classical modeling frameworks. Besides being very labor intensive, the main disadvantages of such was that certain features were not retrievable with domain knowledge nor were their interactions with other features foreseeable in many contexts
\end{itemize}

\item Different Fusion Strategies
\begin{itemize}
  \item Early Fusion
    \begin{itemize}
        \item Concatenation of different data modalities before inserting them to the model
        \item Problematic when dimensions between data modalities differ a lot (e.g. images and non-image data)
        \item Advantage: If dimensionality reduction/feature extraction such as PCA would be preferred over CNN due to limited available data (e.g. clinical images)
    \end{itemize}
  \item Intermediate Fusion
    \begin{itemize}
        \item Feature learning (mostly on image data) and then fusing modalities into joint model (DNN or linear model) to yield final prediction, jointly trained
        \item Advantages: Feature learning can be adjusted during joint training, dimensionality difference between data modes can easily be accounted for
    \end{itemize}
  \item Late Fusion
    \begin{itemize}
        \item Independent Model Training; this is more of an Ensemble (majority voting etc.)
        \item Advantage: beneficial in case of different data modalities not complementing each other as the feature learning would be more straight forward
    \end{itemize}
\end{itemize}

\item Multi-Modality DL in Medical/Biological Contexts
\begin{itemize}
  \item Examples
    \begin{itemize}
      \item Most publications focus on combining clinical image data (such as brain scans) with EHR (electronic health records, such as age, gender, weight etc.) in order to predict certain types of cancer or the progression of Alzheimer
      \item The overall goal of this research is to closely mimic the doctors decision making process, as the doctor is also consulting various data sources from different modalities in order to reach a conclusion regarding a disease
      \item Key findings and developments will be outlined: Huang et al (2020), Pölsterl et al (2020), Yala et al (2019) …
    \end{itemize}
  \item Critical Assessment of Publications
    \begin{itemize}
      \item Publication bias
      \item Small samples sizes (due to limited patient data)
      \item Often only marginal improvements that likely could have been due to hyperparameter tuning (“cheating”)
      \item Results often not comparable (different data, metrics, architectures, hyperparameters)
      \item Hence: Lack of systematic reviews and benchmarking which would yield deeper insights about the appropriateness of different multi-modal DL methods for specific use cases
    \end{itemize}
\end{itemize}

\item Multi-Modality DL in Other Contexts
\begin{itemize}
  \item Finance Li et al. (2020):
    \begin{itemize}
      \item Idea of the paper is that both fundamental financial information but also news reports impact stock movements
      \item They specifically approach the problems of correlation between the two data modes as well as the problem of sampling heterogeneity which is inherent to many multi-modality problems
      \item While fundamental market information normally changes (“sampled”) at fixed time points, news reports can arise (“be sampled”) at any time, which can make feature representation learning harder
      \item Without going into to many details, the paper would be mentioned because of the interesting use case at hand. (Note: Depending on the overall chapter length, this might also be taken out again)
    \end{itemize}
  \item Economics Jean et al. (2016):
    \begin{itemize}
      \item Combining survey and satellite to predict poverty in different countries. The satellites image data are satellite images of luminosity at night and daytime images.  Survey data including several poverty measures.
      \item Although the fusion strategy of modalities itself is not particularly interesting, the use case seems very promising for future research
    \end{itemize}
  \item Wide and Deep Neural Networks (Cheng et al., 2016):
    \begin{itemize}
      \item Very influential publication at that time as it was one of the first publication that introduced the idea of modeling a wide (linear model) and deep (DNN) jointly
      \item They used the idea to improve recommender systems which by then mostly relied on classical statistical model. The idea was to exploit the deep part to yield better generalization (via learned embedding vectors) and the wide part to achieve better memorization of certain special cases.
    \end{itemize}
\end{itemize}

\item The Combination of Both (New Approach for an Interpretable DL Model)
\begin{itemize}
  \item Semi-Structured Deep Distributional Regression (SSDDR):
    \begin{itemize}
       \item Rügamer, Kolb and Klein (2020) aim for a combination of classical statistical models (such as linear model and GAMs) and deep neural networks. They not only focus on mean prediction such as Wide and Deep NN, but incorporate the modeling of many distributional parameters with their special NN architecture.
       \item As previous literature mainly focused on prediction performance, the identifiability of certain effect estimates (that naturally arises e.g. in the case of the Wide and Deep NN of Cheng et al.(2016) due the universal approx. property of NN) was of lesser concern. Hence, interpretability of the models given up in favor of prediction performance improvements
       \item Unique contribution of paper: Orthogonalization cell in architecture which orthogonalizes the effects of the DNN part with respect to linear model or GAM part (their column space). This solves the issue of the identifiability problem and hence yields interpretable models while still being able to utilize the advantages of DNN for unstructured data
\end{itemize}
\end{itemize}

\item Conclusion and Outlook
\begin{itemize}
  \item Summarize the main ideas and developments that have been developed in the past decade. Revisiting the critical assessment of publication and by that laying out the importance of real benchmark and systematic procedures in the field. Discussion of potential of certain methods and outlook on future developments.
\end{itemize}

\item Literature (was not incldued in yet as this would be done after merging in order to avoid merge conflicts)
\begin{itemize}
  \item Cheng et al.(2016): https://arxiv.org/abs/1606.07792
  \item Jean et al. (2016): https://www.science.org/doi/10.1126/science.aaf7894
  \item Yala et al (2019): https://pubs.rsna.org/doi/full/10.1148/radiol.2019182716
  \item Huang et al (2020): https://www.nature.com/articles/s41746-020-00341-z
  \item Pölsterl et al (2020): https://arxiv.org/abs/1909.03890
  \item Li et al. (2020): https://ieeexplore.ieee.org/document/8966989
  \item Rügamer, Kolb and Klein (2020): https://arxiv.org/pdf/2002.05777.pdf
\end{itemize}

\end{enumerate}
-->

</div>
</div>
<div id="multi-purpose-models" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.3</span> Multi-purpose Models<a href="further-topics.html#multi-purpose-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: Philipp Koch</em></p>
<p><em>Supervisor: Rasmus Hvingelby</em></p>
<div id="intro-3" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.1</span> Intro<a href="further-topics.html#intro-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>After we describe further modalities in the previous sections, we will look at truly multipurpose models. Multitask multimodal models have already been proposed, like UniT, which extends the transformer architecture to deal with different modalities and tasks. However, previous multitask multimodal models remain limited in different aspects, which we will describe and discuss further. To become genuinely multipurpose, however, a model must be able to solve different tasks without fine-tuning and must be capable of dealing with different modalities. Thus, it must be able to transfer knowledge in-between tasks but must also be able to allocate capabilities for different modalities.</p>
<p>The recently introduced deep learning architecture Pathways is designed to be multipurpose. Pathways builds on newly designed hardware and software dedicated to addressing the challenges of contemporary deep learning models, which are ever-growing, where GPT-3 might be the most prominent example. We will discuss previous drawbacks and describe how Pathways aims to solve these issues. Besides the hardware aspect, Pathways provides a large neural network constructed as a directed acyclic graph (DAG). The input is passed through the network on different paths. Each node of the network is itself a neural network aimed at solving a specific aspect of a task. Using these different neural networks inside the model allows the model to be multitask and transfer knowledge in-between tasks. Another important aspect of this architecture is the obtained sparsity. When computed, just necessary nodes are computed, resulting in higher overall performance.</p>
<p>Furthermore, the model is intended to absorb different modalities as input, where no implementation has been found. Multimodality is further used hypothetically in the initial blog post. However, the similar model PathNet also achieves multimodality. The only model based on Pathways is the language model (PaLM), which is multilingual and capable of understanding code and solving mathematical tasks. However, the multimodality here remains questionable. Future Pathways-based models might provide more insight if the claim to step further toward artificial general intelligence (AGI) of the authors of Pathways and PathNet is true or not. Eventually, we will discuss the impact of the new Pathways multipurpose model since it might have a large impact on deep learning in the upcoming future. Broader applicable models will become feasible yet also centralize the usage, thus reducing accessibility and subsequently research on these models.</p>
</div>
<div id="introduction-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.2</span> Introduction<a href="further-topics.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In recent years and months, more and more focus has shifted to transformers being used in a multimodal setting (e.g. <span class="citation">(<span class="citeproc-not-found" data-reference-id="Lu2019"><strong>???</strong></span>)</span>). However, with the introduction of ViT (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Dosovitskiy2020"><strong>???</strong></span>)</span>), it became clear that these models are not just appropriate for NLP. Recent developments have proven transformers to be general models as long as input can be tokenized and presented to them. Although transformers have been successful in a multimodal and multitask learning setting, other models were also around too, and transformers might be further enhanced by using so-called Mixture-of-Expert layers as done in <span class="citation">(<span class="citeproc-not-found" data-reference-id="Fedus2021"><strong>???</strong></span>)</span> and recently <span class="citation">(<span class="citeproc-not-found" data-reference-id="Mustafa2022"><strong>???</strong></span>)</span>.
In this chapter, multi-purpose models will be surveyed. At first, the term multi-purpose will be clarified since there is, to our best knowledge, no standard definition for this term. Different approaches from recent work will be presented, and eventually, the chapter will diverge to future developments, as outlined in <span class="citation">(<span class="citeproc-not-found" data-reference-id="Dean2021"><strong>???</strong></span>)</span>. Jeff Dean proposed a promising architecture for future multi-purpose models, which will be further examined on how this proposal has already been implemented. The chapter will conclude with an outlook and a discussion on how the field will likely evolve in the following years.</p>
</div>
<div id="todo-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.3</span> TODO<a href="further-topics.html#todo-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Broader survey on previous work (also VisualBERT, VilBERT etc.)</li>
<li>Adjacent models like FLAVA</li>
</ul>
<div id="multipurpose-models" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.3.1</span> Multipurpose Models<a href="further-topics.html#multipurpose-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Since the early years of machine learning, multitask and multimodal learning paradigms have been around. Multitask learning <span class="citation">(<span class="citeproc-not-found" data-reference-id="Crawshaw2020"><strong>???</strong></span>)</span> is the paradigm of training a model on different tasks with the intention that the model transfers the learned knowledge to new tasks, such that fewer resources are required to learn new tasks. Akin to humans, it is intended that the model benefits from previously learned tasks. Humans do not learn every task from scratch. However, machine learning models do. It is assumed that related tasks let the model further generalize. Although there exists field-specific issues like catastrophic forgetting and negative transfer, this approach is also promising for future implementations.
Multimodal learning <span class="citation">(<span class="citeproc-not-found" data-reference-id="Baltrusaitis2019"><strong>???</strong></span>)</span> is a paradigm in which a machine learning model is supplied with multiple modalities like images, text, tables, etc. As in multitask learning, this approach is also inspired by human intelligence since humans perceive the world through multiple senses. It is thus assumed that multimodal models achieve better performance due to the higher input quality of the provided data. However, this field also has some specific problems, mainly focusing on how the different representations can be aligned when fused.
We want to marry the two paradigms to form a so-called multi-purpose model for this work. These kinds of models are both multimodal and multitask. We assume that this merge even further improves the quality of the predictions. Due to the novelty of this fusion (only a few models have been proposed, as we will see soon), there is no knowledge of any specific problems in this setup. Directions and possible implications will be discussed at the end of the chapter.</p>
</div>
</div>
<div id="previous-work" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.4</span> Previous Work<a href="further-topics.html#previous-work" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="multimodel" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.4.1</span> MultiModel<a href="further-topics.html#multimodel" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The first prominent multi-purpose model is the so-called MultiModel (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Kaiser2017"><strong>???</strong></span>)</span>). This model, from the pre-transformer time, combines multiple architectural approaches from different fields to tackle both multimodality and multiple tasks. The model itself is itself inspired by the encoder-decoder architecture, popular in NLP at that time, making it an autoregressive model.
The model consists of four important modules, which are the so-called modality nets, the encoder, the I/O Mixer, and the decoder.
The modality nets are used to form a representation on which the other modules can work such that it can be fed into the encoder or the I/O encoder (which is necessary because of the autoregressive structure) but also construct the output since they are also used to decode from the internal representation to the specific modality. For the language task, the modality net tokenizes the input sequences and is then transformed into the internal representation using learned embeddings?. For the output, the representation is fed into a simple feed-forward network, which is then fed into a softmax function. The modality net for images is multiple stacked convolution operations as done in Xception (X). Furthermore, there are also nets for audio and categorical modalities.
Inside the model, where the unified representations are used, there is the encoder, which consists of multiple convolution operations and a mixture-of-expert layer block. The output of the encoder is further passed on to the I/O mixer and the decoder which are now used to produce the output in an autoregressive way. The decoder produces the output, and the I/O mixer reads the previous output and combines it with the output of the encoder using attention and convolutional operations. Since this architecture is from the pre-transformer era, the attention mechanism used here is cross-attention.
The decoder eventually processes the output of the encoder and the I/O mixer, thus the input sequence and also the generated sequence, to produce proper output, which is done using attention and convolutional operations.</p>
</div>
<div id="unified-transformer-unit" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.4.2</span> Unified Transformer (UniT)<a href="further-topics.html#unified-transformer-unit" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>The Unified Transformer (UniT) (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Hu2021"><strong>???</strong></span>)</span>) is a thoroughly used transformer network with multiple encoders for each modality. Only a visual and a text encoder have been used in the initial setting. However, the authors state that an arbitrary amount of encoders can be used. For the textual input, a BERT model (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Devlin2019"><strong>???</strong></span>)</span>) has been used, while for the visual encoder, a DETR (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Carion2020"><strong>???</strong></span>)</span>) has been used. In this approach, the images are first pre-encoded using a ResNet (<span class="citation">(<span class="citeproc-not-found" data-reference-id="He2016"><strong>???</strong></span>)</span>). After the input image is encoded and linearly projected to the hidden dimension of the transformer, another task-specific vector is added to the data and fed into the here used visual transformer, which follows DETR. The authors chose BERT (Devlin et a. 2019) as an encoder for the textual representation. To encode the text, words of sentences are tokenized, and BERT-specific tokens, like the [CLS] token, are added. As in the visual encoder, a task-specific vector is added to the input, which is later removed from the output sequence. After the data from all modalities is encoded, it is concatenated and passed to the decoder, a vanilla transformer decoder according to Vaswani et al. 2017 and the one used in DETR. To the embedded sequence, a task-specific query representation sequence is also passed. Initially, the authors used both task-specific and task-agnostic decoders for their experiments.</p>
<p>After some developments in the field of multimodal transformers took place (VilBERT, ViT, etc.), the Unified Transformer (UniT) (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Hu2021"><strong>???</strong></span>)</span>) was introduced. Compared to previous approaches in the multi-purpose models, UniT aims to simplify the architecture. Achieving the capability of multitasking resulted in many hyperparameters and specific submodels to be set by hand for each task and or modality. UniT tried to achieve independence of this caveat, despite also using some task-specific submodules. The approach is as follows; transformers encode the input sequence on each domain, and the input encodings are concatenated and then passed on to a transformer decoder which is connected to task-specific output heads. Even though these heads were to be set manually, the model proved the fit of transformers for multi-purpose models. On top of the decoder, task-specific heads are used to transform the obtained sequence into a solution to the given tasks. These heads are trainable networks, which are to be switched if specific tasks are used.
The model often showed better results than a single-task specifically learned model. The model outperformed the single-task trained model for visual question answering (vqa), COCO (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Lin2014"><strong>???</strong></span>)</span>), and visual genome detection <span class="citation">(<span class="citeproc-not-found" data-reference-id="Krishna2017"><strong>???</strong></span>)</span>. On further tasks, when the model was trained for up to 8 tasks, it showed still comparable performance, however most of the time lower, than domain-specific models like BERT, VisualBERT (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Li2019"><strong>???</strong></span>)</span>), and DETR.</p>
</div>
<div id="ofa" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.4.3</span> OFA<a href="further-topics.html#ofa" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Another transformer-based model is OFA (<span class="citation">(<span class="citeproc-not-found" data-reference-id="Wang2022"><strong>???</strong></span>)</span>). The multi-purpose approach is implemented such that every input is tokenized into a unified vocabulary, which becomes possible since also images can be turned into tokens. Also, output sequences can be turned into their original or intended modality again, such that something like image generation from the text also becomes possible.</p>
</div>
<div id="gato" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.4.4</span> Gato<a href="further-topics.html#gato" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>To combine different modalities and create a model which is also capable of solving different tasks, the generalist agent “gato” was introduced. To additionally improve the model, reinforcement learning was also included to allow the model to become sequential and interact with it’s environment. The model is not just able of solving text and visual tasks, but also to solve classic reinforcement learning tasks like playing ATARI games and proprioception. The results of the model are state-of-the-art.
Reinforcement Learning is another approach in machine learning, where there is supervised learning, unsupervised learning and reinforcement learning. This technique is used to model sequential decision propblems namely, modeling the decision of a model depending on a specific state. The classic RL setup consists of an environment and an agent. The agent is led by a policy function and is informed about it’s decision using reward from the environment. The policy is then updated to optimize for the as best as possible policy by maximizing the reward of the agent. RL proved to be beneficial in many different setups and led in 2016 with DeepMind’s AlphaGo to a breakthrough by beating the grandmaster of go, which is considered a very complex game.
The internally used policy is a transformer, which is capable of dealing with discrete input (tokens) and also continuous properties of it’s environment. Text, visual properties, buttons and movements are tokenized such that these entities can be embedded as it is commonly used in natural language processing. The model itself is a decoder of a transformer and is trained autoregressively, such that based on the previous sequence, the next token is predicted. This approach is akin to the GPT family. To predict multiple modalities based on previous inputs, the model needs to work with embeddings which itself are based on tokens. Although the model is is multi-purpose, different modalities are first processed using specific models at their entry points. Natural language data is encoded using SentencePiece, images are tokenized using the same procedure as in ViT and subsequently encoded using a ResNet v2 <span class="citation">(<span class="citeproc-not-found" data-reference-id="He2016b"><strong>???</strong></span>)</span>, real world entities like buttons for games are also encoded to tokens allowing a transformer to become a multimodal model. Different techniques are applied to further represent these embeddings as sequences since transformer models are specifically designed for translation tasks and thus for sequence modeling. Text tokens remain in their intended order, while image tokens are represented as a raster to represent specific entities in a correct spatial representation. Tensors are also represented in a way such that rows are an important feature for sequencing. Nested structures are also ordered using keys to represent nesting. Specific observations for reinforcement learning are also sequenced such that actions and observations are also tokenized and sequenced.</p>
</div>
</div>
<div id="todo-2" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.5</span> TODO<a href="further-topics.html#todo-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Results on tasks</li>
<li>Describe architecture in detail</li>
</ul>
</div>
<div id="pathway-proposal" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.6</span> Pathway Proposal<a href="further-topics.html#pathway-proposal" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In 2021 Dean proposed a model, which entails many similarities to the Pathnet architecture from 2017. Aiming at making models multi-purpose, the model is meant to be sparse and subdivided into many experts, allowing to deal with multiple Tasks. Instead of previous approaches where a model is solely trained to be an expert on one task, Pathways is, similar to Pathnet, aimed to be a graph of models, where each node is an ffn where data is passed along edges. Using this approach, it is intended to transfer knowledge on specific tasks between the nodes and direct problems to respective experts through the network. Another main aspect of Pathways is to increase efficiency in deep learning by sparsing out networks. The idea is not to use the whole network but to only call respective experts, which leads to a severe drop in parameters used for downstream tasks.
Pathways Architecture
To address the issues in computational limitations and further direct into the direction shown by Dean in the Pathway proposal, a new deep learning framework was introduced aiming to build the foundations of future needs in deep learning.
Pathway builds on the advances of recent years in distributed high performance systems, such that it includes sharding.
A novel approach introduced is parallel asynchronous dispatch mechanism, which allows to use the resources of the TPU Pods more effectively on smaller programs.
An important feature considering the proposal of dean however, is the aim to access fine-grained details of the model. Previous approaches are assumed to train the whole model and update every weights, in this approach for Pathways however, it was specifically addresssed to update fine-grained details fo the mdoel like weights to support sparse models like MIxture of Expert models.
Pathway Implementations
However, no truly multi-purpose model has been publish so far. The only models based on Pathways are the language model PaLM <span class="citation">(<span class="citeproc-not-found" data-reference-id="Chowdhery2022"><strong>???</strong></span>)</span> and the text-to-image model parti <span class="citation">(<span class="citeproc-not-found" data-reference-id="Yu2022"><strong>???</strong></span>)</span>. Both published less than a year after the proposal, indicating that more is to come in the next months.</p>
</div>
<div id="todo-3" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.7</span> TODO<a href="further-topics.html#todo-3" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>More details on how hardware plays in the grand Pw proposal</li>
</ul>
<div id="pathnet" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.7.1</span> PathNet<a href="further-topics.html#pathnet" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>An architecture for neural networks <span class="citation">(<span class="citeproc-not-found" data-reference-id="Fernando2017"><strong>???</strong></span>)</span>. Also achieved multitask solving by introducing a novel training algorithm. In contrast to single-task neural networks, PathNet does not train the network solely on one task but on the whole network, but partially on one task and on a fraction of the network, randomly chosen. With this approach, knowledge sharing becomes possible.
The pathway consists of a graph of networks where the networks are organized in columns of hidden layers in the network. Each node in this graph is a neural network itself. This design intends to only train a path throughout the network on a specific task and subsequently train the network on an alternative path on another task. Thereby allowing the network to transfer already trained capability to the new task.
The model achieves this transfer by selecting the best path using an evolutional approach. At first, an initial population of paths is initialized and evaluated against each other in a binary way after trained T/some epochs. The better-performing algorithm will then be modified to further compete against other pathways. After the winner is found, the winner’s path is frozen, meaning that all weights are not updated anymore to keep the performance on the trained task and avoid catastrophic forgetting. After fixing the winning path, all other parameters are newly initialized.
Now, the same procedure is applied again to another task that is intended to benefit from the knowledge of the previous task. During this procedure, the tournament of different paths starts again, where the paths are trained and evaluated without the nodes from the previous winner path and eventually fixed again.</p>
</div>
<div id="limoe" class="section level4 hasAnchor">
<h4><span class="header-section-number">4.3.7.2</span> LIMoE<a href="further-topics.html#limoe" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Nevertheless, another worth-mentioning model in the context of Pathways is the Language Image Mixture of Expert Model LIMoE (Mustafa et al. 2022). This model changes the transformer encoder structure by swapping the feed-forward layer with a mixture of the expert layer.
The model can input text and images into a single encoder without decoding the different modalities differently. At first, both images and text are tokenized and linearly transformed to fit the dimension of the encoder.</p>
</div>
</div>
<div id="discussion-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.8</span> Discussion<a href="further-topics.html#discussion-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Considering the pace of the field and that results, as seen in Gato, flamingo and parti would not have been anticipated a few years ago, it is highly likely to see further breakthroughs in the field of multi-purpose models, which is especially the case since hardware-related issues are now being addressed and further developed. Based on these developments, more general models will be published in the near future. Models with even higher capabilities will also let new questions and problems arise. At this point, there is already the first issue arising, which is the trend of proprietary models. When GPT-3 was published, OpenAI closed access to the model leaving only a few with API keys to access the model.
In contrast to the introduction of BERT, which was open source, there did not follow a trend of GPTology as it was done with BERTology, leaving the model underresearched compared to its open source and significantly smaller peers. On the other side, there also exists a trend of open sourcing some models, as seen with GPT-J and GPT-Neo or recently with the introduction of OPT by meta. However, even though this trend exists, there is the issue of the increasing size of the models, which makes it almost impossible to train or even fine-tune these models without using massive amounts of computational resources.
Another issue that comes with the higher capability of these models is the societal impact of these models. Pop culture has severely impacted the perception of artificial intelligence, which might also become a topic in the future. It has already been a public issue that a google employee claimed that LamDA is sentient. With further high-quality models, there might be more discussions on how to deal with AI in the future. Since this already showed that the Turing test might not be an appropriate metric anymore, it might also be helpful to research new metrics for AI and possibly AGI.
In his TED-talk, Dean saw Pathway as a promising path toward AGI, which is a bold statement. However, considering the quality of already existing models, it might be a significant step in increasing the quality of generative models for solving multiple general tasks. Nevertheless, new problems will arise which are likely not solved with this model like continuous learning. The examined models and pathways are only trained once and then remain frozen in their knowledge. This issue is already visible in older language models like BERT and GPT, where Donald Trump is still president and COVID-19 does not exist.</p>
</div>
<div id="todo-4" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.9</span> TODO<a href="further-topics.html#todo-4" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Proposal for unifying standardizing evaluation, common benchmarks etc.</li>
<li>Outlook</li>
</ul>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="multimodal-architectures.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="title.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
