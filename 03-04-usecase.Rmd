---
bibliography: generativeArtsBib.bib
---

# Generative Art
*Author: Nadja Sauter*
*Supervisor: Jann Goschenhofer*

It is impressive what computers are able to do nowadays. With the help of artificial intelligence (AI) computers learned how to read, see, hear and in this way interact with humans. For instance, on many websites instead of talking to a person you text with a chatbot. Or in many homes, you tell Apple Siri or Amazon Alexa to put on the lights or to stop playing the music. Computers even learned to create pictures only based on a text prompt with methods mentioned in early chapters. This new ability is also used in arts, called ‘generative art’ or ‘computer art’. This new field comprises all artwork where the human artist cedes control to an autonomous system [@galanter2016generative]. In this way everyone can easily create pictures even if you are not artistically talented as the computer does the work for you. In some way, the computer is becoming the artist with some sort of creativity, a distinct human ability. In this chapter we want to give an overview about how computers became better and better in generating images based on text prompts and how this is used in the art scene.



## Historical Overview
The first attempt to use AI to generate pictures was made by Google engineer Alexander @mordvintsev_2015 and his "DeepDream" Software. He used Convolution Neural Nets to generate very interesting and abstract images based on the activation of a layer visualizing the patterns learned by a neural network. Below you can see a labrador after processed by the DeepDream algorithm.

<center>
![DeepDream generated by this [Google Colab](https://www.tensorflow.org/tutorials/generative/deepdream)](./figures/03-chapter3/DeepDream.png)
</center>

In the following year, @StyleTransfer investigated methods to transfer the style of pictures. Below you can see the same labrodar picture with a Kandinsky style.

<center>
![Labrador with Kandinsky Style generated by this [Google Colab](https://www.tensorflow.org/tutorials/generative/style_transfer)](./figures/03-chapter3/Kandinsky.png)
</center>

Furthermore, the architecture of Generative Adversarial Networks (GANs) first introduced by Google Researcher @NIPS2014_5ca3e9b1 was used by NVIDIA researches @karras2019style to create very realistic “fake” images with their architecture StyleGAN. For instance, you can create pictures of people who do not exist, but look totally realistic.

<center>
![Fake face created by [Google Colab](https://thispersondoesnotexist.com/)](./figures/03-chapter3/StyleGAN.jfif)
</center>


Nevertheless, it was almost not possible to control the output of these early forms of AI art. There was no option to make specifications of how the results should look like. For instance, you always get a human face with the earlier mentioned StyleGAN application, but you cannot specify to generate a blond girl. This is also known as the artist-critic paradigm [@morris_2022]: The computer as artist generates a picture based on what the Neural Network learned in the training phase. However, you also need a critic that tells the computer if the output satisfies the concrete idea of the human artist. For this reason multimodal deep learning models emerged as you can control the output with help of text prompting. In this way you can check if the generated picture matches the initial text prompt. A new class of models for generating pictures evolved. 

TBD
This idea was used by OpenAI for their models DALL-E and CLIP which were released in January 2021. Both architectures are critics for multimodal models. Only a few days after their release, Ryan Murdock combined CLIP with the already existing artist Neural Net “BigGAN” in his “The Big Sleeep” software. Furthermore, in March 2021 Or Patashnik et. al. developed StyleCLIP, a combination of StyleGAN (artist) and CLIP (critic) to specifically edit parts of images via text instructions. In the following month Katherine Corwson combined CLIP with the existing VQGAN algorithm. She also hooked up CLIP with guided diffusion models as artists. This was further investigated by OpenAI which published a paper in May 2021 about guided diffusion models and another paper in December 2021 about GLIDE, a model with CLIP or classifier-free guidance as critics and diffusion models as artists. 

