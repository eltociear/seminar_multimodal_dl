---
bibliography: generativeArtsBib.bib
---

# Generative Art
*Author: Nadja Sauter*  
*Supervisor: Jann Goschenhofer*

<figure align = "center">
<p float="left">
<img src="./figures/03-chapter3/lmu_siegel.gif" height="250" />
<img src="./figures/03-chapter3/VanGogh_Sonnenblumen.png" height="250" />
<img src="./figures/03-chapter3/VanGogh_video_AdobeExpress.gif" height="250" />
</p>
<figcaption align = "center"><b>LMU logo (left) processed by [Neural Stlye Transfer Algorithm ](https://www.tensorflow.org/tutorials/generative/style_transfer) (middle) and [CLIP + VQGAN](https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ#scrollTo=FhhdWrSxQhwg) (right) </b></figcaption>
</figure>


As we have seen in the past chapters, with the help of multimodal deep learning models computers can create pictures only based on text prompts. This new ability is also used in arts, called ‘generative art’ or ‘computer art’. The new field comprises all artwork where the human artist cedes control to an autonomous system [@galanter2016generative]. In this way everyone, even artistically untalented people, can easily create pictures as the computer does the work for you. In some way, the computer becomes the artist with some sort of creativity, a distinct human ability. In this chapter we want to give an overview about how computers became better and better in generating images based on text prompts and how this is used in the art scene.



## Historical Overview
The first attempt to use AI to generate pictures was made by Google engineer Alexander @mordvintsev_2015 and his "DeepDream" Software. He used Convolution Neural Nets to generate very interesting and abstract images based on the activation of a layer, visualizing the patterns learned by a neural network. Below you can see a picture of a Labrador after processed by the DeepDream algorithm.


<figure align = "center">
<img align="center" src="./figures/03-chapter3/DeepDream.png"  width="550" margin="0 auto" />
<figcaption align = "center"><b>Picture of a Labrador processed by DeepDream generated by this [Google Colab](https://www.tensorflow.org/tutorials/generative/deepdream)</b></figcaption>
</figure>

In the following year, @StyleTransfer investigated methods to transfer the style of pictures. Below you can see the same Labrador picture with a Kandinsky style.


<figure align = "center">
<img align="center" src="./figures/03-chapter3/Kandinsky.png"  width="550" margin="0 auto" />
<figcaption align = "center"><b>Labrador with Kandinsky Style generated by this [Google Colab](https://www.tensorflow.org/tutorials/generative/style_transfer)</b></figcaption>
</figure>

Furthermore, the architecture of Generative Adversarial Networks (GANs), which was first introduced by Google Researcher @NIPS2014_5ca3e9b1, was used by NVIDIA researches @karras2019style to create very realistic fake images with their architecture StyleGAN. For instance, you can create pictures of people who do not exist, but look totally realistic.



<figure align = "center">
<img align="center" src="./figures/03-chapter3/StyleGAN.jfif"  width="550" margin="0 auto" />
<figcaption align = "center"><b>Fake face created by [Google Colab](https://thispersondoesnotexist.com/)</b></figcaption>
</figure>

However, it was almost not possible to control the output of these early forms of AI art. There was no option to make specifications of how the result should look like in detail. For instance, you always get a human face with the earlier mentioned StyleGAN application, but you cannot specify to generate a blond girl with blue eyes. This is also known as the artist-critic paradigm [@8477754]: The computer as artist generates a picture based on what the Neural Network learned in the training phase (e.g. learn to generate pictures of faces). However, you also need a critic that tells the computer if the output satisfies the concrete idea of the human artist. For this reason multimodal deep learning models emerged. Here you can control the output with the help of text prompting. In this way you can check if the generated picture matches the initial text prompt. Looking at our StyleGAN example, the multimodal architecture supervises if the output picture is indeed a blond girl with blue eyes. A new class of models for generating pictures evolved. 

This idea was used by OpenAI for their models DALL-E and CLIP which were released in January 2021. Both architectures are critics for multimodal models. Only a few days after the release, Ryan Murdock combined CLIP (critic) with the already existing Neural Net “BigGAN” (artist) in his “The Big Sleep” software. Furthermore, @StyleGAN developed StyleCLIP, a combination of StyleGAN (artist) and CLIP (critic) to specifically edit parts of images via text instructions. In the follwing months, Katherine Corwson combined CLIP with the existing VQGAN algorithm. She also hooked up CLIP with guided diffusion models as artists. This approach was further investigated by OpenAI which published a paper [@DiffusionModels] in May 2021 about guided diffusion models. Moreover, in December 2021 they introduced GLIDE [@GLIDE], a model with CLIP or classifier-free guidance as critics and diffusion models as artists. 


## How to use models?
A lot of different notebooks are publicly available to apply the different pre-trained models. In general, all notebooks work pretty similar: You only need to enter your text prompt in the code and after running the notebook the computer generates a picture based on your instruction. It is easy and no coding knowledge required. Moreover, there are also some API and GUI applications (e.g. [MindsEye beta](https://multimodal.art/mindseye)), thus, no programming is needed at all. Using these models, it is important to think about how exactly you enter your text prompt. With little changes you can influence the output by your short text instruction in a desired way. This is also known as prompt engineering. For instance, you can add “in the style of Van Gogh” to change the style of the output. Besides, a special trick is to append “unreal engine” [@unrealEngine] which makes picture more realistic and higher in quality. That seems strange and surprising, but the models were trained on data from the internet including pictures of the software company Epic Games that has a popular 3D video game engine called "Unreal Engine". This is one of the most popular prompting tricks, but of course there a lot of other approaches which are investigated by different users and researches. 

Unfortunately, OpenAI has never released DALL-E. There is only an open-source version called ruDALL-E [@ruDALLE] that was trained on Russian language data. Besides, PyTorch offers a replication of the DALL-E code [@DALLEpytorch] but no trained model. Furthermore, CLIP was released without publishing the used training data. However, there exists an open source data set with CLIP embeddings called LAION-400m [@LAION]. In the following, we used different publicly available notebooks to try out the different models [CLIP + BigGAN](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing), 
[CLIP + VQGAN](https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ#scrollTo=FhhdWrSxQhwg), 
[CLIP + Guided Diffusion](https://colab.research.google.com/drive/12a_Wrfi2_gwwAuN3VvMTwVMz9TfqctNj#scrollTo=X5gODNAMEUCR), 
[GLIDE](https://colab.research.google.com/github/openai/glide-text2im/blob/main/notebooks/text2im.ipynb)
with the text prompt *"a fall landscape with a small cottage next to a lake"*. Some of the notebooks run on lower resolution due to computational limitations. Besides, GLIDE is also downsized by the publisher: The released smaller model consists of 300 million parameters, whereas the unreleased model has about 3.5 billion parameters [@GLIDE]. So better results are possible with higher computational power and other implementations of the models.


<center>
![Comparison different models](./figures/03-chapter3/fall_landscape.png)
</center>




## Different tasks and modalities
So far, we concentrated on the two modalities text and image. Combining both of them, you can tackle different tasks with the models mentioned above. The main task is to generate images based on a text prompt. Therefore, you can start from noise or you can chose a real image as starting point [@qiao2022initial]. Besides, you can edit, extend, crop and search images with models like GLIDE  [@GLIDE]. It is also possible to combine other modalities as well. For instance, @WZRD accompanies your video with suitable audio. Or you can also create little videos with text prompting (e.g. [CLIP + VQGAN](https://colab.research.google.com/drive/1ZAus_gn2RhTZWzOWUpPERNC0Q8OhZRTZ#scrollTo=FhhdWrSxQhwg)). It is even imaginable to create sculptures with 3D-printers [@3D]. 



## Discussion and prospects
In the last years methods to generate images by text prompting tremendously improved and a new field in arts arised. It is surprising how these models are able to create images only based on a short text instruction. This is quite impressive as AI achieved some kind of creativity. It is up for discussion to which extent the computer is becoming the artist in generative art and in this way replacing the human artist. However, there is still no direct loss function that can calculate how aesthetically pleasing a picture is [@mcateer2021clippe]. This is probably also quite subjective and cannot be answered for everyone in the same way. Most of the time the computer works as aid for the creative process by generating multiple images. Then, the human artist usually picks the best outcome. However, the better AI becomes, the less the human artist is needed for this process. 

Furthermore, as the output becomes more and more realistic, there is the risk that the methods are abused to facilitate plagiarism or create fake content and spread misleading information [@misconduct]. After all, the output looks totally realistic, but are completely made-up and generated by the computer. For this reason, some organisations like Open-AI do not release all their models (e.g. DALL-E) or downstream models (e.g. CLIP). On the other hand from a scientific point of view, it is important to get access to such models to continue research. 

Moreover, similarly to most Deep Learning algorithms, these models are affected by biases of the input data [@bias_ML]. For instance, @bias points out that CLIP text embeddings associate a human being more with a man than a woman. In this way it might be likelier that our models generate a man with the text prompt "human being" than a woman. This effect needs to be further investigated and should be removed. 

After all, generative arts can be used to create Non Fungible Tokens (NFT) really easily. NFTs are digital artwork where a special digital signature is added making them unique and non-fungible [@NFT]. The digital artwork is bought and sold online, often by cryptocurrency. That is why this field is also called Cryptoart. This provides the perfect platform to sell generative arts. However, this trading market is very new and controversial like crypotcurrency in general.




