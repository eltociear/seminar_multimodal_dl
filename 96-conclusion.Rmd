# Conclusion

*Author: Nadja Sauter*  

*Supervisor: Matthias Assenmacher*

It is very impressive how multimodal architectures developed especially in the last two years. Particularly, methods to generate picture based on text prompts like DALL-E became incredibly good in their job. Even in the art scene these methods can be used as shown in our use case “Generative Arts”. Apart from that, these methods can be deployed commercially in the gaming industry or film production for instance. It is impressive how realistic and precise outputs can be achieved by such architectures. However, these methods can also be abused to spread misleading information as it is not possible to distinguish between a fake or a real picture by only looking at it. This can be systematically used to manipulate the public opinion by spreading AI manipulated media, also called deepfakes. That’s why researchers like Luisa Verdoliva [ref paper] demand for automated tool which are capable of detecting these fabrications. Besides, like most Deep Learning models, these multimodal architectures are also not free from bias yet which also has to be investigated further [paper]. Besides, the algorithms are very complex and so-called “black-box” models meaning that you cannot directly retrace how the model came to a certain solution or decision.  This limits their social acceptance and usability as the underlying process is not credible and transparent [paper]. For instance, in medical questions like if someone might have cancer or not, apart from the decision of the AI the reasoning and the certainty are highly relevant for doctors and patients.
Furthermore, there is the clear trend in recent years to build more and more complex architectures in order to achieve higher performance. For instance OpenAI’s language mode GPT-2 has about 1.5 Billion parameter [paper] whereas the follower model GPT-3 has about 175 Billion parameters requiring 800 GB of storage[paper]. Looking at literature, increasing parameters often helps to improve model performance. However, billions of parameters need to be calculated which takes a lot of time and enormous computational power. Training GPT-2 took about 1 week (168 hours) of training on 32 TPUv3 chips [Via the authors on Reddit link]. Researchers [paper] estimate that the cloud compute cost of training GPT-2 once add up to about $12,902–$43,008. Apart from the enormout costs, this also results in an outsize environmental impact as this process is enormously energy intensive. Due to missing power draw data on GPT-2's training hardware, the researchers weren’t able to calculate the CO2 emission. However, for the popular BERT model (110M parameters) they calculated $3,751-$12,571 cloud compute cost, 1,507kWh energy consumption and Carbon footprint of 1,438 lbs of CO2e. In comparison, the CO2 footprint of Air travel, 1 passenger, NY ↔SF about 1,984 lbs of CO2e. In conclusion training BERT once results in almost the same footprint. On top of this, these numbers are only for one training unit. Developing a model or adapting it often takes several fitting and tuning phases. The overall CO2 emission is immense. 
Moreover, this computational power as well as the necessary hardware, technology and expenses to run these models can only be provided by big Tech companies like Google, Facebook and Open AI. This results in a disparate access between researchers in academia versus industry. Besides, the Tech companies often do not publish their models because of their expenses and as these models are their product and intellectual property. This is against scientific fundamentals to publish research and makes it impossible to reproduce or monitor their work and findings. From an economic point of view this is a foundation of a monopoly which can be dangerous and abused by the Tech giants. 
