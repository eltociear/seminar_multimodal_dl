---
title: "Conclusion"
author: "Nadja Sauter"
date: "04.08.2022"
bibliography: IntroConc.bib
---

# Conclusion

*Author: Nadja Sauter*  

*Supervisor: Matthias Assenmacher*

It is very impressive how multimodal architectures developed especially in the last two years. Particularly, methods to generate pictures based on text prompts like DALL-E became incredibly good in their job. Even in the art scene these methods can be used as shown in our use case “Generative Arts”. Apart from that, these methods can be deployed commercially in the gaming industry or film production for instance. It is impressive how realistic and precise outputs are achieved by such architectures. However, these methods can also be abused to spread misleading information as it is often impossible to distinguish between a fake or a real picture by only looking at it. This can be systematically used to manipulate the public opinion by spreading AI manipulated media, also called deepfakes. That’s why researchers like @9115874 demand automated tools which are capable of detecting these fabrications. Besides, like most Deep Learning models, multimodal architectures are not free from bias which also needs to be investigated further [@bias]. Besides, the algorithms are very complex and in ths way so-called “black-box” models meaning that you cannot directly retrace how the model came to a certain solution or decision. This limits their social acceptance and usability as the underlying process is not credible and transparent enough [@9391727]. For instance, in medical questions like if someone might have cancer or not, apart from the decision of the AI the reasoning and the certainty are highly relevant for doctors and patients.  

\newline  

Furthermore, there is the clear trend in recent years to build more and more complex architectures in order to achieve higher performance. For instance OpenAI’s language model GPT-2 has about 1.5 Billion parameter [@Radford2019LanguageMA] whereas the follower model GPT-3 has about 175 Billion parameters [@GPT3]. Increasing the number of parameters often helps to improve model performance, but all of these parameters need to be calculated which takes a lot of time and enormous computational power. Training GPT-2 took about one week (168 hours) of training on 32 TPUv3 chips [@environment]. The researchers @environment estimate that the cloud compute costs of training GPT-2 add up to about \$12,902–\$43,008. Apart from the enormous expenses, this also contributes to our environmental burden as this process is really energy intensive. Due to missing power draw data on GPT-2's training hardware, the researchers weren’t able to calculate the CO~2~ emission. However, for the popular BERT architecture with 110M parameters they calculated cloud compute costs of \$3,751-\$12,571, energy consumption of 1,507 kWh and a Carbon footprint of 1,438 lbs of CO~2~. In comparison, the footprint of flying from New York to San Francisco by plane for one passenger is about 1,984 lbs of CO~2~. In conclusion training BERT once results in almost the same footprint as this long-haul flight. On top of this, these numbers are only for one training run. Developing a new model or adapting it often takes several fitting and tuning phases.  

\newline  

Moreover, this computational power as well as the necessary hardware, technology and  financial means to run these models can only be provided by big technology companies like e.g. Google, Facebook or Open AI. This results in a disparate access between researchers in academia versus industry. Besides, the companies often do not publish their models as these models are their product and intellectual property. In this way it is not possible to reproduce their work and findings independently. Besides, from an economic point of view this is the foundation of a monopoly which can be dangerous for economic competition and abused by the Tech Giants. 

