<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-10-24" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="acknowledgements.html"/>

<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Multimodal Deep Learning</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-to-multimodal-deep-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Multimodal Deep Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-01-sota-nlp"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in NLP</a></li>
<li class="chapter" data-level="2.2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-02-sota-cv"><i class="fa fa-check"></i><b>2.2</b> State-of-the-art in Computer Vision</a></li>
<li class="chapter" data-level="2.3" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-03-benchmarks"><i class="fa fa-check"></i><b>2.3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-01-img2text"><i class="fa fa-check"></i><b>3.1</b> Image2Text</a></li>
<li class="chapter" data-level="3.2" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-02-text2img"><i class="fa fa-check"></i><b>3.2</b> Text-to-image</a></li>
<li class="chapter" data-level="3.3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-03-img-support-text"><i class="fa fa-check"></i><b>3.3</b> Images supporting Language Models</a></li>
<li class="chapter" data-level="3.4" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-04-text-support-img"><i class="fa fa-check"></i><b>3.4</b> Text supporting Computer Vision Models</a></li>
<li class="chapter" data-level="3.5" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-05-text-plus-img"><i class="fa fa-check"></i><b>3.5</b> Models for both modalities</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c03-00-further.html"><a href="c03-00-further.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-01-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a></li>
<li class="chapter" data-level="4.2" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-02-structured-unstructured"><i class="fa fa-check"></i><b>4.2</b> Structured + Unstructured Data</a></li>
<li class="chapter" data-level="4.3" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-03-multi-purpose"><i class="fa fa-check"></i><b>4.3</b> Multi-Purpose Models</a></li>
<li class="chapter" data-level="4.4" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-04-usecase"><i class="fa fa-check"></i><b>4.4</b> Generative Art</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>6</b> Epilogue</a><ul>
<li class="chapter" data-level="6.1" data-path="epilogue.html"><a href="epilogue.html#new-influential-architectures"><i class="fa fa-check"></i><b>6.1</b> New influential architectures</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>7</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered hasAnchor">
<h1>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<div id="refs" class="references">
<div>
<p>Agirre, Eneko, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. “A Study on Similarity and Relatedness Using Distributional and Wordnet-Based Approaches.”</p>
</div>
<div>
<p>Ailem, Melissa, Bowen Zhang, Aurelien Bellet, Pascal Denis, and Fei Sha. 2018. “A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images.”</p>
</div>
<div>
<p>Akbari, Hassan, Liangzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong. 2021. “VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text.” In <em>Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, Neurips 2021, December 6-14, 2021, Virtual</em>, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 24206–21. <a href="https://proceedings.neurips.cc/paper/2021/hash/cb3213ada48302953cb0f166464ab356-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/cb3213ada48302953cb0f166464ab356-Abstract.html</a>.</p>
</div>
<div>
<p>Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” <em>arXiv Preprint arXiv:2204.14198</em>.</p>
</div>
<div>
<p>———. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.14198">https://doi.org/10.48550/ARXIV.2204.14198</a>.</p>
</div>
<div>
<p>Alford, A. 2021. “Google Announces 800M Parameter Vision-Language Ai Model Align.” 2021. <a href="https://www.infoq.com/news/2021/07/google-vision-language-ai/">https://www.infoq.com/news/2021/07/google-vision-language-ai/</a>.</p>
</div>
<div>
<p>Anderson, Peter, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. “SPICE: Semantic Propositional Image Caption Evaluation.” In <em>Computer Vision – Eccv 2016</em>, edited by Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, 382–98. Cham: Springer International Publishing.</p>
</div>
<div>
<p>Anderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. “Bottom-up and Top-down Attention for Image Captioning and Visual Question Answering.” In <em>2018 Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6077–86. <a href="https://doi.org/10.1109/CVPR.2018.00636">https://doi.org/10.1109/CVPR.2018.00636</a>.</p>
</div>
<div>
<p>Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. “Vqa: Visual Question Answering.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2425–33.</p>
</div>
<div>
<p>Aran, Komatsuzaki. 2021. “When You Generate Images with Vqgan Clip, the Image Quality Dramatically Improves If You Add "Unreal Engine" to Your Prompt. People Are Now Calling This "Unreal Engine Trick".” Twitter. <a href="https://twitter.com/arankomatsuzaki/status/1399471244760649729">https://twitter.com/arankomatsuzaki/status/1399471244760649729</a>.</p>
</div>
<div>
<p>Bachmann, Roman, David Mizrahi, Andrei Atanov, and Amir Zamir. 2022. “MultiMAE: Multi-Modal Multi-Task Masked Autoencoders.” <em>arXiv Preprint arXiv: Arxiv-2204.01678</em>.</p>
</div>
<div>
<p>Baevski, Alexei, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. “Data2vec: A General Framework for Self-Supervised Learning in Speech, Vision and Language.” <em>arXiv Preprint arXiv:2202.03555</em>.</p>
</div>
<div>
<p>Baevski, Alexei, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. “Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.” <em>Advances in Neural Information Processing Systems</em> 33: 12449–60.</p>
</div>
<div>
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate.” <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.</p>
</div>
<div>
<p>Baltrušaitis, Tadas, Chaitanya Ahuja, and Louis-Philippe Morency. 2017. “Multimodal Machine Learning: A Survey and Taxonomy.” <em>arXiv Preprint arXiv: Arxiv-1705.09406</em>.</p>
</div>
<div>
<p>———. 2019. “Multimodal Machine Learning: A Survey and Taxonomy.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 41 (2): 423–43. <a href="https://doi.org/10.1109/TPAMI.2018.2798607">https://doi.org/10.1109/TPAMI.2018.2798607</a>.</p>
</div>
<div>
<p>Bandy, Jack, and Nicholas Vincent. 2021. “Addressing" Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for Bookcorpus.” <em>arXiv Preprint arXiv:2105.05241</em>.</p>
</div>
<div>
<p>Banerjee, Satanjeev, and Alon Lavie. 2005. “METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.” In <em>Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</em>, 65–72. Ann Arbor, Michigan: Association for Computational Linguistics. <a href="https://aclanthology.org/W05-0909">https://aclanthology.org/W05-0909</a>.</p>
</div>
<div>
<p>Bao, Hangbo, Li Dong, and Furu Wei. 2021. “Beit: Bert Pre-Training of Image Transformers.” <em>arXiv Preprint arXiv:2106.08254</em>.</p>
</div>
<div>
<p>Barham, Paul, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, et al. 2022. “Pathways: Asynchronous Distributed Dataflow for Ml.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2203.12533">https://doi.org/10.48550/ARXIV.2203.12533</a>.</p>
</div>
<div>
<p>Bäck, Thomas, and Hans-Paul Schwefel. 1993. “An Overview of Evolutionary Algorithms for Parameter Optimization.” <em>Evolutionary Computation</em> 1 (1): 1–23. <a href="https://doi.org/10.1162/evco.1993.1.1.1">https://doi.org/10.1162/evco.1993.1.1.1</a>.</p>
</div>
<div>
<p>Bellemare, Marc G., Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. “The Arcade Learning Environment: An Evaluation Platform for General Agents.” <em>J. Artif. Int. Res.</em> 47 (1): 253–79.</p>
</div>
<div>
<p>Bender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” In <em>Proceedings of the 2021 Acm Conference on Fairness, Accountability, and Transparency</em>, 610–23. FAccT ’21. Virtual Event, Canada: Association for Computing Machinery. <a href="https://doi.org/10.1145/3442188.3445922">https://doi.org/10.1145/3442188.3445922</a>.</p>
</div>
<div>
<p>Bengio, Yoshua, Aaron C. Courville, and Pascal Vincent. 2013. “Representation Learning: A Review and New Perspectives.” <em>IEEE Trans. Pattern Anal. Mach. Intell.</em> 35 (8): 1798–1828. <a href="https://doi.org/10.1109/TPAMI.2013.50">https://doi.org/10.1109/TPAMI.2013.50</a>.</p>
</div>
<div>
<p>Beyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. “Are We Done with Imagenet?” <em>arXiv Preprint arXiv:2006.07159</em>.</p>
</div>
<div>
<p>Birhane, Abeba, Vinay Uday Prabhu, and Emmanuel Kahembwe. 2021. “Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes.” <em>arXiv Preprint arXiv:2110.01963</em>.</p>
</div>
<div>
<p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. “Enriching Word Vectors with Subword Information.” <a href="http://arxiv.org/abs/1607.04606">http://arxiv.org/abs/1607.04606</a>.</p>
</div>
<div>
<p>Bommasani, Rishi, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, et al. 2021. “On the Opportunities and Risks of Foundation Models.” <em>arXiv Preprint arXiv:2108.07258</em>.</p>
</div>
<div>
<p>Bordes, Patrick, Eloi Zablocki, Laure Soulier, Benjamin Piwowarski, and Patrick Gallinari. 2020. “Incorporating Visual Semantics into Sentence Representations Within a Grounded Space.” <em>arXiv Preprint arXiv:2002.02734</em>.</p>
</div>
<div>
<p>Boris, Dayma. 2022. “DALL·E Mini.” <a href="https://huggingface.co/spaces/dalle-mini/dalle-mini">https://huggingface.co/spaces/dalle-mini/dalle-mini</a>.</p>
</div>
<div>
<p>Borji, Ali. 2018. “Pros and Cons of GAN Evaluation Measures.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1802.03446">http://arxiv.org/abs/1802.03446</a>.</p>
</div>
<div>
<p>Bosch, Anna, Andrew Zisserman, and Xavier Munoz. 2007. “Image Classification Using Random Forests and Ferns.” In <em>2007 Ieee 11th International Conference on Computer Vision</em>, 1–8. Ieee.</p>
</div>
<div>
<p>Bowman, Samuel R, and George E Dahl. 2021. “What Will It Take to Fix Benchmarking in Natural Language Understanding?” <em>arXiv Preprint arXiv:2104.02145</em>.</p>
</div>
<div>
<p>Bromley, Jane, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. 1993. “Signature Verification Using a" Siamese" Time Delay Neural Network.” <em>Advances in Neural Information Processing Systems</em> 6.</p>
</div>
<div>
<p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020a. “Language Models Are Few-Shot Learners.” <a href="http://arxiv.org/abs/2005.14165">http://arxiv.org/abs/2005.14165</a>.</p>
</div>
<div>
<p>———. 2020b. “Language Models Are Few-Shot Learners.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2005.14165">https://doi.org/10.48550/ARXIV.2005.14165</a>.</p>
</div>
<div>
<p>Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.</p>
</div>
<div>
<p>Bruni, Elia, Nam-Khanh Tran, and Marco Baroni. 2014. “Multimodal Distributional Semantics.” <em>Journal of Artificial Intelligence Research</em> 49: 1–47.</p>
</div>
<div>
<p>Brysbaert, Marc, Amy Beth Warriner, and Victor Kuperman. 2014. “Concreteness Ratings for 40 Thousand Generally Known English Word Lemmas.” <em>Behavior Research Methods</em> 46 (3): 904–11.</p>
</div>
<div>
<p>Carion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. “End-to-End Object Detection with Transformers.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2005.12872">https://arxiv.org/abs/2005.12872</a>.</p>
</div>
<div>
<p>Caron, Mathilde, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2006.09882">https://arxiv.org/abs/2006.09882</a>.</p>
</div>
<div>
<p>Caron, Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. “Emerging Properties in Self-Supervised Vision Transformers.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 9650–60.</p>
</div>
<div>
<p>Carreira, Joao, Skanda Koppula, Daniel Zoran, Adria Recasens, Catalin Ionescu, Olivier Henaff, Evan Shelhamer, et al. 2022. “Hierarchical Perceiver.” <em>arXiv Preprint arXiv: Arxiv-2202.10890</em>.</p>
</div>
<div>
<p>Cheerla, Anika, and Olivier Gevaert. 2019. “Deep Learning with Multimodal Representation for Pancancer Prognosis Prediction.” <em>Bioinformatics</em> 35 (14): i446–i454.</p>
</div>
<div>
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020a. “A Simple Framework for Contrastive Learning of Visual Representations.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.05709">https://doi.org/10.48550/ARXIV.2002.05709</a>.</p>
</div>
<div>
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020b. “A Simple Framework for Contrastive Learning of Visual Representations.” In <em>Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event</em>, 119:1597–1607. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v119/chen20j.html">http://proceedings.mlr.press/v119/chen20j.html</a>.</p>
</div>
<div>
<p>Chen, Xinlei, Saining Xie, and Kaiming He. 2021. “An Empirical Study of Training Self-Supervised Vision Transformers.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 9640–9.</p>
</div>
<div>
<p>Cheng, Heng-Tze, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, et al. 2016. “Wide &amp; Deep Learning for Recommender Systems.” In <em>Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</em>, 7–10.</p>
</div>
<div>
<p>Cho, Kyunghyun, Bart van Merrienboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. “Learning Phrase Representations Using Rnn Encoder-Decoder for Statistical Machine Translation.” <a href="http://arxiv.org/abs/1406.1078">http://arxiv.org/abs/1406.1078</a>.</p>
</div>
<div>
<p>Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. “Palm: Scaling Language Modeling with Pathways.” <em>arXiv Preprint arXiv:2204.02311</em>.</p>
</div>
<div>
<p>———. 2022. “PaLM: Scaling Language Modeling with Pathways.” <em>Arxiv:2204.02311</em>. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>.</p>
</div>
<div>
<p>Clark, Kevin, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. “What Does Bert Look at? An Analysis of Bert’s Attention.” <a href="http://arxiv.org/abs/1906.04341">http://arxiv.org/abs/1906.04341</a>.</p>
</div>
<div>
<p>Collell, Guillem, Ted Zhang, and Marie-Francine Moens. 2017. “Imagined Visual Representations as Multimodal Embeddings.” In <em>Proceedings of the Aaai Conference on Artificial Intelligence</em>. Vol. 31. 1.</p>
</div>
<div>
<p>Cornia, Marcella, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2019. “Meshed-Memory Transformer for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1912.08226">https://doi.org/10.48550/ARXIV.1912.08226</a>.</p>
</div>
<div>
<p>———. 2020. “Meshed-Memory Transformer for Image Captioning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>.</p>
</div>
<div>
<p>Crawshaw, Michael. 2020. “Multi-Task Learning with Deep Neural Networks: A Survey.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2009.09796">https://doi.org/10.48550/ARXIV.2009.09796</a>.</p>
</div>
<div>
<p>Crowson, Katherine, Stella Biderman, Daniel Kornis, Dashiell Stander, Eric Hallahan, Louis Castricato, and Edward Raff. 2022. “VQGAN-Clip: Open Domain Image Generation and Editing with Natural Language Guidance.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.08583">https://doi.org/10.48550/ARXIV.2204.08583</a>.</p>
</div>
<div>
<p>Da, Jeff, and Jungo Kasai. 2019. “Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations.” <a href="http://arxiv.org/abs/1910.01157">http://arxiv.org/abs/1910.01157</a>.</p>
</div>
<div>
<p>Das, Abhishek, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. 2017. “Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?” <em>Computer Vision and Image Understanding</em> 163: 90–100.</p>
</div>
<div>
<p>Dean, Jeff. 2021. “Introducing Pathways: A Next-Generation Ai Architecture.” 2021. <a href="https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/">https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/</a>.</p>
</div>
<div>
<p>Dean, Jeffrey. 2020. “1.1 the Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design.” In <em>2020 Ieee International Solid- State Circuits Conference - (Isscc)</em>, 8–14. <a href="https://doi.org/10.1109/ISSCC19947.2020.9063049">https://doi.org/10.1109/ISSCC19947.2020.9063049</a>.</p>
</div>
<div>
<p>Dehouche, Nassim. 2021. “Plagiarism in the Age of Massive Generative Pre-Trained Transformers (Gpt-3).” <em>Ethics in Science and Environmental Politics</em> 21: 17–23.</p>
</div>
<div>
<p>Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In <em>2009 Ieee Conference on Computer Vision and Pattern Recognition</em>, 248–55. Ieee.</p>
</div>
<div>
<p>Devereux, Barry J, Lorraine K Tyler, Jeroen Geertzen, and Billi Randall. 2014. “The Centre for Speech, Language and the Brain (Cslb) Concept Property Norms.” <em>Behavior Research Methods</em> 46 (4): 1119–27.</p>
</div>
<div>
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018a. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1810.04805">https://doi.org/10.48550/ARXIV.1810.04805</a>.</p>
</div>
<div>
<p>———. 2018b. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div>
<p>———. 2018c. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” <em>arXiv Preprint arXiv:1810.04805</em>.</p>
</div>
<div>
<p>———. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.</p>
</div>
<div>
<p>Dhamala, Jwala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. “Bold: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation.” In <em>Proceedings of the 2021 Acm Conference on Fairness, Accountability, and Transparency</em>, 862–72.</p>
</div>
<div>
<p>Dhariwal, Prafulla, and Alex Nichol. 2021. “Diffusion Models Beat Gans on Image Synthesis.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a>.</p>
</div>
<div>
<p>Ding, Ming, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, et al. 2021. “CogView: Mastering Text-to-Image Generation via Transformers.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2105.13290">https://arxiv.org/abs/2105.13290</a>.</p>
</div>
<div>
<p>Doerr, Benjamin, and Frank Neumann. 2021. “A Survey on Recent Progress in the Theory of Evolutionary Algorithms for Discrete Optimization.” <em>ACM Trans. Evol. Learn. Optim.</em> 1 (4). <a href="https://doi.org/10.1145/3472304">https://doi.org/10.1145/3472304</a>.</p>
</div>
<div>
<p>Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” <em>arXiv Preprint arXiv:2010.11929</em>.</p>
</div>
<div>
<p>———. 2020a. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.</p>
</div>
<div>
<p>———. 2020b. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.</p>
</div>
<div>
<p>———. 2021. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” In <em>9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021</em>. OpenReview.net. <a href="https://openreview.net/forum?id=YicbFdNTTy">https://openreview.net/forum?id=YicbFdNTTy</a>.</p>
</div>
<div>
<p>Dwibedi, Debidatta, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. 2021. “With a Little Help from My Friends: Nearest-Neighbor Contrastive Learning of Visual Representations.” <em>CoRR</em> abs/2104.14548. <a href="https://arxiv.org/abs/2104.14548">https://arxiv.org/abs/2104.14548</a>.</p>
</div>
<div>
<p>Education, IBM Cloud. 2020a. “What Is Supervised Learning?” www.ibm.com.</p>
</div>
<div>
<p>———. 2020b. “What Is Unsupervised Learning?” www.ibm.com.</p>
</div>
<div>
<p>Esser, Patrick, Robin Rombach, and Bjorn Ommer. 2021. “Taming Transformers for High-Resolution Image Synthesis.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 12873–83.</p>
</div>
<div>
<p>Esser, Patrick, Robin Rombach, and Björn Ommer. 2020. “A Note on Data Biases in Generative Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2012.02516">https://doi.org/10.48550/ARXIV.2012.02516</a>.</p>
</div>
<div>
<p>Ettinger, Allyson. 2019. “What Bert Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for Language Models.” <a href="http://arxiv.org/abs/1907.13528">http://arxiv.org/abs/1907.13528</a>.</p>
</div>
<div>
<p>Everingham, Mark, Luc van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2010. “The Pascal Visual Object Classes (Voc) Challenge.” <em>International Journal of Computer Vision</em> 88 (2): 303–38. <a href="https://doi.org/10.1007/s11263-009-0275-4">https://doi.org/10.1007/s11263-009-0275-4</a>.</p>
</div>
<div>
<p>Fellbaum, Christiane. 2010. “WordNet.” In <em>Theory and Applications of Ontology: Computer Applications</em>, 231–43. Springer.</p>
</div>
<div>
<p>Fellbaum, Christiane D. 2000. “WordNet : An Electronic Lexical Database.” <em>Language</em> 76: 706.</p>
</div>
<div>
<p>Fernando, Chrisantha, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. 2017. “PathNet: Evolution Channels Gradient Descent in Super Neural Networks.” In. <a href="https://arxiv.org/abs/1701.08734">https://arxiv.org/abs/1701.08734</a>.</p>
</div>
<div>
<p>Forbes, Maxwell, Ari Holtzman, and Yejin Choi. 2019. “Do Neural Language Representations Learn Physical Commonsense?” <a href="http://arxiv.org/abs/1908.02899">http://arxiv.org/abs/1908.02899</a>.</p>
</div>
<div>
<p>Gafni, Oran, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. 2022. “Make-a-Scene: Scene-Based Text-to-Image Generation with Human Priors.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2203.13131">https://doi.org/10.48550/ARXIV.2203.13131</a>.</p>
</div>
<div>
<p>Galanter, Philip. 2016. “Generative Art Theory.” <em>A Companion to Digital Art</em> 1: 631.</p>
</div>
<div>
<p>Gao, Jiyang, Zhen Li, Ram Nevatia, and others. 2017. “Knowledge Concentration: Learning 100k Object Classifiers in a Single Cnn.” <em>arXiv Preprint arXiv:1711.07607</em>.</p>
</div>
<div>
<p>Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, et al. 2020. “The Pile: An 800gb Dataset of Diverse Text for Language Modeling.” <em>arXiv Preprint arXiv:2101.00027</em>.</p>
</div>
<div>
<p>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2016. “A Neural Algorithm of Artistic Style.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1508.06576">https://doi.org/10.48550/ARXIV.1508.06576</a>.</p>
</div>
<div>
<p>Gebru, Timnit, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, Erez Aiden, and Li Fei-Fei. 2017. “Using Deep Learning and Google Street View to Estimate the Demographic Makeup of Neighborhoods Across the United States.” <em>Proceedings of the National Academy of Sciences</em> 114: 201700035. <a href="https://doi.org/10.1073/pnas.1700035114">https://doi.org/10.1073/pnas.1700035114</a>.</p>
</div>
<div>
<p>Gesmundo, Andrea, and Jeff Dean. 2022. “MuNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-Tuning Multitask Systems.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2205.10937">https://doi.org/10.48550/ARXIV.2205.10937</a>.</p>
</div>
<div>
<p>Gokaslan, Aaron, and Vanya Cohen. 2019. “OpenWebText Corpus.”</p>
</div>
<div>
<p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014a. “Generative Adversarial Networks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1406.2661">https://doi.org/10.48550/ARXIV.1406.2661</a>.</p>
</div>
<div>
<p>———. 2014b. “Generative Adversarial Networks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1406.2661">https://doi.org/10.48550/ARXIV.1406.2661</a>.</p>
</div>
<div>
<p>Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. “Explaining and Harnessing Adversarial Examples.” <em>arXiv Preprint arXiv:1412.6572</em>.</p>
</div>
<div>
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger. Vol. 27. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</a>.</p>
</div>
<div>
<p>Google. 2022. “Embeddings: Translating to a Lower-Dimensional Space.”</p>
</div>
<div>
<p>Goyal, Yash, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. “Making the V in Vqa Matter: Elevating the Role of Image Understanding in Visual Question Answering.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 6904–13.</p>
</div>
<div>
<p>Grill, Jean-Bastien, Florian Strub, Florent Altch’e, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” <em>Neurips</em>.</p>
</div>
<div>
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2006.07733">https://doi.org/10.48550/ARXIV.2006.07733</a>.</p>
</div>
<div>
<p>Guo, Yandong, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. 2016. “Ms-Celeb-1m: A Dataset and Benchmark for Large-Scale Face Recognition.” In <em>European Conference on Computer Vision</em>, 87–102. Springer.</p>
</div>
<div>
<p>Harnad, Stevan. 1990. “The Symbol Grounding Problem.” <em>Physica D: Nonlinear Phenomena</em> 42 (1-3): 335–46.</p>
</div>
<div>
<p>Harris, Z, and others. 1954. “Distributional Hypothesis.” <em>Word World</em> 10 (23): 146–62.</p>
</div>
<div>
<p>Hart, B., and T. R. Risley. 1995. “Meaningful Differences in the Everyday Experience of Young American Children.” <em>Baltimore, MD: Paul H. Brookes Publishing Company,</em></p>
</div>
<div>
<p>He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. “Masked Autoencoders Are Scalable Vision Learners.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 16000–16009.</p>
</div>
<div>
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div>
<p>Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. “Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning.” <em>Journal of Machine Learning Research</em> 21 (248): 1–43.</p>
</div>
<div>
<p>Herdade, Simao, Armin Kappeler, Kofi Boakye, and Joao Soares. 2019. “Image Captioning: Transforming Objects into Words.” In, 11135–45. <a href="http://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words">http://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words</a>.</p>
</div>
<div>
<p>Heusel, Martin, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. “GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/8a1d694707eb0fefe65871369074926d-Paper.pdf</a>.</p>
</div>
<div>
<p>Hill, Felix, and Anna Korhonen. 2014. “Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can’t See What I Mean.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 255–65.</p>
</div>
<div>
<p>Hill, Felix, Roi Reichart, and Anna Korhonen. 2015. “Simlex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation.” <em>Computational Linguistics</em> 41 (4): 665–95.</p>
</div>
<div>
<p>Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1503.02531">https://doi.org/10.48550/ARXIV.1503.02531</a>.</p>
</div>
<div>
<p>Hinton, Geoffrey, Oriol Vinyals, Jeff Dean, and others. 2015. “Distilling the Knowledge in a Neural Network.” <em>arXiv Preprint arXiv:1503.02531</em> 2 (7).</p>
</div>
<div>
<p>Ho, Jonathan, Ajay Jain, and Pieter Abbeel. 2020a. “Denoising Diffusion Probabilistic Models.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2006.11239">https://arxiv.org/abs/2006.11239</a>.</p>
</div>
<div>
<p>———. 2020b. “Denoising Diffusion Probabilistic Models.” In <em>Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, Neurips 2020, December 6-12, 2020, Virtual</em>, edited by Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin. <a href="https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html">https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html</a>.</p>
</div>
<div>
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8): 1735–80.</p>
</div>
<div>
<p>Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” <em>arXiv Preprint arXiv:2203.15556</em>.</p>
</div>
<div>
<p>Howard, Andrew G., Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. 2017. “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.” <em>CoRR</em> abs/1704.04861. <a href="http://arxiv.org/abs/1704.04861">http://arxiv.org/abs/1704.04861</a>.</p>
</div>
<div>
<p>Hu, Ronghang, and Amanpreet Singh. 2021a. “Unit: Multimodal Multitask Learning with a Unified Transformer.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 1439–49.</p>
</div>
<div>
<p>———. 2021b. “UniT: Multimodal Multitask Learning with a Unified Transformer.” In <em>2021 Ieee/Cvf International Conference on Computer Vision (Iccv)</em>, 1419–29. <a href="https://doi.org/10.1109/ICCV48922.2021.00147">https://doi.org/10.1109/ICCV48922.2021.00147</a>.</p>
</div>
<div>
<p>Huang, Lun, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. 2019. “Attention on Attention for Image Captioning.” In, 4633–42. <a href="https://doi.org/10.1109/ICCV.2019.00473">https://doi.org/10.1109/ICCV.2019.00473</a>.</p>
</div>
<div>
<p>Huang, Shih-Cheng, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P Lungren. 2020. “Fusion of Medical Imaging and Electronic Health Records Using Deep Learning: A Systematic Review and Implementation Guidelines.” <em>NPJ Digital Medicine</em> 3 (1): 1–9.</p>
</div>
<div>
<p>Huang, Yanping, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, and Zhifeng Chen. 2018. “GPipe: Efficient Training of Giant Neural Networks Using Pipeline Parallelism.” <em>CoRR</em> abs/1811.06965. <a href="http://arxiv.org/abs/1811.06965">http://arxiv.org/abs/1811.06965</a>.</p>
</div>
<div>
<p>Hudson, Drew A, and Christopher D Manning. 2019. “Gqa: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6700–6709.</p>
</div>
<div>
<p>IV, William C. Sleeman, Rishabh Kapoor, and Preetam Ghosh. 2021. “Multimodal Classification: Current Landscape, Taxonomy and Future Directions.” <em>arXiv Preprint arXiv: Arxiv-2109.09020</em>.</p>
</div>
<div>
<p>Ive, Julia, Pranava Madhyastha, and Lucia Specia. 2019. “Distilling Translations with Visual Awareness.” <em>arXiv Preprint arXiv:1906.07701</em>.</p>
</div>
<div>
<p>Jacobs, Robert A., Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. “Adaptive Mixtures of Local Experts.” <em>Neural Computation</em> 3 (1): 79–87. <a href="https://doi.org/10.1162/neco.1991.3.1.79">https://doi.org/10.1162/neco.1991.3.1.79</a>.</p>
</div>
<div>
<p>Jaegle, Andrew, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. 2021a. “Perceiver: General Perception with Iterative Attention.” In <em>International Conference on Machine Learning</em>, 4651–64. PMLR.</p>
</div>
<div>
<p>Jaegle, Andrew, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and João Carreira. 2021b. “Perceiver: General Perception with Iterative Attention.” In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:4651–64. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/jaegle21a.html">http://proceedings.mlr.press/v139/jaegle21a.html</a>.</p>
</div>
<div>
<p>Jaspreet. 2019. “A Concise History of Neural Networks by Jaspreet Towards Data Science.” towardsdatascience.com.</p>
</div>
<div>
<p>Jean, Neal, Marshall Burke, Michael Xie, W. Matthew Davis, David B. Lobell, and Stefano Ermon. 2016. “Combining Satellite Imagery and Machine Learning to Predict Poverty.” <em>Science</em> 353 (6301): 790–94. <a href="https://doi.org/10.1126/science.aaf7894">https://doi.org/10.1126/science.aaf7894</a>.</p>
</div>
<div>
<p>Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021a. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” In <em>International Conference on Machine Learning</em>, 4904–16. PMLR.</p>
</div>
<div>
<p>Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021b. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2102.05918">https://arxiv.org/abs/2102.05918</a>.</p>
</div>
<div>
<p>Jordan, Michael I., and Robert A. Jacobs. 1994. “Hierarchical Mixtures of Experts and the Em Algorithm.” <em>Neural Computation</em> 6 (2): 181–214. <a href="https://doi.org/10.1162/neco.1994.6.2.181">https://doi.org/10.1162/neco.1994.6.2.181</a>.</p>
</div>
<div>
<p>Joseph, K. J., Salman H. Khan, Fahad Shahbaz Khan, and Vineeth N. Balasubramanian. 2021. “Towards Open World Object Detection.” <em>CoRR</em> abs/2103.02603. <a href="https://arxiv.org/abs/2103.02603">https://arxiv.org/abs/2103.02603</a>.</p>
</div>
<div>
<p>Joshi, Gargi, Rahee Walambe, and Ketan Kotecha. 2021. “A Review on Explainability in Multimodal Deep Neural Nets.” <em>IEEE Access</em> 9: 59800–59821. <a href="https://doi.org/10.1109/ACCESS.2021.3070212">https://doi.org/10.1109/ACCESS.2021.3070212</a>.</p>
</div>
<div>
<p>Jumper, John, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, et al. 2021. “Highly Accurate Protein Structure Prediction with AlphaFold.” <em>Nature</em> 596 (7873): 583–89. <a href="https://doi.org/10.1038/s41586-021-03819-2">https://doi.org/10.1038/s41586-021-03819-2</a>.</p>
</div>
<div>
<p>Kahatapitiya, Kumara, and Michael S. Ryoo. 2021. “SWAT: Spatial Structure Within and Among Tokens.” <em>arXiv Preprint arXiv: Arxiv-2111.13677</em>.</p>
</div>
<div>
<p>Kaiser, Lukasz, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. 2017. “One Model to Learn Them All.” <em>arXiv</em>. <a href="https://arxiv.org/pdf/1706.05137.pdf">https://arxiv.org/pdf/1706.05137.pdf</a>.</p>
</div>
<div>
<p>Karpathy, Andrej, and Li Fei-Fei. 2014. “Deep Visual-Semantic Alignments for Generating Image Descriptions.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1412.2306">https://doi.org/10.48550/ARXIV.1412.2306</a>.</p>
</div>
<div>
<p>Karras, Tero, Samuli Laine, and Timo Aila. 2019. “A Style-Based Generator Architecture for Generative Adversarial Networks.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 4401–10.</p>
</div>
<div>
<p>Katzman, Jared L, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2018. “DeepSurv: Personalized Treatment Recommender System Using a Cox Proportional Hazards Deep Neural Network.” <em>BMC Medical Research Methodology</em> 18 (1): 1–12.</p>
</div>
<div>
<p>Kiela, Douwe, and Léon Bottou. 2014. “Learning Image Embeddings Using Convolutional Neural Networks for Improved Multi-Modal Semantics.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 36–45.</p>
</div>
<div>
<p>Kiela, Douwe, Alexis Conneau, Allan Jabri, and Maximilian Nickel. 2017. “Learning Visually Grounded Sentence Representations.” <em>arXiv Preprint arXiv:1707.06320</em>.</p>
</div>
<div>
<p>Kingma, Diederik P., and Max Welling. 2019. “An Introduction to Variational Autoencoders.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1906.02691">http://arxiv.org/abs/1906.02691</a>.</p>
</div>
<div>
<p>Kingma, Diederik P, and Max Welling. 2013. “Auto-Encoding Variational Bayes.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1312.6114">https://doi.org/10.48550/ARXIV.1312.6114</a>.</p>
</div>
<div>
<p>Kiros, Jamie, William Chan, and Geoffrey Hinton. 2018. “Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.” In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 922–33.</p>
</div>
<div>
<p>Koehn, Philipp. 2005. “Europarl: A Parallel Corpus for Statistical Machine Translation.” In <em>Proceedings of Machine Translation Summit X: Papers</em>, 79–86.</p>
</div>
<div>
<p>Kolesnikov, Alexander, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. 2019. “Large Scale Learning of General Visual Representations for Transfer.” <em>arXiv Preprint arXiv:1912.11370</em> 2 (8).</p>
</div>
<div>
<p>Kopper, Philipp, Simon Wiegrebe, Bernd Bischl, Andreas Bender, and David Rügamer. 2022. “DeepPAMM: Deep Piecewise Exponential Additive Mixed Models for Complex Hazard Structures in Survival Analysis.” In <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>, 249–61. Springer.</p>
</div>
<div>
<p>Kottur, Satwik, Ramakrishna Vedantam, José MF Moura, and Devi Parikh. 2016. “Visual Word2vec (Vis-W2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 4985–94.</p>
</div>
<div>
<p>Krishna, Ranjay, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, et al. 2016. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” In. <a href="https://arxiv.org/abs/1602.07332">https://arxiv.org/abs/1602.07332</a>.</p>
</div>
<div>
<p>———. 2017. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” <em>International Journal of Computer Vision</em> 123 (1): 32–73.</p>
</div>
<div>
<p>Krizhevsky, Alex, Geoffrey Hinton, and others. 2009. “Learning Multiple Layers of Features from Tiny Images.”</p>
</div>
<div>
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012a. “ImageNet Classification with Deep Convolutional Neural Networks.” In <em>Advances in Neural Information Processing Systems</em>, edited by F. Pereira, C. J. Burges, L. Bottou, and K. Q. Weinberger. Vol. 25. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf</a>.</p>
</div>
<div>
<p>———. 2012b. “Imagenet Classification with Deep Convolutional Neural Networks.” <em>Advances in Neural Information Processing Systems</em> 25.</p>
</div>
<div>
<p>Kudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, 66–71. Brussels, Belgium: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D18-2012">https://doi.org/10.18653/v1/D18-2012</a>.</p>
</div>
<div>
<p>Kuznetsova, Alina, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, et al. 2020. “The Open Images Dataset V4.” <em>International Journal of Computer Vision</em> 128 (7): 1956–81.</p>
</div>
<div>
<p>Kynkäänniemi, Tuomas, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. 2019. “Improved Precision and Recall Metric for Assessing Generative Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1904.06991">https://doi.org/10.48550/ARXIV.1904.06991</a>.</p>
</div>
<div>
<p>Law, Stephen, Brooks Paige, and Chris Russell. 2019. “Take a Look Around.” <em>ACM Transactions on Intelligent Systems and Technology</em> 10 (5): 1–19. <a href="https://doi.org/10.1145/3342240">https://doi.org/10.1145/3342240</a>.</p>
</div>
<div>
<p>Lazaridou, Angeliki, Nghia The Pham, and Marco Baroni. 2015. “Combining Language and Vision with a Multimodal Skip-Gram Model.” <em>arXiv Preprint arXiv:1501.02598</em>.</p>
</div>
<div>
<p>LeCun, Yann. 2022. “A Path Towards Autonomous Machine Intelligence Version 0.9. 2, 2022-06-27.”</p>
</div>
<div>
<p>Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. “BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension.” In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 7871–80. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a>.</p>
</div>
<div>
<p>Lewkowycz, Aitor, Anders Andreassen, David Martin Dohan, Ethan S Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, et al. 2022. “Solving Quantitative Reasoning Problems with Language Models.” Technical report. <a href="https://arxiv.org/abs/2206.14858">https://arxiv.org/abs/2206.14858</a>.</p>
</div>
<div>
<p>Lialin, Vladislav, Kevin Zhao, Namrata Shivagunde, and Anna Rumshisky. 2022. “Life After Bert: What Do Other Muppets Understand About Language?” <a href="http://arxiv.org/abs/2205.10696">http://arxiv.org/abs/2205.10696</a>.</p>
</div>
<div>
<p>Lin, Chin-Yew. 2004. “ROUGE: A Package for Automatic Evaluation of Summaries.” In <em>Text Summarization Branches Out</em>, 74–81. Barcelona, Spain: Association for Computational Linguistics. <a href="https://aclanthology.org/W04-1013">https://aclanthology.org/W04-1013</a>.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2014. “Microsoft Coco: Common Objects in Context.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1405.0312">https://doi.org/10.48550/ARXIV.1405.0312</a>.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014a. “Microsoft Coco: Common Objects in Context.” In <em>European Conference on Computer Vision</em>, 740–55. Springer.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014b. “Microsoft Coco: Common Objects in Context.” In <em>Computer Vision – Eccv 2014</em>, 740–55. Springer International Publishing.</p>
</div>
<div>
<p>Lin, Yongjie, Yi Chern Tan, and Robert Frank. 2019. “Open Sesame: Getting Inside BERT’s Linguistic Knowledge.” In <em>Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP</em>. Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/w19-4825">https://doi.org/10.18653/v1/w19-4825</a>.</p>
</div>
<div>
<p>Liu, Nelson F., Matt Gardner, Yonatan Belinkov, Matthew E. Peters, and Noah A. Smith. 2019. “Linguistic Knowledge and Transferability of Contextual Representations.” <a href="http://arxiv.org/abs/1903.08855">http://arxiv.org/abs/1903.08855</a>.</p>
</div>
<div>
<p>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “Roberta: A Robustly Optimized Bert Pretraining Approach.” <em>arXiv Preprint arXiv:1907.11692</em>.</p>
</div>
<div>
<p>Lottick, Kadan, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. “Energy Usage Reports: Environmental Awareness as Part of Algorithmic Accountability.” <em>arXiv Preprint arXiv:1911.08354</em>.</p>
</div>
<div>
<p>Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019a. “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1908.02265">https://doi.org/10.48550/ARXIV.1908.02265</a>.</p>
</div>
<div>
<p>———. 2019b. “Vilbert: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” <em>Advances in Neural Information Processing Systems</em> 32.</p>
</div>
<div>
<p>Lu, Yujie, Wanrong Zhu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. 2022. “Imagination-Augmented Natural Language Understanding.” <em>arXiv Preprint arXiv:2204.08535</em>.</p>
</div>
<div>
<p>Mahajan, Dhruv, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. 2018. “Exploring the Limits of Weakly Supervised Pretraining.” In <em>Proceedings of the European Conference on Computer Vision (Eccv)</em>, 181–96.</p>
</div>
<div>
<p>Manning, Chris, Anna Goldie, and John Hewitt. 2022. “Stanford Cs224n: Natural Language Processing with Deep Learning.”</p>
</div>
<div>
<p>Mayer, Thomas, and Michael Cysouw. 2014. “Creating a Massively Parallel Bible Corpus.” <em>Oceania</em> 135 (273): 40.</p>
</div>
<div>
<p>Mccormack, Jon, and Camilo Cruz Gambardella. 2022. “Growing and Evolving 3-d Prints.” <em>IEEE Transactions on Evolutionary Computation</em> 26 (1): 88–99. <a href="https://doi.org/10.1109/TEVC.2021.3095156">https://doi.org/10.1109/TEVC.2021.3095156</a>.</p>
</div>
<div>
<p>MICHAEL BARTHEL, JESSE HOLCOMB, GALEN STOCKING, and AMY MITCHELL. 2016. “Reddit News Users More Likely to Be Male, Young and Digital in Their News Preferences.” 2016. <a href="https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/">https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/</a>.</p>
</div>
<div>
<p>Midjourney. 2022. “Midjourney.” <a href="https://www.midjourney.com/">https://www.midjourney.com/</a>.</p>
</div>
<div>
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013a. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv Preprint arXiv:1301.3781</em>.</p>
</div>
<div>
<p>———. 2013b. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv Preprint arXiv:1301.3781</em>.</p>
</div>
<div>
<p>Mikolov, Tomas, Quoc V. Le, and Ilya Sutskever. 2013. “Exploiting Similarities Among Languages for Machine Translation.” <a href="http://arxiv.org/abs/1309.4168">http://arxiv.org/abs/1309.4168</a>.</p>
</div>
<div>
<p>Mikolov, Tomas, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Distributed Representations of Words and Phrases and Their Compositionality.” <a href="http://arxiv.org/abs/1310.4546">http://arxiv.org/abs/1310.4546</a>.</p>
</div>
<div>
<p>Mineault, Patrick. 2021. “Unsupervised Models of the Brain.” 2021. <a href="https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/">https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/</a>.</p>
</div>
<div>
<p>Mircosoft. 2019. “Evaluate:Detection.” 2019. <a href="https://cocodataset.org/#detection-eval">https://cocodataset.org/#detection-eval</a>.</p>
</div>
<div>
<p>Mishkin, Pamela, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. 2022. “DALL·E 2 Preview - Risks and Limitations.” <a href="%5Bhttps://github.com/openai/dalle-2-preview/blob/main/system-card.md%5D(https://github.com/openai/dalle-2-preview/blob/main/system-card.md)">[https://github.com/openai/dalle-2-preview/blob/main/system-card.md](https://github.com/openai/dalle-2-preview/blob/main/system-card.md)</a>.</p>
</div>
<div>
<p>Mordvintsev, Alexander. 2015. “Inceptionism: Going Deeper into Neural Networks.” <em>Google AI Blog</em>. Google. <a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a>.</p>
</div>
<div>
<p>Mustafa, Basil, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. 2022. “Multimodal Contrastive Learning with Limoe: The Language-Image Mixture of Experts.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2206.02770">https://doi.org/10.48550/ARXIV.2206.02770</a>.</p>
</div>
<div>
<p>Nagrani, Arsha, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. 2021. “Attention Bottlenecks for Multimodal Fusion.” In <em>Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, Neurips 2021, December 6-14, 2021, Virtual</em>, edited by Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, 14200–14213. <a href="https://proceedings.neurips.cc/paper/2021/hash/76ba9f564ebbc35b1014ac498fafadd0-Abstract.html">https://proceedings.neurips.cc/paper/2021/hash/76ba9f564ebbc35b1014ac498fafadd0-Abstract.html</a>.</p>
</div>
<div>
<p>“Neural Networks - History.” 2022. cs.stanford.edu.</p>
</div>
<div>
<p>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021a. “GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2112.10741">https://doi.org/10.48550/ARXIV.2112.10741</a>.</p>
</div>
<div>
<p>———. 2021b. “GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a>.</p>
</div>
<div>
<p>Oord, Aäron van den, Oriol Vinyals, and Koray Kavukcuoglu. 2017. “Neural Discrete Representation Learning.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1711.00937">http://arxiv.org/abs/1711.00937</a>.</p>
</div>
<div>
<p>OpenAI. 2021. “DALL-E.” <a href="https://github.com/openai/DALL-E">https://github.com/openai/DALL-E</a>.</p>
</div>
<div>
<p>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “Bleu: A Method for Automatic Evaluation of Machine Translation.” In <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, 311–18. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. <a href="https://doi.org/10.3115/1073083.1073135">https://doi.org/10.3115/1073083.1073135</a>.</p>
</div>
<div>
<p>Parcalabescu, Letitia, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. “VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena.” In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 8253–80. Association for Computational Linguistics. <a href="https://aclanthology.org/2022.acl-long.567">https://aclanthology.org/2022.acl-long.567</a>.</p>
</div>
<div>
<p>Patashnik, Or, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. “StyleCLIP: Text-Driven Manipulation of Stylegan Imagery.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2103.17249">https://arxiv.org/abs/2103.17249</a>.</p>
</div>
<div>
<p>Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. “Glove: Global Vectors for Word Representation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 1532–43.</p>
</div>
<div>
<p>Perez, Ethan, Douwe Kiela, and Kyunghyun Cho. 2021a. “True Few-Shot Learning with Language Models.” <a href="http://arxiv.org/abs/2105.11447">http://arxiv.org/abs/2105.11447</a>.</p>
</div>
<div>
<p>———. 2021b. “True Few-Shot Learning with Language Models.” <em>Advances in Neural Information Processing Systems</em> 34: 11054–70.</p>
</div>
<div>
<p>Pezzelle, Sandro, Ece Takmaz, and Raquel Fernández. 2021. “Word Representation Learning in Multimodal Pre-Trained Transformers: An Intrinsic Evaluation.” <em>Transactions of the Association for Computational Linguistics</em> 9: 1563–79.</p>
</div>
<div>
<p>Pilehvar, Mohammad Taher, and Jose Camacho-Collados. 2021. <em>Embeddings in Natural Language Processing</em>. Springer International Publishing. <a href="https://doi.org/10.1007/978-3-031-02177-0">https://doi.org/10.1007/978-3-031-02177-0</a>.</p>
</div>
<div>
<p>Pont-Tuset, Jordi, Jasper Uijlings, Soravit Changpinyo, Radu Soricut, and Vittorio Ferrari. 2020. “Connecting Vision and Language with Localized Narratives.” In <em>European Conference on Computer Vision</em>, 647–64. Springer.</p>
</div>
<div>
<p>Pölsterl, Sebastian, Ignacio Sarasua, Benjamı́n Gutiérrez-Becker, and Christian Wachinger. 2019. “A Wide and Deep Neural Network for Survival Analysis from Anatomical Shape and Tabular Clinical Data.” In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 453–64. Springer.</p>
</div>
<div>
<p>Prabhu, Vinay Uday, and Abeba Birhane. 2020. “Large Image Datasets: A Pyrrhic Win for Computer Vision?” <em>arXiv Preprint arXiv:2006.16923</em>.</p>
</div>
<div>
<p>Qiao, Han, Vivian Liu, and Lydia Chilton. 2022. “Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal Ai Generated Art.” In <em>Creativity and Cognition</em>, 15–28.</p>
</div>
<div>
<p>Qiao, Tingting, Jing Zhang, Duanqing Xu, and Dacheng Tao. 2019. “MirrorGAN: Learning Text-to-Image Generation by Redescription.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1903.05854">http://arxiv.org/abs/1903.05854</a>.</p>
</div>
<div>
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021a. “Learning Transferable Visual Models from Natural Language Supervision.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2103.00020">https://doi.org/10.48550/ARXIV.2103.00020</a>.</p>
</div>
<div>
<p>———. 2021b. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:8748–63. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/radford21a.html">http://proceedings.mlr.press/v139/radford21a.html</a>.</p>
</div>
<div>
<p>———. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>International Conference on Machine Learning</em>, 8748–63. PMLR.</p>
</div>
<div>
<p>Radford, Alec, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. “Improving Language Understanding by Generative Pre-Training.”</p>
</div>
<div>
<p>Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. “Language Models Are Unsupervised Multitask Learners.” In.</p>
</div>
<div>
<p>Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. 2019b. “Language Models Are Unsupervised Multitask Learners.” <em>OpenAI Blog</em> 1 (8): 9.</p>
</div>
<div>
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019a. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” <a href="http://arxiv.org/abs/1910.10683">http://arxiv.org/abs/1910.10683</a>.</p>
</div>
<div>
<p>———. 2019b. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1910.10683">http://arxiv.org/abs/1910.10683</a>.</p>
</div>
<div>
<p>Raghu, Maithra, Ben Poole, Jon Kleinberg, Surya Ganguli, and Jascha Sohl-Dickstein. 2016. “On the Expressive Power of Deep Neural Networks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1606.05336">https://doi.org/10.48550/ARXIV.1606.05336</a>.</p>
</div>
<div>
<p>Rajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know What You Don’t Know: Unanswerable Questions for Squad.” <em>arXiv Preprint arXiv:1806.03822</em>.</p>
</div>
<div>
<p>Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “Squad: 100,000+ Questions for Machine Comprehension of Text.” <em>arXiv Preprint arXiv:1606.05250</em>.</p>
</div>
<div>
<p>Ramachandran, Prajit, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon Shlens. 2019. “Stand-Alone Self-Attention in Vision Models.” <em>CoRR</em> abs/1906.05909. <a href="http://arxiv.org/abs/1906.05909">http://arxiv.org/abs/1906.05909</a>.</p>
</div>
<div>
<p>Ramesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022a. “Hierarchical Text-Conditional Image Generation with Clip Latents.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.06125">https://doi.org/10.48550/ARXIV.2204.06125</a>.</p>
</div>
<div>
<p>———. 2022b. “Hierarchical Text-Conditional Image Generation with Clip Latents. 2022.” <em>arXiv Preprint arXiv:2204.06125</em>.</p>
</div>
<div>
<p>Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021a. “Zero-Shot Text-to-Image Generation.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2102.12092">https://doi.org/10.48550/ARXIV.2102.12092</a>.</p>
</div>
<div>
<p>———. 2021b. “Zero-Shot Text-to-Image Generation.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2102.12092">https://arxiv.org/abs/2102.12092</a>.</p>
</div>
<div>
<p>———. 2021c. “Zero-Shot Text-to-Image Generation.” In <em>Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event</em>, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. <a href="http://proceedings.mlr.press/v139/ramesh21a.html">http://proceedings.mlr.press/v139/ramesh21a.html</a>.</p>
</div>
<div>
<p>———. 2021d. “Zero-Shot Text-to-Image Generation.” In <em>Proceedings of the 38th International Conference on Machine Learning</em>, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>.</p>
</div>
<div>
<p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div>
<p>Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. 2017. “Learning Multiple Visual Domains with Residual Adapters.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf</a>.</p>
</div>
<div>
<p>Recht, Benjamin, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. “Do Imagenet Classifiers Generalize to Imagenet?” In <em>International Conference on Machine Learning</em>, 5389–5400. PMLR.</p>
</div>
<div>
<p>Reed, Scott E., Zeynep Akata, Santosh Mohan, Samuel Tenka, Bernt Schiele, and Honglak Lee. 2016. “Learning What and Where to Draw.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1610.02454">http://arxiv.org/abs/1610.02454</a>.</p>
</div>
<div>
<p>Reed, Scott E., Zeynep Akata, Bernt Schiele, and Honglak Lee. 2016. “Learning Deep Representations of Fine-Grained Visual Descriptions.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1605.05395">http://arxiv.org/abs/1605.05395</a>.</p>
</div>
<div>
<p>Reed, Scott E., Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee. 2016. “Generative Adversarial Text to Image Synthesis.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1605.05396">http://arxiv.org/abs/1605.05396</a>.</p>
</div>
<div>
<p>Reed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al. 2022. “A Generalist Agent.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2205.06175">https://doi.org/10.48550/ARXIV.2205.06175</a>.</p>
</div>
<div>
<p>Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2015. “Faster R-Cnn: Towards Real-Time Object Detection with Region Proposal Networks.” <em>Advances in Neural Information Processing Systems</em> 28.</p>
</div>
<div>
<p>Rennie, Steven J., Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. “Self-Critical Sequence Training for Image Captioning.” In <em>2017 Ieee Conference on Computer Vision and Pattern Recognition (Cvpr)</em>, 1179–95. <a href="https://doi.org/10.1109/CVPR.2017.131">https://doi.org/10.1109/CVPR.2017.131</a>.</p>
</div>
<div>
<p>Ribeiro, Marco Tulio, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. “Beyond Accuracy: Behavioral Testing of Nlp Models with Checklist.” <em>arXiv Preprint arXiv:2005.04118</em>.</p>
</div>
<div>
<p>Riquelme, Carlos, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. “Scaling Vision with Sparse Mixture of Experts.” In <em>Advances in Neural Information Processing Systems</em>, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:8583–95. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf</a>.</p>
</div>
<div>
<p>Ritchie, Hannah, Max Roser, and Pablo Rosado. 2020. “<span class="math inline">\(CO_2\)</span> And Greenhouse Gas Emissions.” <em>Our World in Data</em>.</p>
</div>
<div>
<p>Rombach, Robin, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2021. “High-Resolution Image Synthesis with Latent Diffusion Models.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2112.10752">https://arxiv.org/abs/2112.10752</a>.</p>
</div>
<div>
<p>———. 2022. “StableDiffusion.” <a href="https://github.com/CompVis/stable-diffusion">https://github.com/CompVis/stable-diffusion</a>.</p>
</div>
<div>
<p>Rosset, Corby. 2020. “Turing-Nlg: A 17-Billion-Parameter Language Model by Microsoft.” <em>Microsoft Blog</em> 1 (2).</p>
</div>
<div>
<p>Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large Scale Visual Recognition Challenge.” <em>Int. J. Comput. Vision</em> 115 (3): 211–52. <a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>.</p>
</div>
<div>
<p>Rügamer, David, Chris Kolb, and Nadja Klein. 2020. “Semi-Structured Deep Distributional Regression: Combining Structured Additive Models and Deep Learning.” <em>arXiv Preprint arXiv:2002.05777</em>.</p>
</div>
<div>
<p>Saharia, Chitwan, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, et al. 2022a. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” <em>arXiv Preprint arXiv: Arxiv-2205.11487</em>.</p>
</div>
<div>
<p>———. 2022b. “Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2205.11487">https://doi.org/10.48550/ARXIV.2205.11487</a>.</p>
</div>
<div>
<p>Saifee, Moiz. 2020. “GPT-3: The New Mighty Language Model from Openai.”</p>
</div>
<div>
<p>Sajjadi, Mehdi S. M., Olivier Bachem, Mario Lucic, Olivier Bousquet, and Sylvain Gelly. 2018. “Assessing Generative Models via Precision and Recall.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1806.00035">https://doi.org/10.48550/ARXIV.1806.00035</a>.</p>
</div>
<div>
<p>Salimans, Tim, Ian J. Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. 2016. “Improved Techniques for Training Gans.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1606.03498">http://arxiv.org/abs/1606.03498</a>.</p>
</div>
<div>
<p>Schick, Timo, and Hinrich Schütze. 2020. “Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference.” <a href="http://arxiv.org/abs/2001.07676">http://arxiv.org/abs/2001.07676</a>.</p>
</div>
<div>
<p>Schuhmann, C. 2022. “Laion-400-Million Open Dataset.” 2022. <a href="https://laion.ai/blog/laion-400-open-dataset/">https://laion.ai/blog/laion-400-open-dataset/</a>.</p>
</div>
<div>
<p>Schuhmann, Christoph, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021a. “Laion-400m: Open Dataset of Clip-Filtered 400 Million Image-Text Pairs.” <em>arXiv Preprint arXiv:2111.02114</em>.</p>
</div>
<div>
<p>———. 2021b. “LAION-400M: Open Dataset of Clip-Filtered 400 Million Image-Text Pairs.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2111.02114">https://arxiv.org/abs/2111.02114</a>.</p>
</div>
<div>
<p>Sejnowski, Terrence J. 2020. “The Unreasonable Effectiveness of Deep Learning in Artificial Intelligence.” <em>Proceedings of the National Academy of Sciences U.S.A. (2020) Https://Www.pnas.org/Content/Early/2020/01/23/1907373117</em>. <a href="https://doi.org/10.1073/pnas.1907373117">https://doi.org/10.1073/pnas.1907373117</a>.</p>
</div>
<div>
<p>Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2015a. “Neural Machine Translation of Rare Words with Subword Units.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1508.07909">http://arxiv.org/abs/1508.07909</a>.</p>
</div>
<div>
<p>———. 2015b. “Neural Machine Translation of Rare Words with Subword Units.” <em>arXiv Preprint arXiv:1508.07909</em>.</p>
</div>
<div>
<p>———. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 1715–25. Berlin, Germany: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P16-1162">https://doi.org/10.18653/v1/P16-1162</a>.</p>
</div>
<div>
<p>Shah, Deval. 2022. “Self-Supervised Learning and Its Applications - Neptune.ai.” neptune.ai.</p>
</div>
<div>
<p>Shao, Shuai, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. 2019. “Objects365: A Large-Scale, High-Quality Dataset for Object Detection.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 8430–9.</p>
</div>
<div>
<p>Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” In. <a href="https://openreview.net/pdf?id=B1ckMDqlg">https://openreview.net/pdf?id=B1ckMDqlg</a>.</p>
</div>
<div>
<p>Shekhar, Ravi, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017. “Foil It! Find One Mismatch Between Image and Language Caption.” <em>arXiv Preprint arXiv:1705.01359</em>.</p>
</div>
<div>
<p>Shen, Sheng, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. “How Much Can Clip Benefit Vision-and-Language Tasks?” <em>arXiv Preprint arXiv:2107.06383</em>.</p>
</div>
<div>
<p>Sheng, Emily, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. “The Woman Worked as a Babysitter: On Biases in Language Generation.” <em>arXiv Preprint arXiv:1909.01326</em>.</p>
</div>
<div>
<p>Shonenkov, Alex. 2021. “RuDALL-E.” <a href="https://github.com/ai-forever/ru-dalle">https://github.com/ai-forever/ru-dalle</a>.</p>
</div>
<div>
<p>Shvetsova, Nina, Brian Chen, Andrew Rouditchenko, Samuel Thomas, Brian Kingsbury, Rogerio Feris, David Harwath, James Glass, and Hilde Kuehne. 2021. “Everything at Once - Multi-Modal Fusion Transformer for Video Retrieval.” <em>arXiv Preprint arXiv: Arxiv-2112.04446</em>.</p>
</div>
<div>
<p>Sikarwar, Ankur, and Gabriel Kreiman. 2022. “On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering.” <em>arXiv Preprint arXiv:2201.03965</em>.</p>
</div>
<div>
<p>Silberer, Carina, and Mirella Lapata. 2012. “Grounded Models of Semantic Representation.” In <em>Tsujii J, Henderson J, Paşca M, Editors. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning; 2012 Jul 12–14; Jeju Island, Korea. Stroudsburg: ACL; 2012. P. 1423-33.</em> ACL (Association for Computational Linguistics).</p>
</div>
<div>
<p>———. 2014. “Learning Grounded Meaning Representations with Autoencoders.” In <em>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 721–32.</p>
</div>
<div>
<p>Simonyan, Karen, and Andrew Zisserman. 2014. “Very deep convolutional networks for large-scale image recognition.” <em>arXiv Preprint arXiv:1409.1556</em>.</p>
</div>
<div>
<p>Singh, Amanpreet, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. “Flava: A Foundational Language and Vision Alignment Model.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 15638–50.</p>
</div>
<div>
<p>Sirko, Wojciech, Sergii Kashubin, Marvin Ritter, Abigail Annkah, Yasser Salah Eddine Bouchareb, Yann N. Dauphin, Daniel Keysers, Maxim Neumann, Moustapha Cissé, and John Quinn. 2021. “Continental-Scale Building Detection from High Resolution Satellite Imagery.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2107.12283">https://arxiv.org/abs/2107.12283</a>.</p>
</div>
<div>
<p>Snell, Charlie. 2021. “Understanding Vq-Vae.” <a href="https://ml.berkeley.edu/blog/posts/vq-vae/">https://ml.berkeley.edu/blog/posts/vq-vae/</a>.</p>
</div>
<div>
<p>Socher, Richard, and Li Fei-fei. 2010. “Connecting Modalities: Semi-Supervised Segmentation and Annotation of Images Using Unaligned Text Corpora.” In <em>In Ieee Computer Society Conference on Computer Vision and Pattern Recognition</em>.</p>
</div>
<div>
<p>Soderlund, Jacob, and Alan Blair. 2018. “Adversarial Image Generation Using Evolution and Deep Learning.” In <em>2018 Ieee Congress on Evolutionary Computation (Cec)</em>, 1–8. <a href="https://doi.org/10.1109/CEC.2018.8477754">https://doi.org/10.1109/CEC.2018.8477754</a>.</p>
</div>
<div>
<p>Sohl-Dickstein, Jascha, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1503.03585">http://arxiv.org/abs/1503.03585</a>.</p>
</div>
<div>
<p>Srinivasan, Krishna, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021. “Wit: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine Learning.” In <em>Proceedings of the 44th International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 2443–9.</p>
</div>
<div>
<p>Srinivasan, Ramya, and Kanji Uchino. 2021. “Biases in Generative Art: A Causal Look from the Lens of Art History.” In <em>Proceedings of the 2021 Acm Conference on Fairness, Accountability, and Transparency</em>, 41–51.</p>
</div>
<div>
<p>Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, et al. 2022. “Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models.” <em>arXiv Preprint arXiv:2206.04615</em>.</p>
</div>
<div>
<p>Steiner, Andreas, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. 2021. “How to Train Your Vit? Data, Augmentation, and Regularization in Vision Transformers.” <a href="https://doi.org/10.48550/ARXIV.2106.10270">https://doi.org/10.48550/ARXIV.2106.10270</a>.</p>
</div>
<div>
<p>Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019a. “Energy and Policy Considerations for Deep Learning in NLP.” In <em>Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</em>, 3645–50. Florence, Italy: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P19-1355">https://doi.org/10.18653/v1/P19-1355</a>.</p>
</div>
<div>
<p>———. 2019b. “Energy and Policy Considerations for Deep Learning in Nlp.” <em>arXiv Preprint arXiv:1906.02243</em>.</p>
</div>
<div>
<p>———. 2019c. “Energy and Policy Considerations for Deep Learning in Nlp.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1906.02243">https://doi.org/10.48550/ARXIV.1906.02243</a>.</p>
</div>
<div>
<p>Sulubacak, Umut, Ozan Caglayan, Stig-Arne Grönroos, Aku Rouhe, Desmond Elliott, Lucia Specia, and Jörg Tiedemann. 2020. “Multimodal Machine Translation Through Visuals and Speech.” <em>Mach. Transl.</em> 34 (2-3): 97–147. <a href="https://doi.org/10.1007/s10590-020-09250-0">https://doi.org/10.1007/s10590-020-09250-0</a>.</p>
</div>
<div>
<p>Sun, Chen, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. 2017. “Revisiting Unreasonable Effectiveness of Data in Deep Learning Era.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 843–52.</p>
</div>
<div>
<p>Sun, Qingfeng, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, and Daxin Jiang. 2021. “Multimodal Dialogue Response Generation.” <em>arXiv Preprint arXiv:2110.08515</em>.</p>
</div>
<div>
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks.” <a href="http://arxiv.org/abs/1409.3215">http://arxiv.org/abs/1409.3215</a>.</p>
</div>
<div>
<p>Sutton, R. S. 2019. “The Bitter Lesson.” 2019. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>.</p>
</div>
<div>
<p>Szegedy, Christian, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. 2015. “Rethinking the Inception Architecture for Computer Vision.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1512.00567">http://arxiv.org/abs/1512.00567</a>.</p>
</div>
<div>
<p>Tan, Hao, and Mohit Bansal. 2020. “Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision.” <em>arXiv Preprint arXiv:2010.06775</em>.</p>
</div>
<div>
<p>Tan, Mingxing, and Quoc V. Le. 2019a. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <a href="https://doi.org/10.48550/ARXIV.1905.11946">https://doi.org/10.48550/ARXIV.1905.11946</a>.</p>
</div>
<div>
<p>———. 2019b. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <em>CoRR</em> abs/1905.11946. <a href="http://arxiv.org/abs/1905.11946">http://arxiv.org/abs/1905.11946</a>.</p>
</div>
<div>
<p>Tao, Ming, Hao Tang, Songsong Wu, Nicu Sebe, Fei Wu, and Xiao-Yuan Jing. 2020. “DF-GAN: Deep Fusion Generative Adversarial Networks for Text-to-Image Synthesis.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2008.05865">https://arxiv.org/abs/2008.05865</a>.</p>
</div>
<div>
<p>techslang. 2020. “What Is Self-Supervised Learning? — Definition by Techslang.” www.techslang.com.</p>
</div>
<div>
<p>Theis, Lucas, Aäron van den Oord, and Matthias Bethge. 2015. “A Note on the Evaluation of Generative Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1511.01844">https://doi.org/10.48550/ARXIV.1511.01844</a>.</p>
</div>
<div>
<p>Tian, Yonglong, Dilip Krishnan, and Phillip Isola. 2020. “Contrastive Multiview Coding.” In <em>European Conference on Computer Vision</em>, 776–94. Springer.</p>
</div>
<div>
<p>Tiu, Ekin. 2021. “Understanding Contrastive Learning by Ekin Tiu Towards Data Science.” towardsdatascience.com.</p>
</div>
<div>
<p>Tong, Chao, Jun Li, Chao Lang, Fanxin Kong, Jianwei Niu, and Joel JPC Rodrigues. 2018. “An Efficient Deep Model for Day-Ahead Electricity Load Forecasting with Stacked Denoising Auto-Encoders.” <em>Journal of Parallel and Distributed Computing</em> 117: 267–73.</p>
</div>
<div>
<p>Torralba, Antonio, and Alexei A Efros. 2011. “Unbiased Look at Dataset Bias.” In <em>CVPR 2011</em>, 1521–8. IEEE.</p>
</div>
<div>
<p>Uppal, Shagun, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumder, Soujanya Poria, Roger Zimmermann, and Amir Zadeh. 2022. “Multimodal Research in Vision and Language: A Review of Current and Emerging Trends.” <em>Information Fusion</em> 77: 149–71.</p>
</div>
<div>
<p>Vale-Silva, Luı́s A, and Karl Rohr. 2021. “Long-Term Cancer Survival Prediction Using Multimodal Deep Learning.” <em>Scientific Reports</em> 11 (1): 1–12.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017a. “Attention Is All You Need.” <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.</p>
</div>
<div>
<p>———. 2017b. “Attention Is All You Need.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1706.03762">http://arxiv.org/abs/1706.03762</a>.</p>
</div>
<div>
<p>———. 2017c. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, ca, USA</em>, edited by Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman Garnett, 5998–6008. <a href="https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html">https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html</a>.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017a. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017b. “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30.</p>
</div>
<div>
<p>———. 2017c. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
<div>
<p>Vedantam, Ramakrishna, C. Lawrence Zitnick, and Devi Parikh. 2015. “CIDEr: Consensus-Based Image Description Evaluation.” In <em>2015 Ieee Conference on Computer Vision and Pattern Recognition (Cvpr)</em>, 4566–75. <a href="https://doi.org/10.1109/CVPR.2015.7299087">https://doi.org/10.1109/CVPR.2015.7299087</a>.</p>
</div>
<div>
<p>Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. “Show and Tell: A Neural Image Caption Generator.” In, 3156–64. <a href="https://doi.org/10.1109/CVPR.2015.7298935">https://doi.org/10.1109/CVPR.2015.7298935</a>.</p>
</div>
<div>
<p>Voita, Elena, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. 2019. “Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned.” <a href="http://arxiv.org/abs/1905.09418">http://arxiv.org/abs/1905.09418</a>.</p>
</div>
<div>
<p>Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” <em>arXiv Preprint arXiv:1804.07461</em>.</p>
</div>
<div>
<p>Wang, Jun, and Shengchen Li. 2018. “Detection and Classification of Acoustic Scenes and Events 2018 Self-Attention Mechanism Based System for Dcase2018 Challenge Task1 and Task4,” August. <a href="https://doi.org/10.13140/RG.2.2.28317.13281">https://doi.org/10.13140/RG.2.2.28317.13281</a>.</p>
</div>
<div>
<p>Wang, Peng, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. “OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.” In <em>Proceedings of the 39th International Conference on Machine Learning</em>, edited by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, 162:23318–40. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v162/wang22al.html">https://proceedings.mlr.press/v162/wang22al.html</a>.</p>
</div>
<div>
<p>Wang, Qin, Rujia Li, Qi Wang, and Shiping Chen. 2021. “Non-Fungible Token (Nft): Overview, Evaluation, Opportunities and Challenges.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2105.07447">https://doi.org/10.48550/ARXIV.2105.07447</a>.</p>
</div>
<div>
<p>Website. 2020. “Localized Narratives Data and Visualization.” 2020. <a href="https://google.github.io/localized-narratives">https://google.github.io/localized-narratives</a>.</p>
</div>
<div>
<p>Wei, Chen, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. 2022. “Masked Feature Prediction for Self-Supervised Visual Pre-Training.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 14668–78.</p>
</div>
<div>
<p>Weng, Lilian. 2018. “From Autoencoder to Beta-Vae.” <em>Lilianweng.github.io</em>. <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">https://lilianweng.github.io/posts/2018-08-12-vae/</a>.</p>
</div>
<div>
<p>———. 2021. “What Are Diffusion Models?” <em>Lilianweng.github.io</em>. <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/">https://lilianweng.github.io/posts/2021-07-11-diffusion-models/</a>.</p>
</div>
<div>
<p>Wenzek, Guillaume, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2019. “Ccnet: Extracting High Quality Monolingual Datasets from Web Crawl Data.” <em>arXiv Preprint arXiv:1911.00359</em>.</p>
</div>
<div>
<p>Wu, Chenfei, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang, Daxin Jiang, and Nan Duan. 2021. “NÜWA: Visual Synthesis Pre-Training for Neural visUal World creAtion.” <em>arXiv Preprint arXiv: Arxiv-2111.12417</em>.</p>
</div>
<div>
<p>WZRD. 2020. “WZRD.” <a href="https://wzrd.ai/">https://wzrd.ai/</a>.</p>
</div>
<div>
<p>Xiao, Jianxiong, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. “SUN Database: Large-Scale Scene Recognition from Abbey to Zoo.” In <em>2010 Ieee Computer Society Conference on Computer Vision and Pattern Recognition</em>, 3485–92. <a href="https://doi.org/10.1109/CVPR.2010.5539970">https://doi.org/10.1109/CVPR.2010.5539970</a>.</p>
</div>
<div>
<p>Xie, Qizhe, Eduard H. Hovy, Minh-Thang Luong, and Quoc V. Le. 2019. “Self-Training with Noisy Student Improves Imagenet Classification.” <em>CoRR</em> abs/1911.04252. <a href="http://arxiv.org/abs/1911.04252">http://arxiv.org/abs/1911.04252</a>.</p>
</div>
<div>
<p>Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1502.03044">https://doi.org/10.48550/ARXIV.1502.03044</a>.</p>
</div>
<div>
<p>Xu, Tao, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2017. “AttnGAN: Fine-Grained Text to Image Generation with Attentional Generative Adversarial Networks.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1711.10485">http://arxiv.org/abs/1711.10485</a>.</p>
</div>
<div>
<p>Xue, Linting, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. “MT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer.” <em>arXiv Preprint arXiv:2010.11934</em>.</p>
</div>
<div>
<p>Yang, Xu, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. 2019. “Auto-Encoding Scene Graphs for Image Captioning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr)</em>.</p>
</div>
<div>
<p>Yann, Lecun, and Misra Ishan. 2021. “Self-Supervised Learning: The Dark Matter of Intelligence.” 2021. <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</a>.</p>
</div>
<div>
<p>Yao, Benjamin Z., Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. “I2T: Image Parsing to Text Description.” <em>Proceedings of the IEEE</em> 98 (8): 1485–1508. <a href="https://doi.org/10.1109/JPROC.2010.2050411">https://doi.org/10.1109/JPROC.2010.2050411</a>.</p>
</div>
<div>
<p>Yao, Jiawen, Xinliang Zhu, Feiyun Zhu, and Junzhou Huang. 2017. “Deep Correlational Learning for Survival Prediction from Multi-Modality Data.” In <em>Medical Image Computing and Computer-Assisted Intervention − Miccai 2017</em>, 406–14. Springer International Publishing.</p>
</div>
<div>
<p>Yao, Ting, Yingwei Pan, Yehao Li, and Tao Mei. 2018a. “Exploring Visual Relationship for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1809.07041">https://doi.org/10.48550/ARXIV.1809.07041</a>.</p>
</div>
<div>
<p>———. 2018b. “Exploring Visual Relationship for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1809.07041">https://doi.org/10.48550/ARXIV.1809.07041</a>.</p>
</div>
<div>
<p>You, Jiaxuan, Xiaocheng Li, Melvin Low, David Lobell, and Stefano Ermon. 2017. “Deep Gaussian Process for Crop Yield Prediction Based on Remote Sensing Data.” In <em>Proceedings of the Thirty-First Aaai Conference on Artificial Intelligence</em>, 4559–65. AAAI’17. San Francisco, California, USA: AAAI Press.</p>
</div>
<div>
<p>Young, Peter, Alice Lai, Micah Hodosh, and Julia Hockenmaier. 2014. “From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions.” <em>Transactions of the Association for Computational Linguistics</em> 2: 67–78.</p>
</div>
<div>
<p>Yu, Jiahui, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. 2021. “Vector-Quantized Image Modeling with Improved VQGAN.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2110.04627">https://arxiv.org/abs/2110.04627</a>.</p>
</div>
<div>
<p>Yu, Jiahui, Yuanzhong Xu, Jing Koh, Thang Luong, Gunjan Baid, Vijay Vasudevan, Alexander Ku, et al. 2022a. “Scaling Autoregressive Models for Content-Rich Text-to-Image Generation.” <a href="https://doi.org/10.48550/arXiv.2206.10789">https://doi.org/10.48550/arXiv.2206.10789</a>.</p>
</div>
<div>
<p>Yu, Jiahui, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, et al. 2022b. “Scaling Autoregressive Models for Content-Rich Text-to-Image Generation.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2206.10789">https://doi.org/10.48550/ARXIV.2206.10789</a>.</p>
</div>
<div>
<p>Yuan, Lu, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, et al. 2021. “Florence: A New Foundation Model for Computer Vision.” <em>arXiv Preprint arXiv:2111.11432</em>.</p>
</div>
<div>
<p>Yuan, Sha, Zhao Shuai, Leng Jiahong, Xue Zhao, Zhao Hanyu, and Tang Jie. 2022. “WuDaoMM: A Large-Scale Multi-Modal Dataset for Pre-Training Models.” <em>arXiv Preprint arXiv:2203.11480</em>.</p>
</div>
<div>
<p>Zagoruyko, Sergey, and Nikos Komodakis. 2016. “Wide Residual Networks.” <em>CoRR</em> abs/1605.07146. <a href="http://arxiv.org/abs/1605.07146">http://arxiv.org/abs/1605.07146</a>.</p>
</div>
<div>
<p>Zellers, Rowan, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. “From Recognition to Cognition: Visual Commonsense Reasoning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6720–31.</p>
</div>
<div>
<p>Zellers, Rowan, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. “Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.” <em>arXiv Preprint arXiv:1808.05326</em>.</p>
</div>
<div>
<p>Zeng, Andy, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, et al. 2022. “Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.” <em>arXiv Preprint arXiv:2204.00598</em>.</p>
</div>
<div>
<p>Zhang, Chao, Zichao Yang, Xiaodong He, and Li Deng. 2020. “Multimodal Intelligence: Representation Learning, Information Fusion, and Applications.” <em>IEEE J. Sel. Top. Signal Process.</em> 14 (3): 478–93. <a href="https://doi.org/10.1109/JSTSP.2020.2987728">https://doi.org/10.1109/JSTSP.2020.2987728</a>.</p>
</div>
<div>
<p>Zhang, Han, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaolei Huang, Xiaogang Wang, and Dimitris N. Metaxas. 2016. “StackGAN: Text to Photo-Realistic Image Synthesis with Stacked Generative Adversarial Networks.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1612.03242">http://arxiv.org/abs/1612.03242</a>.</p>
</div>
<div>
<p>Zhang, Peng, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016. “Yin and Yang: Balancing and Answering Binary Visual Questions.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 5014–22.</p>
</div>
<div>
<p>Zhang, Yuhao, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. 2020. “Contrastive Learning of Medical Visual Representations from Paired Images and Text.” <em>arXiv Preprint arXiv:2010.00747</em>.</p>
</div>
<div>
<p>Zhou, Bolei, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. 2017. “Scene Parsing Through Ade20k Dataset.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 633–41.</p>
</div>
<div>
<p>Zhou, Yanqi, Sudip Roy, Amirali Abdolrashidi, Daniel Wong, Peter Ma, Qiumin Xu, Hanxiao Liu, et al. 2020. “Transferable Graph Optimizers for Ml Compilers.” <em>NeurIPS 2020</em>. <a href="http://arxiv.org/abs/2010.12438">http://arxiv.org/abs/2010.12438</a>.</p>
</div>
<div>
<p>Zhou, Yufan, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. 2021. “LAFITE: Towards Language-Free Training for Text-to-Image Generation.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2111.13792">https://arxiv.org/abs/2111.13792</a>.</p>
</div>
<div>
<p>Zhu, Minfeng, Pingbo Pan, Wei Chen, and Yi Yang. 2019. “DM-GAN: Dynamic Memory Generative Adversarial Networks for Text-to-Image Synthesis.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1904.01310">http://arxiv.org/abs/1904.01310</a>.</p>
</div>
<div>
<p>Zhu, Xinliang, Jiawen Yao, and Junzhou Huang. 2016. “Deep Convolutional Neural Network for Survival Analysis with Pathological Images.” In <em>2016 Ieee International Conference on Bioinformatics and Biomedicine (Bibm)</em>, 544–47. IEEE.</p>
</div>
<div>
<p>Zhu, Yukun, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015. “Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 19–27.</p>
</div>
<div>
<p>Zhuang, Chengxu, Siming Yan, Aran Nayebi, Martin Schrimpf, Michael C Frank, James J DiCarlo, and Daniel LK Yamins. 2021. “Unsupervised Neural Network Models of the Ventral Visual Stream.” <em>Proceedings of the National Academy of Sciences</em> 118 (3): e2014196118.</p>
</div>
</div>
</div>




















            </section>

          </div>
        </div>
      </div>
<a href="acknowledgements.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
