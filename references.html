<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-09-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="acknowledgements.html"/>

<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Multimodal Deep Learning</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-to-multimodal-deep-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Multimodal Deep Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-01-sota-nlp"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in NLP</a></li>
<li class="chapter" data-level="2.2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-02-sota-cv"><i class="fa fa-check"></i><b>2.2</b> State-of-the-art in Computer Vision</a></li>
<li class="chapter" data-level="2.3" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-03-benchmarks"><i class="fa fa-check"></i><b>2.3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-01-img2text"><i class="fa fa-check"></i><b>3.1</b> Image2Text</a></li>
<li class="chapter" data-level="3.2" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-02-text2img"><i class="fa fa-check"></i><b>3.2</b> Text-2-image</a></li>
<li class="chapter" data-level="3.3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-03-img-support-text"><i class="fa fa-check"></i><b>3.3</b> Images supporting Language Models</a></li>
<li class="chapter" data-level="3.4" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-04-text-support-img"><i class="fa fa-check"></i><b>3.4</b> Text supporting Computer Vision Models</a></li>
<li class="chapter" data-level="3.5" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-05-text-plus-img"><i class="fa fa-check"></i><b>3.5</b> Models for both modalities</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c03-00-further.html"><a href="c03-00-further.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-01-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a></li>
<li class="chapter" data-level="4.2" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-02-structured-unstructured"><i class="fa fa-check"></i><b>4.2</b> Structured + Unstructured Data</a></li>
<li class="chapter" data-level="4.3" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-03-multi-purpose"><i class="fa fa-check"></i><b>4.3</b> Multi-Purpose Models</a></li>
<li class="chapter" data-level="4.4" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-04-usecase"><i class="fa fa-check"></i><b>4.4</b> Generative Art</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>6</b> Epilogue</a><ul>
<li class="chapter" data-level="6.1" data-path="epilogue.html"><a href="epilogue.html#new-influential-architectures"><i class="fa fa-check"></i><b>6.1</b> New influential architectures</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>7</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered hasAnchor">
<h1>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<div id="refs" class="references">
<div>
<p>Agirre, Eneko, Enrique Alfonseca, Keith Hall, Jana Kravalova, Marius Pasca, and Aitor Soroa. 2009. “A Study on Similarity and Relatedness Using Distributional and Wordnet-Based Approaches.”</p>
</div>
<div>
<p>Ailem, Melissa, Bowen Zhang, Aurelien Bellet, Pascal Denis, and Fei Sha. 2018. “A Probabilistic Model for Joint Learning of Word Embeddings from Texts and Images.”</p>
</div>
<div>
<p>Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” <em>arXiv Preprint arXiv:2204.14198</em>.</p>
</div>
<div>
<p>———. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.14198">https://doi.org/10.48550/ARXIV.2204.14198</a>.</p>
</div>
<div>
<p>Alford, A. 2021. “Google Announces 800M Parameter Vision-Language Ai Model Align.” July 20, 2021. <a href="https://www.infoq.com/news/2021/07/google-vision-language-ai/">https://www.infoq.com/news/2021/07/google-vision-language-ai/</a>.</p>
</div>
<div>
<p>Anderson, Peter, Basura Fernando, Mark Johnson, and Stephen Gould. 2016. “SPICE: Semantic Propositional Image Caption Evaluation.” In <em>Computer Vision – Eccv 2016</em>, edited by Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, 382–98. Cham: Springer International Publishing.</p>
</div>
<div>
<p>Anderson, Peter, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. “Bottom-up and Top-down Attention for Image Captioning and Visual Question Answering.” In <em>2018 Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6077–86. <a href="https://doi.org/10.1109/CVPR.2018.00636">https://doi.org/10.1109/CVPR.2018.00636</a>.</p>
</div>
<div>
<p>Antol, Stanislaw, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. “Vqa: Visual Question Answering.” In <em>Proceedings of the Ieee International Conference on Computer Vision</em>, 2425–33.</p>
</div>
<div>
<p>Aran, Komatsuzaki. 2021. “When You Generate Images with Vqgan Clip, the Image Quality Dramatically Improves If You Add "Unreal Engine" to Your Prompt. People Are Now Calling This "Unreal Engine Trick".” Twitter. <a href="https://twitter.com/arankomatsuzaki/status/1399471244760649729">https://twitter.com/arankomatsuzaki/status/1399471244760649729</a>.</p>
</div>
<div>
<p>Baevski, Alexei, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. “Data2vec: A General Framework for Self-Supervised Learning in Speech, Vision and Language.” <em>arXiv Preprint arXiv:2202.03555</em>.</p>
</div>
<div>
<p>Baevski, Alexei, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. “Wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations.” <em>Advances in Neural Information Processing Systems</em> 33: 12449–60.</p>
</div>
<div>
<p>Baltrušaitis, Tadas, Chaitanya Ahuja, and Louis-Philippe Morency. 2019. “Multimodal Machine Learning: A Survey and Taxonomy.” <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 41 (2): 423–43. <a href="https://doi.org/10.1109/TPAMI.2018.2798607">https://doi.org/10.1109/TPAMI.2018.2798607</a>.</p>
</div>
<div>
<p>Bandy, Jack, and Nicholas Vincent. 2021. “Addressing" Documentation Debt" in Machine Learning Research: A Retrospective Datasheet for Bookcorpus.” <em>arXiv Preprint arXiv:2105.05241</em>.</p>
</div>
<div>
<p>Banerjee, Satanjeev, and Alon Lavie. 2005. “METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments.” In <em>Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization</em>, 65–72. Ann Arbor, Michigan: Association for Computational Linguistics. <a href="https://aclanthology.org/W05-0909">https://aclanthology.org/W05-0909</a>.</p>
</div>
<div>
<p>Bao, Hangbo, Li Dong, and Furu Wei. 2021. “Beit: Bert Pre-Training of Image Transformers.” <em>arXiv Preprint arXiv:2106.08254</em>.</p>
</div>
<div>
<p>Barham, Paul, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, et al. 2022. “Pathways: Asynchronous Distributed Dataflow for Ml.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2203.12533">https://doi.org/10.48550/ARXIV.2203.12533</a>.</p>
</div>
<div>
<p>Bäck, Thomas, and Hans-Paul Schwefel. 1993. “An Overview of Evolutionary Algorithms for Parameter Optimization.” <em>Evolutionary Computation</em> 1 (1): 1–23. <a href="https://doi.org/10.1162/evco.1993.1.1.1">https://doi.org/10.1162/evco.1993.1.1.1</a>.</p>
</div>
<div>
<p>Bellemare, Marc G., Yavar Naddaf, Joel Veness, and Michael Bowling. 2013. “The Arcade Learning Environment: An Evaluation Platform for General Agents.” <em>J. Artif. Int. Res.</em> 47 (1): 253–79.</p>
</div>
<div>
<p>Beyer, Lucas, Olivier J Hénaff, Alexander Kolesnikov, Xiaohua Zhai, and Aäron van den Oord. 2020. “Are We Done with Imagenet?” <em>arXiv Preprint arXiv:2006.07159</em>.</p>
</div>
<div>
<p>Birhane, Abeba, Vinay Uday Prabhu, and Emmanuel Kahembwe. 2021. “Multimodal Datasets: Misogyny, Pornography, and Malignant Stereotypes.” <em>arXiv Preprint arXiv:2110.01963</em>.</p>
</div>
<div>
<p>Boris, Dayma. 2022. “DALL·E Mini.” <a href="https://huggingface.co/spaces/dalle-mini/dalle-mini">https://huggingface.co/spaces/dalle-mini/dalle-mini</a>.</p>
</div>
<div>
<p>Bosch, Anna, Andrew Zisserman, and Xavier Munoz. 2007. “Image Classification Using Random Forests and Ferns.” In <em>2007 Ieee 11th International Conference on Computer Vision</em>, 1–8. Ieee.</p>
</div>
<div>
<p>Bowman, Samuel R, and George E Dahl. 2021. “What Will It Take to Fix Benchmarking in Natural Language Understanding?” <em>arXiv Preprint arXiv:2104.02145</em>.</p>
</div>
<div>
<p>Bromley, Jane, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. 1993. “Signature Verification Using a" Siamese" Time Delay Neural Network.” <em>Advances in Neural Information Processing Systems</em> 6.</p>
</div>
<div>
<p>Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2005.14165">https://doi.org/10.48550/ARXIV.2005.14165</a>.</p>
</div>
<div>
<p>Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.</p>
</div>
<div>
<p>Bruni, Elia, Nam-Khanh Tran, and Marco Baroni. 2014. “Multimodal Distributional Semantics.” <em>Journal of Artificial Intelligence Research</em> 49: 1–47.</p>
</div>
<div>
<p>Brysbaert, Marc, Amy Beth Warriner, and Victor Kuperman. 2014. “Concreteness Ratings for 40 Thousand Generally Known English Word Lemmas.” <em>Behavior Research Methods</em> 46 (3): 904–11.</p>
</div>
<div>
<p>Carion, Nicolas, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. “End-to-End Object Detection with Transformers.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2005.12872">https://arxiv.org/abs/2005.12872</a>.</p>
</div>
<div>
<p>Caron, Mathilde, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. “Emerging Properties in Self-Supervised Vision Transformers.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 9650–60.</p>
</div>
<div>
<p>Cheerla, Anika, and Olivier Gevaert. 2019. “Deep Learning with Multimodal Representation for Pancancer Prognosis Prediction.” <em>Bioinformatics</em> 35 (14): i446–i454.</p>
</div>
<div>
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. “A Simple Framework for Contrastive Learning of Visual Representations.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.05709">https://doi.org/10.48550/ARXIV.2002.05709</a>.</p>
</div>
<div>
<p>Chen, Xinlei, Saining Xie, and Kaiming He. 2021. “An Empirical Study of Training Self-Supervised Vision Transformers.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 9640–9.</p>
</div>
<div>
<p>Cheng, Heng-Tze, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye, Glen Anderson, et al. 2016. “Wide &amp; Deep Learning for Recommender Systems.” In <em>Proceedings of the 1st Workshop on Deep Learning for Recommender Systems</em>, 7–10.</p>
</div>
<div>
<p>Chowdhery, Aakanksha, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, et al. 2022. “Palm: Scaling Language Modeling with Pathways.” <em>arXiv Preprint arXiv:2204.02311</em>.</p>
</div>
<div>
<p>———. 2022. “PaLM: Scaling Language Modeling with Pathways.” <em>Arxiv:2204.02311</em>. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>.</p>
</div>
<div>
<p>Collell, Guillem, Ted Zhang, and Marie-Francine Moens. 2017. “Imagined Visual Representations as Multimodal Embeddings.” In <em>Proceedings of the Aaai Conference on Artificial Intelligence</em>. Vol. 31. 1.</p>
</div>
<div>
<p>Cornia, Marcella, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2019. “Meshed-Memory Transformer for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1912.08226">https://doi.org/10.48550/ARXIV.1912.08226</a>.</p>
</div>
<div>
<p>Crawshaw, Michael. 2020. “Multi-Task Learning with Deep Neural Networks: A Survey.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2009.09796">https://doi.org/10.48550/ARXIV.2009.09796</a>.</p>
</div>
<div>
<p>Das, Abhishek, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. 2017. “Human Attention in Visual Question Answering: Do Humans and Deep Networks Look at the Same Regions?” <em>Computer Vision and Image Understanding</em> 163: 90–100.</p>
</div>
<div>
<p>Dean, Jeff. 2021. “Introducing Pathways: A Next-Generation Ai Architecture.” 2021. <a href="https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/">https://blog.google/technology/ai/introducing-pathways-next-generation-ai-architecture/</a>.</p>
</div>
<div>
<p>Dean, Jeffrey. 2020. “1.1 the Deep Learning Revolution and Its Implications for Computer Architecture and Chip Design.” In <em>2020 Ieee International Solid- State Circuits Conference - (Isscc)</em>, 8–14. <a href="https://doi.org/10.1109/ISSCC19947.2020.9063049">https://doi.org/10.1109/ISSCC19947.2020.9063049</a>.</p>
</div>
<div>
<p>Dehouche, Nassim. 2021. “Plagiarism in the Age of Massive Generative Pre-Trained Transformers (Gpt-3).” <em>Ethics in Science and Environmental Politics</em> 21: 17–23.</p>
</div>
<div>
<p>Deng, Jia, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. “Imagenet: A Large-Scale Hierarchical Image Database.” In <em>2009 Ieee Conference on Computer Vision and Pattern Recognition</em>, 248–55. Ieee.</p>
</div>
<div>
<p>Devereux, Barry J, Lorraine K Tyler, Jeroen Geertzen, and Billi Randall. 2014. “The Centre for Speech, Language and the Brain (Cslb) Concept Property Norms.” <em>Behavior Research Methods</em> 46 (4): 1119–27.</p>
</div>
<div>
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018a. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1810.04805">https://doi.org/10.48550/ARXIV.1810.04805</a>.</p>
</div>
<div>
<p>———. 2018b. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” <em>arXiv Preprint arXiv:1810.04805</em>.</p>
</div>
<div>
<p>———. 2019. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” In, 4171–86. Minneapolis, Minnesota: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/N19-1423">https://doi.org/10.18653/v1/N19-1423</a>.</p>
</div>
<div>
<p>———. 2018c. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” October. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div>
<p>Dhamala, Jwala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. “Bold: Dataset and Metrics for Measuring Biases in Open-Ended Language Generation.” In <em>Proceedings of the 2021 Acm Conference on Fairness, Accountability, and Transparency</em>, 862–72.</p>
</div>
<div>
<p>Dhariwal, Prafulla, and Alex Nichol. 2021. “Diffusion Models Beat Gans on Image Synthesis.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a>.</p>
</div>
<div>
<p>Doerr, Benjamin, and Frank Neumann. 2021. “A Survey on Recent Progress in the Theory of Evolutionary Algorithms for Discrete Optimization.” <em>ACM Trans. Evol. Learn. Optim.</em> 1 (4). <a href="https://doi.org/10.1145/3472304">https://doi.org/10.1145/3472304</a>.</p>
</div>
<div>
<p>Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” <em>arXiv Preprint arXiv:2010.11929</em>.</p>
</div>
<div>
<p>Esser, Patrick, Robin Rombach, and Bjorn Ommer. 2021. “Taming Transformers for High-Resolution Image Synthesis.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 12873–83.</p>
</div>
<div>
<p>Esser, Patrick, Robin Rombach, and Björn Ommer. 2020. “A Note on Data Biases in Generative Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2012.02516">https://doi.org/10.48550/ARXIV.2012.02516</a>.</p>
</div>
<div>
<p>Everingham, Mark, Luc van Gool, Christopher K. I. Williams, John Winn, and Andrew Zisserman. 2010. “The Pascal Visual Object Classes (Voc) Challenge.” <em>International Journal of Computer Vision</em> 88 (2): 303–38. <a href="https://doi.org/10.1007/s11263-009-0275-4">https://doi.org/10.1007/s11263-009-0275-4</a>.</p>
</div>
<div>
<p>Fellbaum, Christiane D. 2000. “WordNet : An Electronic Lexical Database.” <em>Language</em> 76: 706.</p>
</div>
<div>
<p>Fernando, Chrisantha, Dylan Banarse, Charles Blundell, Yori Zwols, David Ha, Andrei A. Rusu, Alexander Pritzel, and Daan Wierstra. 2017. “PathNet: Evolution Channels Gradient Descent in Super Neural Networks.” In. <a href="https://arxiv.org/abs/1701.08734">https://arxiv.org/abs/1701.08734</a>.</p>
</div>
<div>
<p>Galanter, Philip. 2016. “Generative Art Theory.” <em>A Companion to Digital Art</em> 1: 631.</p>
</div>
<div>
<p>Gao, Jiyang, Zhen Li, Ram Nevatia, and others. 2017. “Knowledge Concentration: Learning 100k Object Classifiers in a Single Cnn.” <em>arXiv Preprint arXiv:1711.07607</em>.</p>
</div>
<div>
<p>Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. 2016. “A Neural Algorithm of Artistic Style.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1508.06576">https://doi.org/10.48550/ARXIV.1508.06576</a>.</p>
</div>
<div>
<p>Gesmundo, Andrea, and Jeff Dean. 2022. “MuNet: Evolving Pretrained Deep Neural Networks into Scalable Auto-Tuning Multitask Systems.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2205.10937">https://doi.org/10.48550/ARXIV.2205.10937</a>.</p>
</div>
<div>
<p>Gokaslan, Aaron, and Vanya Cohen. 2019. “OpenWebText Corpus.”</p>
</div>
<div>
<p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Networks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1406.2661">https://doi.org/10.48550/ARXIV.1406.2661</a>.</p>
</div>
<div>
<p>Goodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. “Explaining and Harnessing Adversarial Examples.” <em>arXiv Preprint arXiv:1412.6572</em>.</p>
</div>
<div>
<p>Goodfellow, Ian, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. “Generative Adversarial Nets.” In <em>Advances in Neural Information Processing Systems</em>, edited by Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K. Q. Weinberger. Vol. 27. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf">https://proceedings.neurips.cc/paper/2014/file/5ca3e9b122f61f8f06494c97b1afccf3-Paper.pdf</a>.</p>
</div>
<div>
<p>Goyal, Yash, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2017. “Making the V in Vqa Matter: Elevating the Role of Image Understanding in Visual Question Answering.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 6904–13.</p>
</div>
<div>
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020a. “Bootstrap Your Own Latent-a New Approach to Self-Supervised Learning.” <em>Advances in Neural Information Processing Systems</em> 33: 21271–84.</p>
</div>
<div>
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020b. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2006.07733">https://doi.org/10.48550/ARXIV.2006.07733</a>.</p>
</div>
<div>
<p>Guo, Yandong, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. 2016. “Ms-Celeb-1m: A Dataset and Benchmark for Large-Scale Face Recognition.” In <em>European Conference on Computer Vision</em>, 87–102. Springer.</p>
</div>
<div>
<p>Harnad, Stevan. 1990. “The Symbol Grounding Problem.” <em>Physica D: Nonlinear Phenomena</em> 42 (1-3): 335–46.</p>
</div>
<div>
<p>Harris, Z, and others. 1954. “Distributional Hypothesis.” <em>Word World</em> 10 (23): 146–62.</p>
</div>
<div>
<p>He, Kaiming, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. 2022. “Masked Autoencoders Are Scalable Vision Learners.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 16000–16009.</p>
</div>
<div>
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div>
<p>Henderson, Peter, Jieru Hu, Joshua Romoff, Emma Brunskill, Dan Jurafsky, and Joelle Pineau. 2020. “Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning.” <em>Journal of Machine Learning Research</em> 21 (248): 1–43.</p>
</div>
<div>
<p>Herdade, Simao, Armin Kappeler, Kofi Boakye, and Joao Soares. 2019. “Image Captioning: Transforming Objects into Words.” In, 11135–45. <a href="http://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words">http://papers.nips.cc/paper/9293-image-captioning-transforming-objects-into-words</a>.</p>
</div>
<div>
<p>Hill, Felix, and Anna Korhonen. 2014. “Learning Abstract Concept Embeddings from Multi-Modal Data: Since You Probably Can’t See What I Mean.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 255–65.</p>
</div>
<div>
<p>Hill, Felix, Roi Reichart, and Anna Korhonen. 2015. “Simlex-999: Evaluating Semantic Models with (Genuine) Similarity Estimation.” <em>Computational Linguistics</em> 41 (4): 665–95.</p>
</div>
<div>
<p>Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. 2015. “Distilling the Knowledge in a Neural Network.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1503.02531">https://doi.org/10.48550/ARXIV.1503.02531</a>.</p>
</div>
<div>
<p>Hochreiter, Sepp, and Jürgen Schmidhuber. 1997. “Long Short-Term Memory.” <em>Neural Computation</em> 9 (8): 1735–80.</p>
</div>
<div>
<p>Hoffmann, Jordan, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, et al. 2022. “Training Compute-Optimal Large Language Models.” <em>arXiv Preprint arXiv:2203.15556</em>.</p>
</div>
<div>
<p>Hu, Ronghang, and Amanpreet Singh. 2021a. “Unit: Multimodal Multitask Learning with a Unified Transformer.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 1439–49.</p>
</div>
<div>
<p>———. 2021b. “UniT: Multimodal Multitask Learning with a Unified Transformer.” In <em>2021 Ieee/Cvf International Conference on Computer Vision (Iccv)</em>, 1419–29. <a href="https://doi.org/10.1109/ICCV48922.2021.00147">https://doi.org/10.1109/ICCV48922.2021.00147</a>.</p>
</div>
<div>
<p>Huang, Lun, Wenmin Wang, Jie Chen, and Xiao-Yong Wei. 2019. “Attention on Attention for Image Captioning.” In, 4633–42. <a href="https://doi.org/10.1109/ICCV.2019.00473">https://doi.org/10.1109/ICCV.2019.00473</a>.</p>
</div>
<div>
<p>Huang, Shih-Cheng, Anuj Pareek, Saeed Seyyedi, Imon Banerjee, and Matthew P Lungren. 2020. “Fusion of Medical Imaging and Electronic Health Records Using Deep Learning: A Systematic Review and Implementation Guidelines.” <em>NPJ Digital Medicine</em> 3 (1): 1–9.</p>
</div>
<div>
<p>Hudson, Drew A, and Christopher D Manning. 2019. “Gqa: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6700–6709.</p>
</div>
<div>
<p>Ive, Julia, Pranava Madhyastha, and Lucia Specia. 2019. “Distilling Translations with Visual Awareness.” <em>arXiv Preprint arXiv:1906.07701</em>.</p>
</div>
<div>
<p>Jacobs, Robert A., Michael I. Jordan, Steven J. Nowlan, and Geoffrey E. Hinton. 1991. “Adaptive Mixtures of Local Experts.” <em>Neural Computation</em> 3 (1): 79–87. <a href="https://doi.org/10.1162/neco.1991.3.1.79">https://doi.org/10.1162/neco.1991.3.1.79</a>.</p>
</div>
<div>
<p>Jaegle, Andrew, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. 2021. “Perceiver: General Perception with Iterative Attention.” In <em>International Conference on Machine Learning</em>, 4651–64. PMLR.</p>
</div>
<div>
<p>Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021a. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” In <em>International Conference on Machine Learning</em>, 4904–16. PMLR.</p>
</div>
<div>
<p>Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021b. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2102.05918">https://arxiv.org/abs/2102.05918</a>.</p>
</div>
<div>
<p>Jordan, Michael I., and Robert A. Jacobs. 1994. “Hierarchical Mixtures of Experts and the Em Algorithm.” <em>Neural Computation</em> 6 (2): 181–214. <a href="https://doi.org/10.1162/neco.1994.6.2.181">https://doi.org/10.1162/neco.1994.6.2.181</a>.</p>
</div>
<div>
<p>Joshi, Gargi, Rahee Walambe, and Ketan Kotecha. 2021. “A Review on Explainability in Multimodal Deep Neural Nets.” <em>IEEE Access</em> 9: 59800–59821. <a href="https://doi.org/10.1109/ACCESS.2021.3070212">https://doi.org/10.1109/ACCESS.2021.3070212</a>.</p>
</div>
<div>
<p>Kaiser, Lukasz, Aidan N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion Jones, and Jakob Uszkoreit. 2017. “One Model to Learn Them All.” <em>arXiv</em>. <a href="https://arxiv.org/pdf/1706.05137.pdf">https://arxiv.org/pdf/1706.05137.pdf</a>.</p>
</div>
<div>
<p>Karpathy, Andrej, and Li Fei-Fei. 2014. “Deep Visual-Semantic Alignments for Generating Image Descriptions.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1412.2306">https://doi.org/10.48550/ARXIV.1412.2306</a>.</p>
</div>
<div>
<p>Karras, Tero, Samuli Laine, and Timo Aila. 2019. “A Style-Based Generator Architecture for Generative Adversarial Networks.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 4401–10.</p>
</div>
<div>
<p>Katzman, Jared L, Uri Shaham, Alexander Cloninger, Jonathan Bates, Tingting Jiang, and Yuval Kluger. 2018. “DeepSurv: Personalized Treatment Recommender System Using a Cox Proportional Hazards Deep Neural Network.” <em>BMC Medical Research Methodology</em> 18 (1): 1–12.</p>
</div>
<div>
<p>Kiela, Douwe, and Léon Bottou. 2014. “Learning Image Embeddings Using Convolutional Neural Networks for Improved Multi-Modal Semantics.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 36–45.</p>
</div>
<div>
<p>Kiela, Douwe, Alexis Conneau, Allan Jabri, and Maximilian Nickel. 2017. “Learning Visually Grounded Sentence Representations.” <em>arXiv Preprint arXiv:1707.06320</em>.</p>
</div>
<div>
<p>Kingma, Diederik P., and Max Welling. 2019. “An Introduction to Variational Autoencoders.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1906.02691">http://arxiv.org/abs/1906.02691</a>.</p>
</div>
<div>
<p>Kiros, Jamie, William Chan, and Geoffrey Hinton. 2018. “Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search.” In <em>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 922–33.</p>
</div>
<div>
<p>Koehn, Philipp. 2005. “Europarl: A Parallel Corpus for Statistical Machine Translation.” In <em>Proceedings of Machine Translation Summit X: Papers</em>, 79–86.</p>
</div>
<div>
<p>Kolesnikov, Alexander, Lucas Beyer, Xiaohua Zhai, Joan Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby. 2019. “Large Scale Learning of General Visual Representations for Transfer.” <em>arXiv Preprint arXiv:1912.11370</em> 2 (8).</p>
</div>
<div>
<p>Kopper, Philipp, Simon Wiegrebe, Bernd Bischl, Andreas Bender, and David Rügamer. 2022. “DeepPAMM: Deep Piecewise Exponential Additive Mixed Models for Complex Hazard Structures in Survival Analysis.” In <em>Pacific-Asia Conference on Knowledge Discovery and Data Mining</em>, 249–61. Springer.</p>
</div>
<div>
<p>Kottur, Satwik, Ramakrishna Vedantam, José MF Moura, and Devi Parikh. 2016. “Visual Word2vec (Vis-W2v): Learning Visually Grounded Word Embeddings Using Abstract Scenes.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 4985–94.</p>
</div>
<div>
<p>Krishna, Ranjay, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, et al. 2016. “Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations.” In. <a href="https://arxiv.org/abs/1602.07332">https://arxiv.org/abs/1602.07332</a>.</p>
</div>
<div>
<p>Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E Hinton. 2012. “Imagenet Classification with Deep Convolutional Neural Networks.” <em>Advances in Neural Information Processing Systems</em> 25.</p>
</div>
<div>
<p>Kudo, Taku, and John Richardson. 2018. “SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing.” In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, 66–71. Brussels, Belgium: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/D18-2012">https://doi.org/10.18653/v1/D18-2012</a>.</p>
</div>
<div>
<p>Lazaridou, Angeliki, Nghia The Pham, and Marco Baroni. 2015. “Combining Language and Vision with a Multimodal Skip-Gram Model.” <em>arXiv Preprint arXiv:1501.02598</em>.</p>
</div>
<div>
<p>Lewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. “BART: Denoising Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension.” In <em>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, 7871–80. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.acl-main.703">https://doi.org/10.18653/v1/2020.acl-main.703</a>.</p>
</div>
<div>
<p>Lewkowycz, Aitor, Anders Andreassen, David Martin Dohan, Ethan S Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, et al. 2022. “Solving Quantitative Reasoning Problems with Language Models.” Technical report. <a href="https://arxiv.org/abs/2206.14858">https://arxiv.org/abs/2206.14858</a>.</p>
</div>
<div>
<p>Lin, Chin-Yew. 2004. “ROUGE: A Package for Automatic Evaluation of Summaries.” In <em>Text Summarization Branches Out</em>, 74–81. Barcelona, Spain: Association for Computational Linguistics. <a href="https://aclanthology.org/W04-1013">https://aclanthology.org/W04-1013</a>.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2014. “Microsoft Coco: Common Objects in Context.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1405.0312">https://doi.org/10.48550/ARXIV.1405.0312</a>.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick. 2014a. “Microsoft Coco: Common Objects in Context.” In <em>European Conference on Computer Vision</em>, 740–55. Springer.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014b. “Microsoft Coco: Common Objects in Context.” In <em>Computer Vision – Eccv 2014</em>, 740–55. Springer International Publishing.</p>
</div>
<div>
<p>Liu, Yinhan, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. “Roberta: A Robustly Optimized Bert Pretraining Approach.” <em>arXiv Preprint arXiv:1907.11692</em>.</p>
</div>
<div>
<p>Lottick, Kadan, Silvia Susai, Sorelle A Friedler, and Jonathan P Wilson. 2019. “Energy Usage Reports: Environmental Awareness as Part of Algorithmic Accountability.” <em>arXiv Preprint arXiv:1911.08354</em>.</p>
</div>
<div>
<p>Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019a. “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1908.02265">https://doi.org/10.48550/ARXIV.1908.02265</a>.</p>
</div>
<div>
<p>———. 2019b. “Vilbert: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” <em>Advances in Neural Information Processing Systems</em> 32.</p>
</div>
<div>
<p>Lu, Yujie, Wanrong Zhu, Xin Eric Wang, Miguel Eckstein, and William Yang Wang. 2022. “Imagination-Augmented Natural Language Understanding.” <em>arXiv Preprint arXiv:2204.08535</em>.</p>
</div>
<div>
<p>Mahajan, Dhruv, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. 2018. “Exploring the Limits of Weakly Supervised Pretraining.” In <em>Proceedings of the European Conference on Computer Vision (Eccv)</em>, 181–96.</p>
</div>
<div>
<p>Mayer, Thomas, and Michael Cysouw. 2014. “Creating a Massively Parallel Bible Corpus.” <em>Oceania</em> 135 (273): 40.</p>
</div>
<div>
<p>Mccormack, Jon, and Camilo Cruz Gambardella. 2022. “Growing and Evolving 3-d Prints.” <em>IEEE Transactions on Evolutionary Computation</em> 26 (1): 88–99. <a href="https://doi.org/10.1109/TEVC.2021.3095156">https://doi.org/10.1109/TEVC.2021.3095156</a>.</p>
</div>
<div>
<p>MICHAEL BARTHEL, JESSE HOLCOMB, GALEN STOCKING, and AMY MITCHELL. 2016. “Reddit News Users More Likely to Be Male, Young and Digital in Their News Preferences.” 2016. <a href="https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/">https://www.pewresearch.org/journalism/2016/02/25/reddit-news-users-more-likely-to-be-male-young-and-digital-in-their-news-preferences/</a>.</p>
</div>
<div>
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” <em>arXiv Preprint arXiv:1301.3781</em>.</p>
</div>
<div>
<p>Mineault, Patrick. 2021. “Unsupervised Models of the Brain.” 2021. <a href="https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/">https://xcorr.net/2021/12/31/2021-in-review-unsupervised-brain-models/</a>.</p>
</div>
<div>
<p>Mircosoft. 2019. “Evaluate:Detection.” 2019. <a href="https://cocodataset.org/#detection-eval">https://cocodataset.org/#detection-eval</a>.</p>
</div>
<div>
<p>Mordvintsev, Alexander. 2015. “Inceptionism: Going Deeper into Neural Networks.” <em>Google AI Blog</em>. Google. <a href="https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html">https://ai.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html</a>.</p>
</div>
<div>
<p>Mustafa, Basil, Carlos Riquelme, Joan Puigcerver, Rodolphe Jenatton, and Neil Houlsby. 2022. “Multimodal Contrastive Learning with Limoe: The Language-Image Mixture of Experts.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2206.02770">https://doi.org/10.48550/ARXIV.2206.02770</a>.</p>
</div>
<div>
<p>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. “GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2112.10741">https://doi.org/10.48550/ARXIV.2112.10741</a>.</p>
</div>
<div>
<p>OpenAI. 2021. “DALL-E.” <a href="https://github.com/openai/DALL-E">https://github.com/openai/DALL-E</a>.</p>
</div>
<div>
<p>Papineni, Kishore, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. “Bleu: A Method for Automatic Evaluation of Machine Translation.” In <em>Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, 311–18. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics. <a href="https://doi.org/10.3115/1073083.1073135">https://doi.org/10.3115/1073083.1073135</a>.</p>
</div>
<div>
<p>Parcalabescu, Letitia, Michele Cafagna, Lilitta Muradjan, Anette Frank, Iacer Calixto, and Albert Gatt. 2022. “VALSE: A Task-Independent Benchmark for Vision and Language Models Centered on Linguistic Phenomena.” In <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 8253–80. Association for Computational Linguistics. <a href="https://aclanthology.org/2022.acl-long.567">https://aclanthology.org/2022.acl-long.567</a>.</p>
</div>
<div>
<p>Patashnik, Or, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. “StyleCLIP: Text-Driven Manipulation of Stylegan Imagery.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2103.17249">https://arxiv.org/abs/2103.17249</a>.</p>
</div>
<div>
<p>Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014. “Glove: Global Vectors for Word Representation.” In <em>Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (Emnlp)</em>, 1532–43.</p>
</div>
<div>
<p>Perez, Ethan, Douwe Kiela, and Kyunghyun Cho. 2021. “True Few-Shot Learning with Language Models.” <em>Advances in Neural Information Processing Systems</em> 34: 11054–70.</p>
</div>
<div>
<p>Pezzelle, Sandro, Ece Takmaz, and Raquel Fernández. 2021. “Word Representation Learning in Multimodal Pre-Trained Transformers: An Intrinsic Evaluation.” <em>Transactions of the Association for Computational Linguistics</em> 9: 1563–79.</p>
</div>
<div>
<p>Pölsterl, Sebastian, Ignacio Sarasua, Benjamı́n Gutiérrez-Becker, and Christian Wachinger. 2019. “A Wide and Deep Neural Network for Survival Analysis from Anatomical Shape and Tabular Clinical Data.” In <em>Joint European Conference on Machine Learning and Knowledge Discovery in Databases</em>, 453–64. Springer.</p>
</div>
<div>
<p>Prabhu, Vinay Uday, and Abeba Birhane. 2020. “Large Image Datasets: A Pyrrhic Win for Computer Vision?” <em>arXiv Preprint arXiv:2006.16923</em>.</p>
</div>
<div>
<p>Qiao, Han, Vivian Liu, and Lydia Chilton. 2022. “Initial Images: Using Image Prompts to Improve Subject Representation in Multimodal Ai Generated Art.” In <em>Creativity and Cognition</em>, 15–28.</p>
</div>
<div>
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2103.00020">https://doi.org/10.48550/ARXIV.2103.00020</a>.</p>
</div>
<div>
<p>———. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>International Conference on Machine Learning</em>, 8748–63. PMLR.</p>
</div>
<div>
<p>Radford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019a. “Language Models Are Unsupervised Multitask Learners.” In.</p>
</div>
<div>
<p>Radford, Alec, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, and others. 2019b. “Language Models Are Unsupervised Multitask Learners.” <em>OpenAI Blog</em> 1 (8): 9.</p>
</div>
<div>
<p>Rajpurkar, Pranav, Robin Jia, and Percy Liang. 2018. “Know What You Don’t Know: Unanswerable Questions for Squad.” <em>arXiv Preprint arXiv:1806.03822</em>.</p>
</div>
<div>
<p>Rajpurkar, Pranav, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. “Squad: 100,000+ Questions for Machine Comprehension of Text.” <em>arXiv Preprint arXiv:1606.05250</em>.</p>
</div>
<div>
<p>Ramesh, Aditya, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. “Hierarchical Text-Conditional Image Generation with Clip Latents.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.06125">https://doi.org/10.48550/ARXIV.2204.06125</a>.</p>
</div>
<div>
<p>Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021a. “Zero-Shot Text-to-Image Generation.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2102.12092">https://doi.org/10.48550/ARXIV.2102.12092</a>.</p>
</div>
<div>
<p>———. 2021b. “Zero-Shot Text-to-Image Generation.” In <em>Proceedings of the 38th International Conference on Machine Learning</em>, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>.</p>
</div>
<div>
<p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div>
<p>Rebuffi, Sylvestre-Alvise, Hakan Bilen, and Andrea Vedaldi. 2017. “Learning Multiple Visual Domains with Residual Adapters.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/e7b24b112a44fdd9ee93bdf998c6ca0e-Paper.pdf</a>.</p>
</div>
<div>
<p>Recht, Benjamin, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. “Do Imagenet Classifiers Generalize to Imagenet?” In <em>International Conference on Machine Learning</em>, 5389–5400. PMLR.</p>
</div>
<div>
<p>Reed, Scott, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, et al. 2022. “A Generalist Agent.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2205.06175">https://doi.org/10.48550/ARXIV.2205.06175</a>.</p>
</div>
<div>
<p>Ren, Shaoqing, Kaiming He, Ross Girshick, and Jian Sun. 2015. “Faster R-Cnn: Towards Real-Time Object Detection with Region Proposal Networks.” <em>Advances in Neural Information Processing Systems</em> 28.</p>
</div>
<div>
<p>Rennie, Steven J., Etienne Marcheret, Youssef Mroueh, Jerret Ross, and Vaibhava Goel. 2017. “Self-Critical Sequence Training for Image Captioning.” In <em>2017 Ieee Conference on Computer Vision and Pattern Recognition (Cvpr)</em>, 1179–95. <a href="https://doi.org/10.1109/CVPR.2017.131">https://doi.org/10.1109/CVPR.2017.131</a>.</p>
</div>
<div>
<p>Ribeiro, Marco Tulio, Tongshuang Wu, Carlos Guestrin, and Sameer Singh. 2020. “Beyond Accuracy: Behavioral Testing of Nlp Models with Checklist.” <em>arXiv Preprint arXiv:2005.04118</em>.</p>
</div>
<div>
<p>Riquelme, Carlos, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. “Scaling Vision with Sparse Mixture of Experts.” In <em>Advances in Neural Information Processing Systems</em>, edited by M. Ranzato, A. Beygelzimer, Y. Dauphin, P. S. Liang, and J. Wortman Vaughan, 34:8583–95. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf">https://proceedings.neurips.cc/paper/2021/file/48237d9f2dea8c74c2a72126cf63d933-Paper.pdf</a>.</p>
</div>
<div>
<p>Russakovsky, Olga, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, et al. 2015. “ImageNet Large Scale Visual Recognition Challenge.” <em>Int. J. Comput. Vision</em> 115 (3): 211–52. <a href="https://doi.org/10.1007/s11263-015-0816-y">https://doi.org/10.1007/s11263-015-0816-y</a>.</p>
</div>
<div>
<p>Rügamer, David, Chris Kolb, and Nadja Klein. 2020. “Semi-Structured Deep Distributional Regression: Combining Structured Additive Models and Deep Learning.” <em>arXiv Preprint arXiv:2002.05777</em>.</p>
</div>
<div>
<p>Schuhmann, C. 2022. “Laion-400-Million Open Dataset.” July 7, 2022. <a href="https://laion.ai/blog/laion-400-open-dataset/">https://laion.ai/blog/laion-400-open-dataset/</a>.</p>
</div>
<div>
<p>Schuhmann, Christoph, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. “LAION-400M: Open Dataset of Clip-Filtered 400 Million Image-Text Pairs.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2111.02114">https://arxiv.org/abs/2111.02114</a>.</p>
</div>
<div>
<p>Sennrich, Rico, Barry Haddow, and Alexandra Birch. 2015. “Neural Machine Translation of Rare Words with Subword Units.” <em>arXiv Preprint arXiv:1508.07909</em>.</p>
</div>
<div>
<p>———. 2016. “Neural Machine Translation of Rare Words with Subword Units.” In <em>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 1715–25. Berlin, Germany: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/P16-1162">https://doi.org/10.18653/v1/P16-1162</a>.</p>
</div>
<div>
<p>Shao, Shuai, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. 2019. “Objects365: A Large-Scale, High-Quality Dataset for Object Detection.” In <em>Proceedings of the Ieee/Cvf International Conference on Computer Vision</em>, 8430–9.</p>
</div>
<div>
<p>Shazeer, Noam, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. “Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer.” In. <a href="https://openreview.net/pdf?id=B1ckMDqlg">https://openreview.net/pdf?id=B1ckMDqlg</a>.</p>
</div>
<div>
<p>Shekhar, Ravi, Sandro Pezzelle, Yauhen Klimovich, Aurélie Herbelot, Moin Nabi, Enver Sangineto, and Raffaella Bernardi. 2017. “Foil It! Find One Mismatch Between Image and Language Caption.” <em>arXiv Preprint arXiv:1705.01359</em>.</p>
</div>
<div>
<p>Shen, Sheng, Liunian Harold Li, Hao Tan, Mohit Bansal, Anna Rohrbach, Kai-Wei Chang, Zhewei Yao, and Kurt Keutzer. 2021. “How Much Can Clip Benefit Vision-and-Language Tasks?” <em>arXiv Preprint arXiv:2107.06383</em>.</p>
</div>
<div>
<p>Sheng, Emily, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. “The Woman Worked as a Babysitter: On Biases in Language Generation.” <em>arXiv Preprint arXiv:1909.01326</em>.</p>
</div>
<div>
<p>Shonenkov, Alex. 2021. “RuDALL-E.” <a href="https://github.com/ai-forever/ru-dalle">https://github.com/ai-forever/ru-dalle</a>.</p>
</div>
<div>
<p>Sikarwar, Ankur, and Gabriel Kreiman. 2022. “On the Efficacy of Co-Attention Transformer Layers in Visual Question Answering.” <em>arXiv Preprint arXiv:2201.03965</em>.</p>
</div>
<div>
<p>Silberer, Carina, and Mirella Lapata. 2012. “Grounded Models of Semantic Representation.” In <em>Tsujii J, Henderson J, Paşca M, Editors. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning; 2012 Jul 12–14; Jeju Island, Korea. Stroudsburg: ACL; 2012. P. 1423-33.</em> ACL (Association for Computational Linguistics).</p>
</div>
<div>
<p>———. 2014. “Learning Grounded Meaning Representations with Autoencoders.” In <em>Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, 721–32.</p>
</div>
<div>
<p>Singh, Amanpreet, Ronghang Hu, Vedanuj Goswami, Guillaume Couairon, Wojciech Galuba, Marcus Rohrbach, and Douwe Kiela. 2022. “Flava: A Foundational Language and Vision Alignment Model.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 15638–50.</p>
</div>
<div>
<p>Socher, Richard, and Li Fei-fei. 2010. “Connecting Modalities: Semi-Supervised Segmentation and Annotation of Images Using Unaligned Text Corpora.” In <em>In Ieee Computer Society Conference on Computer Vision and Pattern Recognition</em>.</p>
</div>
<div>
<p>Soderlund, Jacob, and Alan Blair. 2018. “Adversarial Image Generation Using Evolution and Deep Learning.” In <em>2018 Ieee Congress on Evolutionary Computation (Cec)</em>, 1–8. <a href="https://doi.org/10.1109/CEC.2018.8477754">https://doi.org/10.1109/CEC.2018.8477754</a>.</p>
</div>
<div>
<p>Srinivasan, Krishna, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. 2021. “Wit: Wikipedia-Based Image Text Dataset for Multimodal Multilingual Machine Learning.” In <em>Proceedings of the 44th International Acm Sigir Conference on Research and Development in Information Retrieval</em>, 2443–9.</p>
</div>
<div>
<p>Srinivasan, Ramya, and Kanji Uchino. 2021. “Biases in Generative Art: A Causal Look from the Lens of Art History.” In <em>Proceedings of the 2021 Acm Conference on Fairness, Accountability, and Transparency</em>, 41–51.</p>
</div>
<div>
<p>Srivastava, Aarohi, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, et al. 2022. “Beyond the Imitation Game: Quantifying and Extrapolating the Capabilities of Language Models.” <em>arXiv Preprint arXiv:2206.04615</em>.</p>
</div>
<div>
<p>Steiner, Andreas, Alexander Kolesnikov, Xiaohua Zhai, Ross Wightman, Jakob Uszkoreit, and Lucas Beyer. 2021. “How to Train Your Vit? Data, Augmentation, and Regularization in Vision Transformers.” <a href="https://doi.org/10.48550/ARXIV.2106.10270">https://doi.org/10.48550/ARXIV.2106.10270</a>.</p>
</div>
<div>
<p>Strubell, Emma, Ananya Ganesh, and Andrew McCallum. 2019a. “Energy and Policy Considerations for Deep Learning in Nlp.” <em>arXiv Preprint arXiv:1906.02243</em>.</p>
</div>
<div>
<p>———. 2019b. “Energy and Policy Considerations for Deep Learning in Nlp.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1906.02243">https://doi.org/10.48550/ARXIV.1906.02243</a>.</p>
</div>
<div>
<p>Sun, Qingfeng, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica Zhang, Xiubo Geng, and Daxin Jiang. 2021. “Multimodal Dialogue Response Generation.” <em>arXiv Preprint arXiv:2110.08515</em>.</p>
</div>
<div>
<p>Sutton, R. S. 2019. “The Bitter Lesson.” March 13, 2019. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>.</p>
</div>
<div>
<p>Tan, Hao, and Mohit Bansal. 2020. “Vokenization: Improving Language Understanding with Contextualized, Visual-Grounded Supervision.” <em>arXiv Preprint arXiv:2010.06775</em>.</p>
</div>
<div>
<p>Tan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <a href="https://doi.org/10.48550/ARXIV.1905.11946">https://doi.org/10.48550/ARXIV.1905.11946</a>.</p>
</div>
<div>
<p>Tong, Chao, Jun Li, Chao Lang, Fanxin Kong, Jianwei Niu, and Joel JPC Rodrigues. 2018. “An Efficient Deep Model for Day-Ahead Electricity Load Forecasting with Stacked Denoising Auto-Encoders.” <em>Journal of Parallel and Distributed Computing</em> 117: 267–73.</p>
</div>
<div>
<p>Uppal, Shagun, Sarthak Bhagat, Devamanyu Hazarika, Navonil Majumder, Soujanya Poria, Roger Zimmermann, and Amir Zadeh. 2022. “Multimodal Research in Vision and Language: A Review of Current and Emerging Trends.” <em>Information Fusion</em> 77: 149–71.</p>
</div>
<div>
<p>Vale-Silva, Luı́s A, and Karl Rohr. 2021. “Long-Term Cancer Survival Prediction Using Multimodal Deep Learning.” <em>Scientific Reports</em> 11 (1): 1–12.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017a. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017b. “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30.</p>
</div>
<div>
<p>———. 2017c. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
<div>
<p>Vedantam, Ramakrishna, C. Lawrence Zitnick, and Devi Parikh. 2015. “CIDEr: Consensus-Based Image Description Evaluation.” In <em>2015 Ieee Conference on Computer Vision and Pattern Recognition (Cvpr)</em>, 4566–75. <a href="https://doi.org/10.1109/CVPR.2015.7299087">https://doi.org/10.1109/CVPR.2015.7299087</a>.</p>
</div>
<div>
<p>Vinyals, Oriol, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. “Show and Tell: A Neural Image Caption Generator.” In, 3156–64. <a href="https://doi.org/10.1109/CVPR.2015.7298935">https://doi.org/10.1109/CVPR.2015.7298935</a>.</p>
</div>
<div>
<p>Wang, Alex, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. “GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding.” <em>arXiv Preprint arXiv:1804.07461</em>.</p>
</div>
<div>
<p>Wang, Peng, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. “OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework.” In <em>Proceedings of the 39th International Conference on Machine Learning</em>, edited by Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, 162:23318–40. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v162/wang22al.html">https://proceedings.mlr.press/v162/wang22al.html</a>.</p>
</div>
<div>
<p>Wang, Qin, Rujia Li, Qi Wang, and Shiping Chen. 2021. “Non-Fungible Token (Nft): Overview, Evaluation, Opportunities and Challenges.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2105.07447">https://doi.org/10.48550/ARXIV.2105.07447</a>.</p>
</div>
<div>
<p>Wei, Chen, Haoqi Fan, Saining Xie, Chao-Yuan Wu, Alan Yuille, and Christoph Feichtenhofer. 2022. “Masked Feature Prediction for Self-Supervised Visual Pre-Training.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 14668–78.</p>
</div>
<div>
<p>Wenzek, Guillaume, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2019. “Ccnet: Extracting High Quality Monolingual Datasets from Web Crawl Data.” <em>arXiv Preprint arXiv:1911.00359</em>.</p>
</div>
<div>
<p>WZRD. 2020. “WZRD.” <a href="https://wzrd.ai/">https://wzrd.ai/</a>.</p>
</div>
<div>
<p>Xiao, Jianxiong, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. “SUN Database: Large-Scale Scene Recognition from Abbey to Zoo.” In <em>2010 Ieee Computer Society Conference on Computer Vision and Pattern Recognition</em>, 3485–92. <a href="https://doi.org/10.1109/CVPR.2010.5539970">https://doi.org/10.1109/CVPR.2010.5539970</a>.</p>
</div>
<div>
<p>Xu, Kelvin, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. 2015. “Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1502.03044">https://doi.org/10.48550/ARXIV.1502.03044</a>.</p>
</div>
<div>
<p>Xue, Linting, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. “MT5: A Massively Multilingual Pre-Trained Text-to-Text Transformer.” <em>arXiv Preprint arXiv:2010.11934</em>.</p>
</div>
<div>
<p>Yang, Xu, Kaihua Tang, Hanwang Zhang, and Jianfei Cai. 2019. “Auto-Encoding Scene Graphs for Image Captioning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition (Cvpr)</em>.</p>
</div>
<div>
<p>Yann, Lecun, and Misra Ishan. 2021. “Self-Supervised Learning: The Dark Matter of Intelligence.” 2021. <a href="https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/">https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/</a>.</p>
</div>
<div>
<p>Yao, Benjamin Z., Xiong Yang, Liang Lin, Mun Wai Lee, and Song-Chun Zhu. 2010. “I2T: Image Parsing to Text Description.” <em>Proceedings of the IEEE</em> 98 (8): 1485–1508. <a href="https://doi.org/10.1109/JPROC.2010.2050411">https://doi.org/10.1109/JPROC.2010.2050411</a>.</p>
</div>
<div>
<p>Yao, Ting, Yingwei Pan, Yehao Li, and Tao Mei. 2018a. “Exploring Visual Relationship for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1809.07041">https://doi.org/10.48550/ARXIV.1809.07041</a>.</p>
</div>
<div>
<p>———. 2018b. “Exploring Visual Relationship for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1809.07041">https://doi.org/10.48550/ARXIV.1809.07041</a>.</p>
</div>
<div>
<p>Yu, Jiahui, Yuanzhong Xu, Jing Koh, Thang Luong, Gunjan Baid, Vijay Vasudevan, Alexander Ku, et al. 2022. “Scaling Autoregressive Models for Content-Rich Text-to-Image Generation,” June. <a href="https://doi.org/10.48550/arXiv.2206.10789">https://doi.org/10.48550/arXiv.2206.10789</a>.</p>
</div>
<div>
<p>Yuan, Lu, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, et al. 2021. “Florence: A New Foundation Model for Computer Vision.” <em>arXiv Preprint arXiv:2111.11432</em>.</p>
</div>
<div>
<p>Yuan, Sha, Zhao Shuai, Leng Jiahong, Xue Zhao, Zhao Hanyu, and Tang Jie. 2022. “WuDaoMM: A Large-Scale Multi-Modal Dataset for Pre-Training Models.” <em>arXiv Preprint arXiv:2203.11480</em>.</p>
</div>
<div>
<p>Zellers, Rowan, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. “From Recognition to Cognition: Visual Commonsense Reasoning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>, 6720–31.</p>
</div>
<div>
<p>Zellers, Rowan, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. “Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference.” <em>arXiv Preprint arXiv:1808.05326</em>.</p>
</div>
<div>
<p>Zeng, Andy, Adrian Wong, Stefan Welker, Krzysztof Choromanski, Federico Tombari, Aveek Purohit, Michael Ryoo, et al. 2022. “Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language.” <em>arXiv Preprint arXiv:2204.00598</em>.</p>
</div>
<div>
<p>Zhang, Peng, Yash Goyal, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. 2016. “Yin and Yang: Balancing and Answering Binary Visual Questions.” In <em>Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition</em>, 5014–22.</p>
</div>
<div>
<p>Zhang, Yuhao, Hang Jiang, Yasuhide Miura, Christopher D Manning, and Curtis P Langlotz. 2020. “Contrastive Learning of Medical Visual Representations from Paired Images and Text.” <em>arXiv Preprint arXiv:2010.00747</em>.</p>
</div>
<div>
<p>Zhu, Xinliang, Jiawen Yao, and Junzhou Huang. 2016. “Deep Convolutional Neural Network for Survival Analysis with Pathological Images.” In <em>2016 Ieee International Conference on Bioinformatics and Biomedicine (Bibm)</em>, 544–47. IEEE.</p>
</div>
<div>
<p>Zhuang, Chengxu, Siming Yan, Aran Nayebi, Martin Schrimpf, Michael C Frank, James J DiCarlo, and Daniel LK Yamins. 2021. “Unsupervised Neural Network Models of the Ventral Visual Stream.” <em>Proceedings of the National Academy of Sciences</em> 118 (3): e2014196118.</p>
</div>
</div>
</div>




















            </section>

          </div>
        </div>
      </div>
<a href="acknowledgements.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
