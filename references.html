<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Multimodal Deep Learning</title>
  <meta name="description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  

<meta name="author" content="" />


<meta name="date" content="2022-06-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="acknowledgements.html"/>

<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multimodal Deep Learning></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-1.html"><a href="chapter-1.html"><i class="fa fa-check"></i><b>2</b> Chapter 1</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter-1.html"><a href="chapter-1.html#title"><i class="fa fa-check"></i><b>2.1</b> title</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-1.html"><a href="chapter-1.html#title-1"><i class="fa fa-check"></i><b>2.2</b> title</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"><a href="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"><i class="fa fa-check"></i><b>3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
<li class="chapter" data-level="4" data-path="chapter-1-1.html"><a href="chapter-1-1.html"><i class="fa fa-check"></i><b>4</b> Chapter 1</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter-1-1.html"><a href="chapter-1-1.html#lorem-ipsum"><i class="fa fa-check"></i><b>4.1</b> Lorem Ipsum</a></li>
<li class="chapter" data-level="4.2" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-figures"><i class="fa fa-check"></i><b>4.2</b> Using Figures</a></li>
<li class="chapter" data-level="4.3" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-tex"><i class="fa fa-check"></i><b>4.3</b> Using Tex</a></li>
<li class="chapter" data-level="4.4" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-stored-results"><i class="fa fa-check"></i><b>4.4</b> Using Stored Results</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="title-2.html"><a href="title-2.html"><i class="fa fa-check"></i><b>5</b> title</a></li>
<li class="chapter" data-level="6" data-path="title-3.html"><a href="title-3.html"><i class="fa fa-check"></i><b>6</b> title</a></li>
<li class="chapter" data-level="7" data-path="title-4.html"><a href="title-4.html"><i class="fa fa-check"></i><b>7</b> title</a></li>
<li class="chapter" data-level="8" data-path="title-5.html"><a href="title-5.html"><i class="fa fa-check"></i><b>8</b> title</a></li>
<li class="chapter" data-level="9" data-path="title-6.html"><a href="title-6.html"><i class="fa fa-check"></i><b>9</b> title</a></li>
<li class="chapter" data-level="10" data-path="title-7.html"><a href="title-7.html"><i class="fa fa-check"></i><b>10</b> title</a></li>
<li class="chapter" data-level="11" data-path="chapter-2-multimodal-architectures.html"><a href="chapter-2-multimodal-architectures.html"><i class="fa fa-check"></i><b>11</b> Chapter 2 Multimodal architectures</a><ul>
<li class="chapter" data-level="11.1" data-path="chapter-2-multimodal-architectures.html"><a href="chapter-2-multimodal-architectures.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="further-topics.html"><a href="further-topics.html"><i class="fa fa-check"></i><b>12</b> Further Topics</a></li>
<li class="chapter" data-level="13" data-path="further-modalities.html"><a href="further-modalities.html"><i class="fa fa-check"></i><b>13</b> Further Modalities</a><ul>
<li class="chapter" data-level="13.1" data-path="further-modalities.html"><a href="further-modalities.html#intro"><i class="fa fa-check"></i><b>13.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="strucutered-unstrucutered-data.html"><a href="strucutered-unstrucutered-data.html"><i class="fa fa-check"></i><b>14</b> Strucutered + Unstrucutered Data</a><ul>
<li class="chapter" data-level="14.1" data-path="strucutered-unstrucutered-data.html"><a href="strucutered-unstrucutered-data.html#intro-1"><i class="fa fa-check"></i><b>14.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="multi-purpose-models.html"><a href="multi-purpose-models.html"><i class="fa fa-check"></i><b>15</b> Multi-purpose Models</a><ul>
<li class="chapter" data-level="15.1" data-path="multi-purpose-models.html"><a href="multi-purpose-models.html#intro-2"><i class="fa fa-check"></i><b>15.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="title-8.html"><a href="title-8.html"><i class="fa fa-check"></i><b>16</b> title</a></li>
<li class="chapter" data-level="17" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>17</b> Epilogue</a><ul>
<li class="chapter" data-level="17.1" data-path="epilogue.html"><a href="epilogue.html#test"><i class="fa fa-check"></i><b>17.1</b> test</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>18</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered hasAnchor">
<h1>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<div id="refs" class="references">
<div>
<p>Baevski, Alexei, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. “Data2vec: A General Framework for Self-Supervised Learning in Speech, Vision and Language.” <em>arXiv Preprint arXiv:2202.03555</em>.</p>
</div>
<div>
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate,” September. <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.</p>
</div>
<div>
<p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. “Enriching Word Vectors with Subword Information,” July. <a href="http://arxiv.org/abs/1607.04606">http://arxiv.org/abs/1607.04606</a>.</p>
</div>
<div>
<p>Bordes, Patrick, Eloi Zablocki, Laure Soulier, Benjamin Piwowarski, and Patrick Gallinari. 2020. “Incorporating Visual Semantics into Sentence Representations Within a Grounded Space.” <em>arXiv Preprint arXiv:2002.02734</em>.</p>
</div>
<div>
<p>Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.</p>
</div>
<div>
<p>Caron, Mathilde, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.” <em>CoRR</em> abs/2006.09882. <a href="https://arxiv.org/abs/2006.09882">https://arxiv.org/abs/2006.09882</a>.</p>
</div>
<div>
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. “A Simple Framework for Contrastive Learning of Visual Representations.” <em>CoRR</em> abs/2002.05709. <a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a>.</p>
</div>
<div>
<p>Cornia, Marcella, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. “Meshed-Memory Transformer for Image Captioning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>.</p>
</div>
<div>
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” October. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div>
<p>Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.</p>
</div>
<div>
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” <em>CoRR</em> abs/2006.07733. <a href="https://arxiv.org/abs/2006.07733">https://arxiv.org/abs/2006.07733</a>.</p>
</div>
<div>
<p>Harnad, Stevan. 1990. “The Symbol Grounding Problem.” <em>Physica D: Nonlinear Phenomena</em> 42 (1-3): 335–46.</p>
</div>
<div>
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” <em>CoRR</em> abs/1512.03385. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div>
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. “Microsoft Coco: Common Objects in Context.” In <em>Computer Vision – Eccv 2014</em>, edited by David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, 740–55. Cham: Springer International Publishing.</p>
</div>
<div>
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space,” January. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</p>
</div>
<div>
<p>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. “GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.” <em>CoRR</em> abs/2112.10741. <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a>.</p>
</div>
<div>
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>International Conference on Machine Learning</em>, 8748–63. PMLR.</p>
</div>
<div>
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,” October. <a href="http://arxiv.org/abs/1910.10683">http://arxiv.org/abs/1910.10683</a>.</p>
</div>
<div>
<p>Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot Text-to-Image Generation.” In <em>Proceedings of the 38th International Conference on Machine Learning</em>, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>.</p>
</div>
<div>
<p>R Core Team. 2018. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div>
<p>Silberer, Carina, and Mirella Lapata. 2012. “Grounded Models of Semantic Representation.” In <em>Tsujii J, Henderson J, Paşca M, Editors. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning; 2012 Jul 12–14; Jeju Island, Korea. Stroudsburg: ACL; 2012. P. 1423-33.</em> ACL (Association for Computational Linguistics).</p>
</div>
<div>
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks,” September. <a href="http://arxiv.org/abs/1409.3215">http://arxiv.org/abs/1409.3215</a>.</p>
</div>
<div>
<p>Tan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <em>CoRR</em> abs/1905.11946. <a href="http://arxiv.org/abs/1905.11946">http://arxiv.org/abs/1905.11946</a>.</p>
</div>
<div>
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30.</p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="acknowledgements.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
