<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Chapter 1 | Multimodal Deep Learning</title>
  <meta name="description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Chapter 1 | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Chapter 1 | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  

<meta name="author" content="" />


<meta name="date" content="2022-06-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction.html"/>
<link rel="next" href="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multimodal Deep Learning></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-1.html"><a href="chapter-1.html"><i class="fa fa-check"></i><b>2</b> Chapter 1</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter-1.html"><a href="chapter-1.html#title"><i class="fa fa-check"></i><b>2.1</b> title</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-1.html"><a href="chapter-1.html#title-1"><i class="fa fa-check"></i><b>2.2</b> title</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"><a href="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"><i class="fa fa-check"></i><b>3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
<li class="chapter" data-level="4" data-path="chapter-1-1.html"><a href="chapter-1-1.html"><i class="fa fa-check"></i><b>4</b> Chapter 1</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter-1-1.html"><a href="chapter-1-1.html#lorem-ipsum"><i class="fa fa-check"></i><b>4.1</b> Lorem Ipsum</a></li>
<li class="chapter" data-level="4.2" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-figures"><i class="fa fa-check"></i><b>4.2</b> Using Figures</a></li>
<li class="chapter" data-level="4.3" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-tex"><i class="fa fa-check"></i><b>4.3</b> Using Tex</a></li>
<li class="chapter" data-level="4.4" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-stored-results"><i class="fa fa-check"></i><b>4.4</b> Using Stored Results</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="title-2.html"><a href="title-2.html"><i class="fa fa-check"></i><b>5</b> title</a></li>
<li class="chapter" data-level="6" data-path="title-3.html"><a href="title-3.html"><i class="fa fa-check"></i><b>6</b> title</a></li>
<li class="chapter" data-level="7" data-path="title-4.html"><a href="title-4.html"><i class="fa fa-check"></i><b>7</b> title</a></li>
<li class="chapter" data-level="8" data-path="title-5.html"><a href="title-5.html"><i class="fa fa-check"></i><b>8</b> title</a></li>
<li class="chapter" data-level="9" data-path="title-6.html"><a href="title-6.html"><i class="fa fa-check"></i><b>9</b> title</a></li>
<li class="chapter" data-level="10" data-path="title-7.html"><a href="title-7.html"><i class="fa fa-check"></i><b>10</b> title</a></li>
<li class="chapter" data-level="11" data-path="chapter-2-multimodal-architectures.html"><a href="chapter-2-multimodal-architectures.html"><i class="fa fa-check"></i><b>11</b> Chapter 2 Multimodal architectures</a><ul>
<li class="chapter" data-level="11.1" data-path="chapter-2-multimodal-architectures.html"><a href="chapter-2-multimodal-architectures.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="further-topics.html"><a href="further-topics.html"><i class="fa fa-check"></i><b>12</b> Further Topics</a></li>
<li class="chapter" data-level="13" data-path="further-modalities.html"><a href="further-modalities.html"><i class="fa fa-check"></i><b>13</b> Further Modalities</a><ul>
<li class="chapter" data-level="13.1" data-path="further-modalities.html"><a href="further-modalities.html#intro"><i class="fa fa-check"></i><b>13.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="strucutered-unstrucutered-data.html"><a href="strucutered-unstrucutered-data.html"><i class="fa fa-check"></i><b>14</b> Strucutered + Unstrucutered Data</a><ul>
<li class="chapter" data-level="14.1" data-path="strucutered-unstrucutered-data.html"><a href="strucutered-unstrucutered-data.html#intro-1"><i class="fa fa-check"></i><b>14.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="multi-purpose-models.html"><a href="multi-purpose-models.html"><i class="fa fa-check"></i><b>15</b> Multi-purpose Models</a><ul>
<li class="chapter" data-level="15.1" data-path="multi-purpose-models.html"><a href="multi-purpose-models.html#intro-2"><i class="fa fa-check"></i><b>15.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="title-8.html"><a href="title-8.html"><i class="fa fa-check"></i><b>16</b> title</a></li>
<li class="chapter" data-level="17" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>17</b> Epilogue</a><ul>
<li class="chapter" data-level="17.1" data-path="epilogue.html"><a href="epilogue.html#test"><i class="fa fa-check"></i><b>17.1</b> test</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>18</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-1" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 2</span> Chapter 1<a href="chapter-1.html#chapter-1" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Authors: Cem Akkus, Vladana Djakovic, Christopher Benjamin Marquardt</em></p>
<p><em>Supervisor: Dr. Matthias Aßenmacher</em></p>
<p>Natural Language Processing (NLP) has existed for about 50 years, but it is more relevant than ever. There have been several breakthroughs in this branch of machine learning that is concerned with spoken and written language. For example, learning internal representations of words was one of the greater advances of the last decade. Word embeddings (<span class="citation">Mikolov et al. (<a href="#ref-Mikolov2013" role="doc-biblioref">2013</a>)</span>, <span class="citation">Bojanowski et al. (<a href="#ref-Bojanowski2016" role="doc-biblioref">2016</a>)</span>) made it possible and allowed developers to encode words as dense vectors that capture their underlying semantic content. In this way, similar words are embedded close to each other in a lower-dimensional feature space. Another important challenge was solved by Encoder-decoder (also called sequence-to-sequence) architectures <span class="citation">Sutskever, Vinyals, and Le (<a href="#ref-Sutskever2014" role="doc-biblioref">2014</a>)</span>, which made it possible to map input sequences to output sequences of different lengths. They are especially useful for complex tasks like machine translation, video captioning or question answering. This approach makes minimal assumptions on the sequence structure and can deal with different word orders and active, as well as passive voice.</p>
<p>A definitely significant state-of-the-art technique is Attention <span class="citation">Bahdanau, Cho, and Bengio (<a href="#ref-Bahdanau2014" role="doc-biblioref">2014</a>)</span>, which enables models to actively shift their focus – just like humans do. It allows following one thought at a time while suppressing information irrelevant to the task. As a consequence, it has been shown to significantly improve performance for tasks like machine translation. By giving the decoder access to directly look at the source, the bottleneck is avoided and at the same time, it provides a shortcut to faraway states and thus helps with the vanishing gradient problem. One of the most recent sequence data modeling techniques is Transformers (<span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>), which are solely based on attention and do not have to process the input data sequentially (like RNNs). Therefore, the deep learning model is better in remembering context-induced earlier in long sequences. It is the dominant paradigm in NLP currently and even makes better use of GPUs, because it can perform parallel operations. Transformer architectures like BERT (<span class="citation">Devlin et al. (<a href="#ref-Devlin2018" role="doc-biblioref">2018</a>)</span>), T5 (<span class="citation">Raffel et al. (<a href="#ref-Raffel2019" role="doc-biblioref">2019</a>)</span>) or GPT-3 (<span class="citation">Brown et al. (<a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>) are pre-trained on a large corpus and can be fine-tuned for specific language tasks. They have the capability to generate stories, poems, code and much more. With the help of the aforementioned breakthroughs, deep networks have been successful in retrieving information and finding representations of semantics in the modality text. In the next paragraphs, developments for another modality image are going to be presented.</p>
<p>Computer vision (CV) focuses on replicating parts of the complexity of the human visual system and enabling computers to identify and process objects in images and videos in the same way that humans do. In recent years it has become one of the main and widely applied fields of computer science. However, there are still problems that are current research topics, whose solutions depend on the research’s view on the topic. One of the problems is how to optimize deep convolutional neural networks for image classification. The accuracy of classification depends on width, depth and image resolution. One way to address the degradation of training accuracy is by introducing a deep residual learning framework <span class="citation">(He et al. <a href="#ref-ResNet" role="doc-biblioref">2015</a>)</span>. On the other hand, another less common method is to scale up ConvNets, to achieve better accuracy is by scaling up image resolution. Based on this observation, there was proposed a simple yet effective compound scaling method, called EfficientNets <span class="citation">(Tan and Le <a href="#ref-EfficientNet" role="doc-biblioref">2019</a>)</span>.</p>
<p>Another state-of-the-art trend in computer vision is learning effective visual representations without human supervision. Discriminative approaches based on contrastive learning in the latent space have recently shown great promise, achieving state-of-the-art results, but the simple framework for contrastive learning of visual representations, which is called SimCLR, outperforms previous work <span class="citation">(Chen et al. <a href="#ref-SimCLR" role="doc-biblioref">2020</a>)</span>. However, another research proposes as an alternative a simple “swapped” prediction problem where we predict the code of a view from the representation of another view. Where features are learned by Swapping Assignments between multiple Views of the same image (SwAV) <span class="citation">(Caron et al. <a href="#ref-SwAV" role="doc-biblioref">2020</a>)</span>.
Further recent contrastive methods are trained by reducing the distance between representations of different augmented views of the same image (‘positive pairs’) and increasing the distance between representations of augmented views from different images (‘negative pairs’). Bootstrap Your Own Latent (BYOL) is a new algorithm for self-supervised learning of image representatios <span class="citation">(Grill et al. <a href="#ref-BYOL" role="doc-biblioref">2020</a>)</span>.</p>
<p>Self-attention-based architectures, in particular, Transformers have become the model of choice in natural language processing (NLP). Inspired by NLP successes, multiple works try combining CNN-like architectures with self-attention, some replacing the convolutions entirely. The latter models, while theoretically efficient, have not yet been scaled effectively on modern hardware accelerators due to the use of specialized attention patterns. Inspired by the Transformer scaling successes in NLP, one of the experiments is applying a standard Transformer directly to the image <span class="citation">(Dosovitskiy et al. <a href="#ref-ImageT" role="doc-biblioref">2020</a>)</span>. Due to the widespread application of computer vision, these problems differ and are constantly being at the center of attention of more and more research.</p>
<p>With the rapid development in NLP and CV in recent years, it was just a question of time to merge both modalities to tackle multi-modal tasks. The release of DALL-E 2 just hints at what one can expect from this merge in the future. DALL-E 2 is able to create photorealistic images or even art from any given text input. So it takes the information of one modality and turns it into another modality. It needs multi-modal datasets to make this possible, which are still relatively rare. This shows the importance of available data and the ability to use it even more. Nevertheless, all modalities are in need of huge datasets to pre-train their models. It’s common to pre-train a model and fine-tune it afterwards for a specific task on another dataset. For example, every state-of-the-art CV model uses a classifier pre-trained on an ImageNet based dataset. The cardinality of the datasets used for CV is immense, but the datasets used for NLP are of a completely different magnitude. BERT uses the English Wikipedia and the Bookscorpus to pre-train the model. The latter consists of almost 1 billion words and 74 million sentences. The pre-training of GPT-3 is composed of five huge corpora: CommonCrawl, Books1 and Books2, Wikipedia and WebText2. Unlike language model pre-training that can leverage tremendous natural language data, vision-language tasks require high-quality image descriptions that are hard to obtain for free. Widely used pre-training datasets for VL-PTM are Microsoft Common Objects in Context (COCO), Visual Genome (VG), Conceptual Captions (CC), Flickr30k, LAION-400M and LAION-5B, which is now the biggest openly accessible image-text dataset.</p>
<p>Besides the importance of pre-training data, there must also be a way to test or compare the different models. A reasonable approach is to compare the performance on specific tasks, which is called benchmarking. A nice feature of benchmarks is that they allow us to compare the models to a human baseline. Different metrics are used to compare the performance of the models. Accuracy is widely used, but there are also some others. For CV the most common benchmark datasets are ImageNet, ImageNetReaL, CIFAR-10(0), OXFORD-IIIT PET, OXFORD Flower 102, COCO and Visual Task Adaptation Benchmark (VTAB). The most common benchmarks for NLP are General Language Understanding Evaluation (GLUE), SuperGLUE, SQuAD 1.1, SQuAD 2.0, SWAG, RACE, ReCoRD, and CoNLL-2003. VTAB, GLUE and SuperGLUE also provide a public leader board. Cross-modal tasks such as Visual Question Answering (VQA), Visual Commonsense Reasoning (VCR), Natural Language Visual Reasoning (NLVR), Flickr30K, COCO and Visual Entailment are common benchmarks for VL-PTM.</p>

<div id="title" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.1</span> title<a href="chapter-1.html#title" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: </em></p>
<p><em>Supervisor: </em></p>

</div>
<div id="title-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.2</span> title<a href="chapter-1.html#title-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: </em></p>
<p><em>Supervisor: </em></p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-Bahdanau2014">
<p>Bahdanau, Dzmitry, Kyunghyun Cho, and Yoshua Bengio. 2014. “Neural Machine Translation by Jointly Learning to Align and Translate,” September. <a href="http://arxiv.org/abs/1409.0473">http://arxiv.org/abs/1409.0473</a>.</p>
</div>
<div id="ref-Bojanowski2016">
<p>Bojanowski, Piotr, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. “Enriching Word Vectors with Subword Information,” July. <a href="http://arxiv.org/abs/1607.04606">http://arxiv.org/abs/1607.04606</a>.</p>
</div>
<div id="ref-brown2020language">
<p>Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.</p>
</div>
<div id="ref-SwAV">
<p>Caron, Mathilde, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. 2020. “Unsupervised Learning of Visual Features by Contrasting Cluster Assignments.” <em>CoRR</em> abs/2006.09882. <a href="https://arxiv.org/abs/2006.09882">https://arxiv.org/abs/2006.09882</a>.</p>
</div>
<div id="ref-SimCLR">
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. 2020. “A Simple Framework for Contrastive Learning of Visual Representations.” <em>CoRR</em> abs/2002.05709. <a href="https://arxiv.org/abs/2002.05709">https://arxiv.org/abs/2002.05709</a>.</p>
</div>
<div id="ref-Devlin2018">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding,” October. <a href="http://arxiv.org/abs/1810.04805">http://arxiv.org/abs/1810.04805</a>.</p>
</div>
<div id="ref-ImageT">
<p>Dosovitskiy, Alexey, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, et al. 2020. “An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale.” <em>CoRR</em> abs/2010.11929. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>.</p>
</div>
<div id="ref-BYOL">
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” <em>CoRR</em> abs/2006.07733. <a href="https://arxiv.org/abs/2006.07733">https://arxiv.org/abs/2006.07733</a>.</p>
</div>
<div id="ref-ResNet">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” <em>CoRR</em> abs/1512.03385. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-Mikolov2013">
<p>Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space,” January. <a href="http://arxiv.org/abs/1301.3781">http://arxiv.org/abs/1301.3781</a>.</p>
</div>
<div id="ref-Raffel2019">
<p>Raffel, Colin, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2019. “Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer,” October. <a href="http://arxiv.org/abs/1910.10683">http://arxiv.org/abs/1910.10683</a>.</p>
</div>
<div id="ref-Sutskever2014">
<p>Sutskever, Ilya, Oriol Vinyals, and Quoc V. Le. 2014. “Sequence to Sequence Learning with Neural Networks,” September. <a href="http://arxiv.org/abs/1409.3215">http://arxiv.org/abs/1409.3215</a>.</p>
</div>
<div id="ref-EfficientNet">
<p>Tan, Mingxing, and Quoc V. Le. 2019. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <em>CoRR</em> abs/1905.11946. <a href="http://arxiv.org/abs/1905.11946">http://arxiv.org/abs/1905.11946</a>.</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
