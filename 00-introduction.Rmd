# Introduction

*Author: Nadja Sauter*  

*Supervisor: Matthias Assenmacher*



## Introduction to Multimodal Deep Learning  

There are five basic human senses: hearing, touch, smell, taste and sight. Processing these five modalities, we are able to perceive and understand the world around us. So multimodal means to combine different channels of information simultaneously to understand our surroundings. For example, when toddlers learn the word “cat”, they use different modalities by saying the word out loud, pointing on cats and making sounds like “meow”. Using the human learning process as role model, AI researcher also try to combine different modalities to train  Deep Learning models. On a superficial level, a Deep learning algorithm is based on a neural network that tries to optimize some objective which is mathematical defined in the so-called loss function. The optimization of minimizing the loss function is done via several mathematical operation also known as gradient descent. Consequently, Deep learning models can only handle numeric input and can only result in a numeric output. However, in multimodal tasks we often have unstructured data like pictures or text. Thus, the first major problem is how to represent the input numerically. The second issue with regard to multimodal tasks is how exactly to synergize different modalities.  For instance, a typical task could be to train a Deep Learning model to generate a picture of a cat. First of all, the computer needs to understand the text input “cat” and then somehow translate this information into an image. Therefore, it is necessary to identify the contextual relationships between words in the text input and the spatial pixel relationships in the image output. What might be easy for a toddler in pre-school, is a huge challenge for the computer. Both have to learn some understanding of the word "cat" that comprises the meaning and appearance of the animal. A common approach in AI is to generate a high-level embedding that represents our cat numerically as vector in some latent space. However, to achieve this different approaches and algorithmic architectures developed in recent years. This book gives an overview of the different methods used in multimodal deep learning to overcome challenges arising with unstructured data and combining different kind of inputs.


## Outline of the Booklet  
Since multimodal modals often use text and images as input, methods of Natural Language Processing (NLP) and Computer Vision (CV) are introduced as foundation in Chapter 1. Methods in the area of NLP try to handle text data, whereas CV deals with image processing. With regard to NLP, one major concept are word embeddings that are used in Encode-Decoder Models. This is the foundation for transformers like BERT which achieved a huge improvement in several NLP tasks. Especially the attention mechanism of transformers revolutionized these models. In Computer Vision different network architectures, namely ResNet, EfficientNet, SimCLR and BYOL, will be introduced. Especially interesting is it to compare the different approaches in CV and their performance on different benchmarks. For this reason, the last subsection of chapter 1 gives an overall overview of different data sets, pre-training tasks and benchmarks for CV as well as NLP.  
The second chapter focuses on different multimodal architectures. These models combine and advance different methods of NLP and CV. First of all, looking at Img2Text tasks, the data set Microsoft COCO for object recognition and the meshed-memory transformer for Image Captioning (M^2^ Transformer) will be presented. Contrariwise, researcher developed methods to generate pictures based on a short text prompt (TBD: …Text2Img… after presentaton Monday).Another interesting question is how images can support language models. This can be done via sequential embeddings, more advanced grounded embeddings or transformers. On the other hand, you can also look at text supporting CV models like CLIP, ALIGN and Florence. They use foundation models as well as methods like contrastive learning or zero-shooting which will be explained in detail in this subsection. Especially the open-source CLIP architecture for image classification and generation attracted a lot of attention last year. In the end of the second chapter, some further architectures to handle text and images are introduced. For instance, Data2Vec uses the same learning method for speech, vison and language and in this way aims to find a general approach to handle different modalities in the same way. Furthermore, VilBert extends the popular BERT architecture to handle both image and text as input by implementing co-attention. This method is also used in Google's Deepmind Flamingo. In addition, Flamingo aims to tackle multiple tasks with a single visual language model via few-shot learning and freezing the pre-trained Vision and Language model.  
In the last chapter, methods are introduced that include other modalities than text and image like video or voice. The overall goal here is to find a general multimodal architecture based on challenges rather than modalities. Therefore, you need to handle problems of multimodal fusion and alignment and decide whether you use a join or coordinated representation.  Moreover we want to go more into detail how exactly to combine structured and unstructured data. Therefore, different fusion strategies evolved in recent years. This is illustrated in this book by two use cases in survival analysis and economics. Besides, another interesting research question is to tackle different tasks in one so called multi-purpose model like it is done by Google’s Pathways (2022). Last but not least, we show one application of Multimodal Deep Learning in the arts scene where image generation models like DALL-E are used to create art pieces in the area of Gerenative Arts. 




