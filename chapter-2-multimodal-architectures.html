<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 11 Chapter 2 Multimodal architectures | Multimodal Deep Learning</title>
  <meta name="description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  <meta name="generator" content="bookdown 0.26 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 11 Chapter 2 Multimodal architectures | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 11 Chapter 2 Multimodal architectures | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="In the last few years, there have been several breakthroughs in the methodologies used in Natural Language Processing (NLP) as well as Computer Vision (CV). Beyond these improvements on single-modality models, large-scale multi-modal approaches have become a very active area of research. In this seminar, we are planning to review these approaches and create a solid overview of the field, starting with the current state-of-the-art approaches in the two subfields of Deep Learning individually. We will further discuss modeling frameworks, where one modality is transformed into the other as well as models in which one modality is utilized to enhance representation learning for the other. Finally, we plan to also potentially cover other modalities as well as general-purpose multi-modal models, which are able to handle different tasks on different modalities within one unified architecture. Interesting applications/use cases could also be potential topics for a seminar paper." />
  

<meta name="author" content="" />


<meta name="date" content="2022-06-07" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="title-7.html"/>
<link rel="next" href="further-topics.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multimodal Deep Learning></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#intro-about-the-seminar-topic"><i class="fa fa-check"></i><b>1.1</b> Intro About the Seminar Topic</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-1.html"><a href="chapter-1.html"><i class="fa fa-check"></i><b>2</b> Chapter 1</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter-1.html"><a href="chapter-1.html#title"><i class="fa fa-check"></i><b>2.1</b> title</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-1.html"><a href="chapter-1.html#title-1"><i class="fa fa-check"></i><b>2.2</b> title</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"><a href="resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks.html"><i class="fa fa-check"></i><b>3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
<li class="chapter" data-level="4" data-path="chapter-1-1.html"><a href="chapter-1-1.html"><i class="fa fa-check"></i><b>4</b> Chapter 1</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter-1-1.html"><a href="chapter-1-1.html#lorem-ipsum"><i class="fa fa-check"></i><b>4.1</b> Lorem Ipsum</a></li>
<li class="chapter" data-level="4.2" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-figures"><i class="fa fa-check"></i><b>4.2</b> Using Figures</a></li>
<li class="chapter" data-level="4.3" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-tex"><i class="fa fa-check"></i><b>4.3</b> Using Tex</a></li>
<li class="chapter" data-level="4.4" data-path="chapter-1-1.html"><a href="chapter-1-1.html#using-stored-results"><i class="fa fa-check"></i><b>4.4</b> Using Stored Results</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="title-2.html"><a href="title-2.html"><i class="fa fa-check"></i><b>5</b> title</a></li>
<li class="chapter" data-level="6" data-path="title-3.html"><a href="title-3.html"><i class="fa fa-check"></i><b>6</b> title</a></li>
<li class="chapter" data-level="7" data-path="title-4.html"><a href="title-4.html"><i class="fa fa-check"></i><b>7</b> title</a></li>
<li class="chapter" data-level="8" data-path="title-5.html"><a href="title-5.html"><i class="fa fa-check"></i><b>8</b> title</a></li>
<li class="chapter" data-level="9" data-path="title-6.html"><a href="title-6.html"><i class="fa fa-check"></i><b>9</b> title</a></li>
<li class="chapter" data-level="10" data-path="title-7.html"><a href="title-7.html"><i class="fa fa-check"></i><b>10</b> title</a></li>
<li class="chapter" data-level="11" data-path="chapter-2-multimodal-architectures.html"><a href="chapter-2-multimodal-architectures.html"><i class="fa fa-check"></i><b>11</b> Chapter 2 Multimodal architectures</a><ul>
<li class="chapter" data-level="11.1" data-path="chapter-2-multimodal-architectures.html"><a href="chapter-2-multimodal-architectures.html#introduction-1"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="further-topics.html"><a href="further-topics.html"><i class="fa fa-check"></i><b>12</b> Further Topics</a></li>
<li class="chapter" data-level="13" data-path="further-modalities.html"><a href="further-modalities.html"><i class="fa fa-check"></i><b>13</b> Further Modalities</a><ul>
<li class="chapter" data-level="13.1" data-path="further-modalities.html"><a href="further-modalities.html#intro"><i class="fa fa-check"></i><b>13.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="strucutered-unstrucutered-data.html"><a href="strucutered-unstrucutered-data.html"><i class="fa fa-check"></i><b>14</b> Strucutered + Unstrucutered Data</a><ul>
<li class="chapter" data-level="14.1" data-path="strucutered-unstrucutered-data.html"><a href="strucutered-unstrucutered-data.html#intro-1"><i class="fa fa-check"></i><b>14.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="multi-purpose-models.html"><a href="multi-purpose-models.html"><i class="fa fa-check"></i><b>15</b> Multi-purpose Models</a><ul>
<li class="chapter" data-level="15.1" data-path="multi-purpose-models.html"><a href="multi-purpose-models.html#intro-2"><i class="fa fa-check"></i><b>15.1</b> Intro</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="title-8.html"><a href="title-8.html"><i class="fa fa-check"></i><b>16</b> title</a></li>
<li class="chapter" data-level="17" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>17</b> Epilogue</a><ul>
<li class="chapter" data-level="17.1" data-path="epilogue.html"><a href="epilogue.html#test"><i class="fa fa-check"></i><b>17.1</b> test</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>18</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-2-multimodal-architectures" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 11</span> Chapter 2 Multimodal architectures<a href="chapter-2-multimodal-architectures.html#chapter-2-multimodal-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Authors: Luyang Chu, Karol Urbanczyk, Giacomo Loss, Max Schneider, Steffen Jauch-Walser</em></p>
<p><em>Supervisor: Christian Heumann</em></p>
<div id="introduction-1" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.1</span> Introduction<a href="chapter-2-multimodal-architectures.html#introduction-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Multimodal learning refers to the process of learning representations from different types of input modalities, such as image data, text or speech.
Due to methodological breakthroughs in the fields of Natural Language Processing (NLP) as well as Computer Vision (CV), in recent years multimodal models have gained increasing attention as they are able to strengthen predictions and better emulate the way humans learn.
This chapter focuses on discussing images and text as input data.
The remainder of the chapter is structured as follows:</p>
<p>The first part “Image2Text” discusses how transformer-based architectures improve meaningful captioning for complex images using a new large scale, richly annotated dataset COCO <span class="citation">(Lin et al. <a href="#ref-mccoco" role="doc-biblioref">2014</a>; Cornia et al. <a href="#ref-cornia2020m2" role="doc-biblioref">2020</a>)</span>.
Whether it is seeing a photograph and describing it or parsing a complex scene and describing its context, it is not a difficult task for humans.
But it is much more complex and challenging for computers.
We start with focusing on images as input modalities.
In 2014 Microsoft COCO was developed with a primary goal of advancing the state-of-the-art (SOTA) in object recognition by diving deeper into a broader question of scene understanding <span class="citation">(Lin et al. <a href="#ref-mccoco" role="doc-biblioref">2014</a>)</span>.
COCO stands for Common Objects in Context.
It addresses three core problems in scene understanding: object detection (non-iconic views), segmentation, and captioning.
For tasks like machine translation and language understanding in NLP, transformer-based architecture is widely used.
However, the potential of these applications in the multi-modal context has not been fully covered.
With the help of the COCO dataset, a transformer-based architecture: Meshed-Memory Transformer for Image Captioning (<span class="math inline">\(M^2\)</span>) will be introduced to improve both image encoding and the language generation steps <span class="citation">(Cornia et al. <a href="#ref-cornia2020m2" role="doc-biblioref">2020</a>)</span>.
The performance of the (<span class="math inline">\(M^2\)</span>) Transformer and different fully-attentive models will be evaluated and compared on the COCO dataset.</p>
<p>Next, in “Text2Image”, the idea of incorporating textual input in order to generate visual representations is described.
Current advancements in this field have been made possible largely due to recent breakthroughs in NLP, which first allowed for learning contextual representations of text.
Transformer-like architectures are being used to encode the input into embedding vectors, which are later helpful in guiding the process of image generation.
The chapter looks into details and discusses two SOTA model architectures by OpenAI, which both condition on text representations.
Surprisingly, none of them uses a GAN approach - a method which probably has been seen as the go-to idea for image generation over the last years.
The first model is DALL-E <span class="citation">(Ramesh et al. <a href="#ref-ramesh2021dalle" role="doc-biblioref">2021</a>)</span>, which essentially combines Variational Encoder (VAE) with Autoregressive Transformer.
In the first step, VAE is being trained to learn downsized image representations.
Such embeddings are concatenated with text embeddings into one text-image pair input.
However, both of them use different dimensionality and vocabulary size.
In the second step, the transformer is trained on a next token prediction task given these data pairs.
Finally, at inference time, the model is able to generate images in the following way:</p>
<ol style="list-style-type: decimal">
<li>Encode text input into text embedding</li>
<li>Use trained transformer from step 2 to generate image embedding</li>
<li>Use VAE from step 1 to generate image from image embedding</li>
</ol>
<p>The next approach to text-to-image generation is a GLIDE model <span class="citation">(Nichol et al. <a href="#ref-nichol2021glide" role="doc-biblioref">2021</a>)</span>.
GLIDE stands for Guided Language to Image Diffusion for Generation and Editing.
Its idea is to use Diffusion Models.
In its core, Diffusion Model is a simple idea – random noise is being added to the image in an iterative fashion, and then model learns how to reconstruct this image.
In the case of GLIDE this learning process is conditioned on the text prompt, which is first passed through a transformer.
Both models differ in their results.
While DALL-E’s resulting images might have been overwhelming back in the beginning of 2021, GLIDE is thought to significantly improve on photorealism and resolution the generated images.
Since the field has already seen further improvements following GLIDE, these new developments are also going to be mentioned in the chapter.</p>
<p>The third part, “Images supporting Language Models”, deals with the integration of visual elements in pure textual language models.
Distributional semantic models such as Word2Vec and BERT assume that the meaning of a given word or sentence can be understood by looking at how (in which context) and when the word or the sentence appear in the text corpus, namely from its “distribution” within the text.
But this assumption has been historically questioned, because words and sentences must be grounded in other perceptual dimensions in order to understand their meaning <span class="citation">(see for example the “symbol grounding problem”; Harnad <a href="#ref-harnad1990symbol" role="doc-biblioref">1990</a>)</span>.
For these reasons, a broad range of models has been developed with the aim to improve pure language models, leveraging on the addition of other perceptual information, such as visual ones.
This subchapter focuses in particular on the integration of visual elements (images) to support pure language models for various tasks at the word-level and sentence-level.
The starting point is always a language model, on which visual representations (extracted often with the help of large pools of images like MS COCO, see chapter “Img2Text” for further references) are to be “integrated”.
But how?
There has been proposed a wide range of solutions:
On one side of the spectrum, textual elements and visual ones are learned separately and then “combined” together whereas on the other side, the learning of textual and visual features takes place simultaneously/jointly.</p>
<br>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="figures/02-chapter2/Img_Ch_Intro.png" alt="Left, Silberer et al., 2014: stacked autoencoders to learn higher-level embeddings from textual and visual modalities, encoded as vectors of attributes. Right, Bordes et al., 2020: textual and visual information fused in an Intermediate space denoted as “grounded space”; the “grounding objective function” is not applied directly on sentence embeddings but trained on this intermediate space, on which sentence embeddings are projected." width="100%" />
<p class="caption">
FIGURE 11.1: Left, Silberer et al., 2014: stacked autoencoders to learn higher-level embeddings from textual and visual modalities, encoded as vectors of attributes. Right, Bordes et al., 2020: textual and visual information fused in an Intermediate space denoted as “grounded space”; the “grounding objective function” is not applied directly on sentence embeddings but trained on this intermediate space, on which sentence embeddings are projected.
</p>
</div>
<p><br></p>
<p>For example, <span class="citation">Silberer and Lapata (<a href="#ref-silberer2012grounded" role="doc-biblioref">2012</a>)</span> implement a model where a one-to-one correspondence between textual and visual space is assumed.
Text and visual representations are passed to two separate unimodal encoders and both outputs are then fed to a bimodal autoencoder.
On the other side, <span class="citation">Bordes et al. (<a href="#ref-bordes2020incorporating" role="doc-biblioref">2020</a>)</span> propose a “text objective function” whose parameters are shared with an additional “grounded objective function”.
The training of the latter takes place in what the authors called a “grounded space”, which allows to avoid the one-to-one correspondence between textual and visual space.
These are just introductory examples and between these two approaches there are many shades of gray (maybe more than fifty…).
These models exhibit in many instances better performance than pure language models, but they still struggle on some aspects, for example when they deal with abstract words and sentences.</p>
<p>Afterwards, in “Text supporting Image Models”, approaches where natural language is used as supervision for CV models are described.
Intuitively these models should be more powerful compared to models supervised solely by manually labeled data, simply because there is much more training data available.
An important example for this is the CLIP model <span class="citation">(Radford et al. <a href="#ref-radford2021learning" role="doc-biblioref">2021</a>)</span> with its new dataset WIT (WebImageText) comprising 400 million text-image pairs scraped from the internet.<br />
Similar to “Text2Image” the recent successes in NLP have inspired new approaches in this field.
Most importantly pre-train methods, which directly learn from raw text <span class="citation">(e. g. GPT-n, Generative Pre-trained Transformer; Brown et al. <a href="#ref-brown2020language" role="doc-biblioref">2020</a>)</span>.
So, CLIP stands for Contrastive Language-Image Pre-training.
A transformer-like architecture is used for jointly pre-training a text encoder and an image encoder.
For this the contrastive goal to correctly predict which natural language text pertains to which image inside a certain batch, is employed.
Training this way turned out to be more efficient than to generate captions for images.<br />
This leads to a flexible model, which at test time uses the learned text encoder as a “zero-shot” classifier on embeddings of the target dataset’s classes.
The model, for example, can perform optical character recognition, geo-location and action-recognition.
Performance-wise CLIP can be competitive with task-specific supervised models, while never seeing an instance of the specific dataset before.
This suggests an important step towards closing the “robustness gap”, where machine learning models fail to meet the expectations set by their previous performance – especially on ImageNet test-sets – on new datasets.</p>
<p>Finally, “Text plus Images” discusses how text and image inputs can be incorporated into a single unifying framework in order to get closer to a general self-supervised learning model.
There are two key advantages that make such a model particularly interesting.
Similar to models mentioned in previous parts, devoid of human labelling, self-supervised models don’t suffer from the same capacity constraints as regular supervised learning models.
Nevertheless, while there have been notable advances in dealing with different modalities, it is often unclear to which extend a model structure generalizes across different modalities.
Rather than potentially learning modality-specific biases, a general multipurpose framework can help increase robustness while also simplifying the learner portfolio and thereby better emulating human learning processes.<br />
Data2vec <span class="citation">(Baevski et al. <a href="#ref-baevski2022data2vec" role="doc-biblioref">2022</a>)</span> is a new multimodal self-supervised learning model which uses a single framework for either speech, NLP or computer vision.
This is in contrast to earlier models which used different algorithms for different modalities.
The core idea of data2vec, developed by MetaAI, is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard transformer architecture <span class="citation">(Baevski et al. <a href="#ref-baevski2022data2vec" role="doc-biblioref">2022</a>)</span>.
As a result, the main improvement is in the framework, not the underlying models themselves.
For example, the transformer architecture follows <span class="citation">Vaswani et al. (<a href="#ref-vaswani2017attention" role="doc-biblioref">2017</a>)</span>.
Transformers have several advantages over CNNs, such as encoding the relative position of features (citation needed).
The central building block of the data2vec framework is a student-teacher structure that allows the learning process to occur without supervision.
To achieve this, inputs serve both as training data and as learning targets by being masked.
A key issue to be aware of is model collapse, i.e the model collapsing into a constant representation.
Normalization helps prevent that, as well as the domination of certain layers with high norm.
The encoding, normalization and masking strategies are modality-specific.
However, the learning objective remains the same across all modalities.
The model is trained to predict the model representation of the original unmasked training sample.
As a result of the use of self-attention in creating teacher representations, the data2vec model works with continuous and contextualized targets which are richer in information than a fixed set of targets based on local context as used in most prior work.
On top of that, working with latent representations of the network itself can be seen as a simplification of many prior modality-specific models <span class="citation">(Baevski et al. <a href="#ref-baevski2022data2vec" role="doc-biblioref">2022</a>)</span>.
As far as the results are concerned, data2vec is effective in all three modalities.
It sets new SOTA scores on computer vision, speech recognition as well as speech learning benchmarking sets.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-baevski2022data2vec">
<p>Baevski, Alexei, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. “Data2vec: A General Framework for Self-Supervised Learning in Speech, Vision and Language.” <em>arXiv Preprint arXiv:2202.03555</em>.</p>
</div>
<div id="ref-bordes2020incorporating">
<p>Bordes, Patrick, Eloi Zablocki, Laure Soulier, Benjamin Piwowarski, and Patrick Gallinari. 2020. “Incorporating Visual Semantics into Sentence Representations Within a Grounded Space.” <em>arXiv Preprint arXiv:2002.02734</em>.</p>
</div>
<div id="ref-brown2020language">
<p>Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” <em>Advances in Neural Information Processing Systems</em> 33: 1877–1901.</p>
</div>
<div id="ref-cornia2020m2">
<p>Cornia, Marcella, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. “Meshed-Memory Transformer for Image Captioning.” In <em>Proceedings of the Ieee/Cvf Conference on Computer Vision and Pattern Recognition</em>.</p>
</div>
<div id="ref-harnad1990symbol">
<p>Harnad, Stevan. 1990. “The Symbol Grounding Problem.” <em>Physica D: Nonlinear Phenomena</em> 42 (1-3): 335–46.</p>
</div>
<div id="ref-mccoco">
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C. Lawrence Zitnick. 2014. “Microsoft Coco: Common Objects in Context.” In <em>Computer Vision – Eccv 2014</em>, edited by David Fleet, Tomas Pajdla, Bernt Schiele, and Tinne Tuytelaars, 740–55. Cham: Springer International Publishing.</p>
</div>
<div id="ref-nichol2021glide">
<p>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. “GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.” <em>CoRR</em> abs/2112.10741. <a href="https://arxiv.org/abs/2112.10741">https://arxiv.org/abs/2112.10741</a>.</p>
</div>
<div id="ref-radford2021learning">
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021. “Learning Transferable Visual Models from Natural Language Supervision.” In <em>International Conference on Machine Learning</em>, 8748–63. PMLR.</p>
</div>
<div id="ref-ramesh2021dalle">
<p>Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. “Zero-Shot Text-to-Image Generation.” In <em>Proceedings of the 38th International Conference on Machine Learning</em>, edited by Marina Meila and Tong Zhang, 139:8821–31. Proceedings of Machine Learning Research. PMLR. <a href="https://proceedings.mlr.press/v139/ramesh21a.html">https://proceedings.mlr.press/v139/ramesh21a.html</a>.</p>
</div>
<div id="ref-silberer2012grounded">
<p>Silberer, Carina, and Mirella Lapata. 2012. “Grounded Models of Semantic Representation.” In <em>Tsujii J, Henderson J, Paşca M, Editors. Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning; 2012 Jul 12–14; Jeju Island, Korea. Stroudsburg: ACL; 2012. P. 1423-33.</em> ACL (Association for Computational Linguistics).</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” <em>Advances in Neural Information Processing Systems</em> 30.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="title-7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-topics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
