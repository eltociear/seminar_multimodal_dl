<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Multimodal architectures | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.28 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Multimodal architectures | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Multimodal architectures | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introducing-the-modalities.html"/>
<link rel="next" href="further-topics.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Multimodal Deep Learning></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="0.1" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i><b>0.1</b> Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a></li>
<li class="chapter" data-level="2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#state-of-the-art-in-computer-vision"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in computer vision</a><ul>
<li class="chapter" data-level="2.1.1" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#history"><i class="fa fa-check"></i><b>2.1.1</b> History</a></li>
<li class="chapter" data-level="2.1.2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#supervised-and-unsupervised-learning"><i class="fa fa-check"></i><b>2.1.2</b> Supervised and unsupervised learning</a></li>
<li class="chapter" data-level="2.1.3" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#resnet"><i class="fa fa-check"></i><b>2.1.3</b> ResNet</a></li>
<li class="chapter" data-level="2.1.4" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#efficientnet"><i class="fa fa-check"></i><b>2.1.4</b> EfficientNet</a></li>
<li class="chapter" data-level="2.1.5" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#simclr"><i class="fa fa-check"></i><b>2.1.5</b> SimCLR</a></li>
<li class="chapter" data-level="2.1.6" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#bootstrap-your-own-latent-byol"><i class="fa fa-check"></i><b>2.1.6</b> Bootstrap Your Own Latent (BYOL)</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#resources-and-benchmarks-for-nlp-cv-and-multimodal-tasks"><i class="fa fa-check"></i><b>2.2</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a><ul>
<li class="chapter" data-level="2.2.1" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#datasets"><i class="fa fa-check"></i><b>2.2.1</b> Datasets</a></li>
<li class="chapter" data-level="2.2.2" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#pre-training-tasks"><i class="fa fa-check"></i><b>2.2.2</b> Pre-Training Tasks</a></li>
<li class="chapter" data-level="2.2.3" data-path="introducing-the-modalities.html"><a href="introducing-the-modalities.html#benchmarks"><i class="fa fa-check"></i><b>2.2.3</b> Benchmarks</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#img2text"><i class="fa fa-check"></i><b>3.1</b> img2text</a><ul>
<li class="chapter" data-level="3.1.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#microsoft-coco-common-objects-in-context"><i class="fa fa-check"></i><b>3.1.1</b> 2.1.1 Microsoft COCO: Common Objects in Context</a></li>
<li class="chapter" data-level="3.1.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#meshed-memory-transformer-for-image-captioning-m2"><i class="fa fa-check"></i><b>3.1.2</b> 2.1.2 Meshed-Memory Transformer for Image Captioning (<span class="math inline">\(M^2\)</span>)</a></li>
<li class="chapter" data-level="3.1.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#seeking-objectivity"><i class="fa fa-check"></i><b>3.1.3</b> Seeking objectivity</a></li>
<li class="chapter" data-level="3.1.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#generative-adversarial-networks"><i class="fa fa-check"></i><b>3.1.4</b> Generative Adversarial Networks</a></li>
<li class="chapter" data-level="3.1.5" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#dall-e-starting-post-gan-era"><i class="fa fa-check"></i><b>3.1.5</b> Dall-E starting post-GAN era</a></li>
<li class="chapter" data-level="3.1.6" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#glide"><i class="fa fa-check"></i><b>3.1.6</b> GLIDE</a></li>
<li class="chapter" data-level="3.1.7" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#dall-e-2"><i class="fa fa-check"></i><b>3.1.7</b> Dall-E 2</a></li>
<li class="chapter" data-level="3.1.8" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#imagen"><i class="fa fa-check"></i><b>3.1.8</b> Imagen</a></li>
<li class="chapter" data-level="3.1.9" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#parti"><i class="fa fa-check"></i><b>3.1.9</b> Parti</a></li>
<li class="chapter" data-level="3.1.10" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#open-source-community"><i class="fa fa-check"></i><b>3.1.10</b> Open-Source Community</a></li>
<li class="chapter" data-level="3.1.11" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#discussion"><i class="fa fa-check"></i><b>3.1.11</b> Discussion</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#images-supporting-language-models"><i class="fa fa-check"></i><b>3.2</b> Images supporting language models</a><ul>
<li class="chapter" data-level="3.2.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#words-in-non-symbolic-contexts"><i class="fa fa-check"></i><b>3.2.1</b> Words In (Non-Symbolic) Contexts</a></li>
<li class="chapter" data-level="3.2.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#word-embeddings-survival-kit"><i class="fa fa-check"></i><b>3.2.2</b> Word-Embeddings: Survival-Kit</a></li>
<li class="chapter" data-level="3.2.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-beginning-sequential-multimodal-embeddings"><i class="fa fa-check"></i><b>3.2.3</b> The Beginning: Sequential Multimodal Embeddings</a></li>
<li class="chapter" data-level="3.2.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-grounded-space"><i class="fa fa-check"></i><b>3.2.4</b> The Grounded Space</a></li>
<li class="chapter" data-level="3.2.5" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-transformers-era"><i class="fa fa-check"></i><b>3.2.5</b> The Transformers Era</a></li>
<li class="chapter" data-level="3.2.6" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#was-it-worth"><i class="fa fa-check"></i><b>3.2.6</b> Was It Worth?</a></li>
<li class="chapter" data-level="3.2.7" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#the-end-of-this-story"><i class="fa fa-check"></i><b>3.2.7</b> The End Of This Story</a></li>
<li class="chapter" data-level="3.2.8" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#appendix-selected-models---summary"><i class="fa fa-check"></i><b>3.2.8</b> Appendix: Selected Models - Summary</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#text-supporting-computer-vision-models"><i class="fa fa-check"></i><b>3.3</b> Text supporting computer vision models</a><ul>
<li class="chapter" data-level="3.3.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#intro"><i class="fa fa-check"></i><b>3.3.1</b> Intro</a></li>
<li class="chapter" data-level="3.3.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#concepts"><i class="fa fa-check"></i><b>3.3.2</b> Concepts</a></li>
<li class="chapter" data-level="3.3.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#contrastive-loss"><i class="fa fa-check"></i><b>3.3.3</b> Contrastive loss</a></li>
<li class="chapter" data-level="3.3.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#clip"><i class="fa fa-check"></i><b>3.3.4</b> CLIP</a></li>
<li class="chapter" data-level="3.3.5" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#align"><i class="fa fa-check"></i><b>3.3.5</b> ALIGN</a></li>
<li class="chapter" data-level="3.3.6" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#florence"><i class="fa fa-check"></i><b>3.3.6</b> Florence</a></li>
<li class="chapter" data-level="3.3.7" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#performance-comparison"><i class="fa fa-check"></i><b>3.3.7</b> Performance comparison</a></li>
<li class="chapter" data-level="3.3.8" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#resources"><i class="fa fa-check"></i><b>3.3.8</b> Resources</a></li>
<li class="chapter" data-level="3.3.9" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#outlook"><i class="fa fa-check"></i><b>3.3.9</b> Outlook</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#text-image"><i class="fa fa-check"></i><b>3.4</b> Text + Image</a><ul>
<li class="chapter" data-level="3.4.1" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#todo"><i class="fa fa-check"></i><b>3.4.1</b> Todo</a></li>
<li class="chapter" data-level="3.4.2" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#challenges-in-ai"><i class="fa fa-check"></i><b>3.4.2</b> challenges in AI</a></li>
<li class="chapter" data-level="3.4.3" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#vilbert"><i class="fa fa-check"></i><b>3.4.3</b> vilbert</a></li>
<li class="chapter" data-level="3.4.4" data-path="multimodal-architectures.html"><a href="multimodal-architectures.html#flamingo-alayrac2022flamingo"><i class="fa fa-check"></i><b>3.4.4</b> flamingo <span class="citation">(<span class="citeproc-not-found" data-reference-id="alayrac2022flamingo"><strong>???</strong></span>)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="further-topics.html"><a href="further-topics.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="further-topics.html"><a href="further-topics.html#including-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a><ul>
<li class="chapter" data-level="4.1.1" data-path="further-topics.html"><a href="further-topics.html#intro-1"><i class="fa fa-check"></i><b>4.1.1</b> Intro</a></li>
<li class="chapter" data-level="4.1.2" data-path="further-topics.html"><a href="further-topics.html#motivation"><i class="fa fa-check"></i><b>4.1.2</b> Motivation</a></li>
<li class="chapter" data-level="4.1.3" data-path="further-topics.html"><a href="further-topics.html#taxonomy-of-multimodal-challenges"><i class="fa fa-check"></i><b>4.1.3</b> Taxonomy of Multimodal Challenges</a></li>
<li class="chapter" data-level="4.1.4" data-path="further-topics.html"><a href="further-topics.html#general-multimodal-architectures"><i class="fa fa-check"></i><b>4.1.4</b> General Multimodal Architectures</a></li>
<li class="chapter" data-level="4.1.5" data-path="further-topics.html"><a href="further-topics.html#multimodal-training-paradigms"><i class="fa fa-check"></i><b>4.1.5</b> Multimodal Training Paradigms</a></li>
<li class="chapter" data-level="4.1.6" data-path="further-topics.html"><a href="further-topics.html#combining-general-architectures-and-training-paradigms"><i class="fa fa-check"></i><b>4.1.6</b> Combining General Architectures and Training Paradigms</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="further-topics.html"><a href="further-topics.html#strucutered-unstrucutered-data"><i class="fa fa-check"></i><b>4.2</b> Strucutered + Unstrucutered Data</a><ul>
<li class="chapter" data-level="4.2.1" data-path="further-topics.html"><a href="further-topics.html#intro-2"><i class="fa fa-check"></i><b>4.2.1</b> Intro</a></li>
<li class="chapter" data-level="4.2.2" data-path="further-topics.html"><a href="further-topics.html#taxonomy-structured-vs.-unstructured-data"><i class="fa fa-check"></i><b>4.2.2</b> Taxonomy: Structured vs. Unstructured Data</a></li>
<li class="chapter" data-level="4.2.3" data-path="further-topics.html"><a href="further-topics.html#fusion-strategies"><i class="fa fa-check"></i><b>4.2.3</b> Fusion Strategies</a></li>
<li class="chapter" data-level="4.2.4" data-path="further-topics.html"><a href="further-topics.html#applications-of-multimodal-dl"><i class="fa fa-check"></i><b>4.2.4</b> Applications of Multimodal DL</a></li>
<li class="chapter" data-level="4.2.5" data-path="further-topics.html"><a href="further-topics.html#multimodal-dl-in-survival"><i class="fa fa-check"></i><b>4.2.5</b> Multimodal DL in Survival</a></li>
<li class="chapter" data-level="4.2.6" data-path="further-topics.html"><a href="further-topics.html#traditional-survival-analysis-cox-proportional-hazard-model"><i class="fa fa-check"></i><b>4.2.6</b> Traditional Survival Analysis (Cox Proportional Hazard Model)</a></li>
<li class="chapter" data-level="4.2.7" data-path="further-topics.html"><a href="further-topics.html#deepconvsurvdeepcorrsurv"><i class="fa fa-check"></i><b>4.2.7</b> DeepConvSurv+DeepCorrSurv</a></li>
<li class="chapter" data-level="4.2.8" data-path="further-topics.html"><a href="further-topics.html#concat-cross-auto-encoders"><i class="fa fa-check"></i><b>4.2.8</b> Concat + Cross Auto Encoders</a></li>
<li class="chapter" data-level="4.2.9" data-path="further-topics.html"><a href="further-topics.html#cheerla-and-gevaert-2019"><i class="fa fa-check"></i><b>4.2.9</b> Cheerla and Gevaert (2019)</a></li>
<li class="chapter" data-level="4.2.10" data-path="further-topics.html"><a href="further-topics.html#multimodal-dl-in-economics"><i class="fa fa-check"></i><b>4.2.10</b> Multimodal DL in Economics</a></li>
<li class="chapter" data-level="4.2.11" data-path="further-topics.html"><a href="further-topics.html#critical-assessment"><i class="fa fa-check"></i><b>4.2.11</b> Critical Assessment</a></li>
<li class="chapter" data-level="4.2.12" data-path="further-topics.html"><a href="further-topics.html#conclusion-and-outlook"><i class="fa fa-check"></i><b>4.2.12</b> Conclusion and Outlook</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="further-topics.html"><a href="further-topics.html#multi-purpose-models"><i class="fa fa-check"></i><b>4.3</b> Multi-purpose Models</a><ul>
<li class="chapter" data-level="4.3.1" data-path="further-topics.html"><a href="further-topics.html#intro-3"><i class="fa fa-check"></i><b>4.3.1</b> Intro</a></li>
<li class="chapter" data-level="4.3.2" data-path="further-topics.html"><a href="further-topics.html#introduction-1"><i class="fa fa-check"></i><b>4.3.2</b> Introduction</a></li>
<li class="chapter" data-level="4.3.3" data-path="further-topics.html"><a href="further-topics.html#todo-1"><i class="fa fa-check"></i><b>4.3.3</b> TODO</a></li>
<li class="chapter" data-level="4.3.4" data-path="further-topics.html"><a href="further-topics.html#previous-work"><i class="fa fa-check"></i><b>4.3.4</b> Previous Work</a></li>
<li class="chapter" data-level="4.3.5" data-path="further-topics.html"><a href="further-topics.html#todo-2"><i class="fa fa-check"></i><b>4.3.5</b> TODO</a></li>
<li class="chapter" data-level="4.3.6" data-path="further-topics.html"><a href="further-topics.html#pathway-proposal"><i class="fa fa-check"></i><b>4.3.6</b> Pathway Proposal</a></li>
<li class="chapter" data-level="4.3.7" data-path="further-topics.html"><a href="further-topics.html#todo-3"><i class="fa fa-check"></i><b>4.3.7</b> TODO</a></li>
<li class="chapter" data-level="4.3.8" data-path="further-topics.html"><a href="further-topics.html#discussion-1"><i class="fa fa-check"></i><b>4.3.8</b> Discussion</a></li>
<li class="chapter" data-level="4.3.9" data-path="further-topics.html"><a href="further-topics.html#todo-4"><i class="fa fa-check"></i><b>4.3.9</b> TODO</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="title.html"><a href="title.html"><i class="fa fa-check"></i><b>5</b> title</a></li>
<li class="chapter" data-level="6" data-path="conclusion.html"><a href="conclusion.html"><i class="fa fa-check"></i><b>6</b> Conclusion</a></li>
<li class="chapter" data-level="7" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>7</b> Epilogue</a></li>
<li class="chapter" data-level="8" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>8</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="multimodal-architectures" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 3</span> Multimodal architectures<a href="multimodal-architectures.html#multimodal-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Authors: Luyang Chu, Karol Urbanczyk, Giacomo Loss, Max Schneider, Steffen Jauch-Walser</em></p>
<p><em>Supervisor: Christian Heumann</em></p>
<p>Multimodal learning refers to the process of learning representations from different types of input modalities, such as image data, text or speech.
Due to methodological breakthroughs in the fields of Natural Language Processing (NLP) as well as Computer Vision (CV), in recent years multimodal models have gained increasing attention as they are able to strengthen predictions and better emulate the way humans learn.
This chapter focuses on discussing images and text as input data.
The remainder of the chapter is structured as follows:</p>
<p>The first part “Image2Text” discusses how transformer-based architectures improve meaningful captioning for complex images using a new large scale, richly annotated dataset COCO <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>; <span class="citeproc-not-found" data-reference-id="cornia2020m2"><strong>???</strong></span>)</span>.
Whether it is seeing a photograph and describing it or parsing a complex scene and describing its context, it is not a difficult task for humans.
But it is much more complex and challenging for computers.
We start with focusing on images as input modalities.
In 2014 Microsoft COCO was developed with a primary goal of advancing the state-of-the-art (SOTA) in object recognition by diving deeper into a broader question of scene understanding <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>.
COCO stands for Common Objects in Context.
It addresses three core problems in scene understanding: object detection (non-iconic views), segmentation, and captioning.
For tasks like machine translation and language understanding in NLP, transformer-based architecture is widely used.
However, the potential of these applications in the multi-modal context has not been fully covered.
With the help of the COCO dataset, a transformer-based architecture: Meshed-Memory Transformer for Image Captioning (<span class="math inline">\(M^2\)</span>) will be introduced to improve both image encoding and the language generation steps <span class="citation">(<span class="citeproc-not-found" data-reference-id="cornia2020m2"><strong>???</strong></span>)</span>.
The performance of the (<span class="math inline">\(M^2\)</span>) Transformer and different fully-attentive models will be evaluated and compared on the COCO dataset.</p>
<p>Next, in “Text2Image”, the idea of incorporating textual input in order to generate visual representations is described.
Current advancements in this field have been made possible largely due to recent breakthroughs in NLP, which first allowed for learning contextual representations of text.
Transformer-like architectures are being used to encode the input into embedding vectors, which are later helpful in guiding the process of image generation.
The chapter looks into details and discusses two SOTA model architectures by OpenAI, which both condition on text representations.
Surprisingly, none of them uses a GAN approach - a method which probably has been seen as the go-to idea for image generation over the last years.
The first model is DALL-E <span class="citation">(<span class="citeproc-not-found" data-reference-id="ramesh2021dalle"><strong>???</strong></span>)</span>, which essentially combines Variational Encoder (VAE) with Autoregressive Transformer.
In the first step, VAE is being trained to learn downsized image representations.
Such embeddings are concatenated with text embeddings into one text-image pair input.
However, both of them use different dimensionality and vocabulary size.
In the second step, the transformer is trained on a next token prediction task given these data pairs.
Finally, at inference time, the model is able to generate images in the following way:</p>
<ol style="list-style-type: decimal">
<li>Encode text input into text embedding</li>
<li>Use trained transformer from step 2 to generate image embedding</li>
<li>Use VAE from step 1 to generate image from image embedding</li>
</ol>
<p>The next approach to text-to-image generation is a GLIDE model <span class="citation">(<span class="citeproc-not-found" data-reference-id="nichol2021glide"><strong>???</strong></span>)</span>.
GLIDE stands for Guided Language to Image Diffusion for Generation and Editing.
Its idea is to use Diffusion Models.
In its core, Diffusion Model is a simple idea – random noise is being added to the image in an iterative fashion, and then model learns how to reconstruct this image.
In the case of GLIDE this learning process is conditioned on the text prompt, which is first passed through a transformer.
Both models differ in their results.
While DALL-E’s resulting images might have been overwhelming back in the beginning of 2021, GLIDE is thought to significantly improve on photorealism and resolution the generated images.
Since the field has already seen further improvements following GLIDE, these new developments are also going to be mentioned in the chapter.</p>
<p>The third part, “Images supporting Language Models”, deals with the integration of visual elements in pure textual language models.
Distributional semantic models such as Word2Vec and BERT assume that the meaning of a given word or sentence can be understood by looking at how (in which context) and when the word or the sentence appear in the text corpus, namely from its “distribution” within the text.
But this assumption has been historically questioned, because words and sentences must be grounded in other perceptual dimensions in order to understand their meaning <span class="citation">(see for example the “symbol grounding problem”; <span class="citeproc-not-found" data-reference-id="harnad1990symbol"><strong>???</strong></span>)</span>.
For these reasons, a broad range of models has been developed with the aim to improve pure language models, leveraging on the addition of other perceptual information, such as visual ones.
This subchapter focuses in particular on the integration of visual elements (images) to support pure language models for various tasks at the word-level and sentence-level.
The starting point is always a language model, on which visual representations (extracted often with the help of large pools of images like MS COCO, see chapter “Img2Text” for further references) are to be “integrated”.
But how?
There has been proposed a wide range of solutions:
On one side of the spectrum, textual elements and visual ones are learned separately and then “combined” together whereas on the other side, the learning of textual and visual features takes place simultaneously/jointly.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="figures/02-chapter2/Img_Ch_Intro.png" alt="Left, Silberer et al., 2014: stacked autoencoders to learn higher-level embeddings from textual and visual modalities, encoded as vectors of attributes. Right, Bordes et al., 2020: textual and visual information fused in an Intermediate space denoted as “grounded space”; the “grounding objective function” is not applied directly on sentence embeddings but trained on this intermediate space, on which sentence embeddings are projected." width="100%" />
<p class="caption">
FIGURE 3.1: Left, Silberer et al., 2014: stacked autoencoders to learn higher-level embeddings from textual and visual modalities, encoded as vectors of attributes. Right, Bordes et al., 2020: textual and visual information fused in an Intermediate space denoted as “grounded space”; the “grounding objective function” is not applied directly on sentence embeddings but trained on this intermediate space, on which sentence embeddings are projected.
</p>
</div>
<p>For example, <span class="citation">(<span class="citeproc-not-found" data-reference-id="silberer2012grounded"><strong>???</strong></span>)</span> implement a model where a one-to-one correspondence between textual and visual space is assumed.
Text and visual representations are passed to two separate unimodal encoders and both outputs are then fed to a bimodal autoencoder.
On the other side, <span class="citation">(<span class="citeproc-not-found" data-reference-id="bordes2020incorporating"><strong>???</strong></span>)</span> propose a “text objective function” whose parameters are shared with an additional “grounded objective function”.
The training of the latter takes place in what the authors called a “grounded space”, which allows to avoid the one-to-one correspondence between textual and visual space.
These are just introductory examples and between these two approaches there are many shades of gray (maybe more than fifty…).
These models exhibit in many instances better performance than pure language models, but they still struggle on some aspects, for example when they deal with abstract words and sentences.</p>
<p>Afterwards, in “Text supporting Image Models”, approaches where natural language is used as supervision for CV models are described.
Intuitively these models should be more powerful compared to models supervised solely by manually labeled data, simply because there is much more training data available.
An important example for this is the CLIP model <span class="citation">(<span class="citeproc-not-found" data-reference-id="radford2021learning"><strong>???</strong></span>)</span> with its new dataset WIT (WebImageText) comprising 400 million text-image pairs scraped from the internet.<br />
Similar to “Text2Image” the recent successes in NLP have inspired new approaches in this field.
Most importantly pre-train methods, which directly learn from raw text <span class="citation">(e. g. GPT-n, Generative Pre-trained Transformer; <span class="citeproc-not-found" data-reference-id="brown2020language"><strong>???</strong></span>)</span>.
So, CLIP stands for Contrastive Language-Image Pre-training.
A transformer-like architecture is used for jointly pre-training a text encoder and an image encoder.
For this the contrastive goal to correctly predict which natural language text pertains to which image inside a certain batch, is employed.
Training this way turned out to be more efficient than to generate captions for images.<br />
This leads to a flexible model, which at test time uses the learned text encoder as a “zero-shot” classifier on embeddings of the target dataset’s classes.
The model, for example, can perform optical character recognition, geo-location and action-recognition.
Performance-wise CLIP can be competitive with task-specific supervised models, while never seeing an instance of the specific dataset before.
This suggests an important step towards closing the “robustness gap”, where machine learning models fail to meet the expectations set by their previous performance – especially on ImageNet test-sets – on new datasets.</p>
<p>Finally, “Text plus Images” discusses how text and image inputs can be incorporated into a single unifying framework in order to get closer to a general self-supervised learning model.
There are two key advantages that make such a model particularly interesting.
Similar to models mentioned in previous parts, devoid of human labelling, self-supervised models don’t suffer from the same capacity constraints as regular supervised learning models.
Nevertheless, while there have been notable advances in dealing with different modalities, it is often unclear to which extend a model structure generalizes across different modalities.
Rather than potentially learning modality-specific biases, a general multipurpose framework can help increase robustness while also simplifying the learner portfolio and thereby better emulating human learning processes.<br />
Data2vec <span class="citation">(<span class="citeproc-not-found" data-reference-id="baevski2022data2vec"><strong>???</strong></span>)</span> is a new multimodal self-supervised learning model which uses a single framework for either speech, NLP or computer vision.
This is in contrast to earlier models which used different algorithms for different modalities.
The core idea of data2vec, developed by MetaAI, is to predict latent representations of the full input data based on a masked view of the input in a self-distillation setup using a standard transformer architecture <span class="citation">(<span class="citeproc-not-found" data-reference-id="baevski2022data2vec"><strong>???</strong></span>)</span>.
As a result, the main improvement is in the framework, not the underlying models themselves.
For example, the transformer architecture follows <span class="citation">(<span class="citeproc-not-found" data-reference-id="vaswani2017attention"><strong>???</strong></span>)</span>.
Transformers have several advantages over CNNs, such as encoding the relative position of features (citation needed).
The central building block of the data2vec framework is a student-teacher structure that allows the learning process to occur without supervision.
To achieve this, inputs serve both as training data and as learning targets by being masked.
A key issue to be aware of is model collapse, i.e the model collapsing into a constant representation.
Normalization helps prevent that, as well as the domination of certain layers with high norm.
The encoding, normalization and masking strategies are modality-specific.
However, the learning objective remains the same across all modalities.
The model is trained to predict the model representation of the original unmasked training sample.
As a result of the use of self-attention in creating teacher representations, the data2vec model works with continuous and contextualized targets which are richer in information than a fixed set of targets based on local context as used in most prior work.
On top of that, working with latent representations of the network itself can be seen as a simplification of many prior modality-specific models <span class="citation">(<span class="citeproc-not-found" data-reference-id="baevski2022data2vec"><strong>???</strong></span>)</span>.
As far as the results are concerned, data2vec is effective in all three modalities.
It sets new SOTA scores on computer vision, speech recognition as well as speech learning benchmarking sets.</p>

<div id="img2text" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.1</span> img2text<a href="multimodal-architectures.html#img2text" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>*Author: Luyang Chu</p>
<p>*Supervisor: Christian Heumann</p>
<div id="microsoft-coco-common-objects-in-context" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.1</span> 2.1.1 Microsoft COCO: Common Objects in Context<a href="multimodal-architectures.html#microsoft-coco-common-objects-in-context" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><br>
Understanding of visual scenes plays an important role in computer vision research (CV)
Many tasks are included in it, such as image classification, object detection, object localization and semantic scene labeling.
Through the computer vision research history, Image Datasets have played a critical role. They are not only essential for training and evaluating new algorithms, but also lead the research to new challenging directions.<span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span> In the early year, researchers developed Datasets[345] which enabled the direct comparison of hundreds of image recognition algorithms, that was the early evolution in object recognition. Recent years, ImageNet dataset [1] which contains millions of images has enabled breakthroughs in both object classification and detection research using new deep learning algorithms.
With the goal of advancing the state-of-art in object recognition especially scene understanding, a new large scale data called Microsoft COCO was published in 2014. MS COCO focuses on three core problems in scene understanding: detecting non-iconic views, detecting the semantic relationships between objects and precise localization of image objects.<span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>
MS COCO Dataset contains 91 common object categories with a total of 328,000 images as well as 2,500,000 labeled instances. All these images could be recognized by a 4 year old child.82 categories include more than 5000 labeled The labeled instances which may support the detection of relationships between objects is much larger per image in COCO (7.7) than in ImageNet(3.0)<span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>. In order to provide precise localization of object instances, only “Thing” categories like car, table, dog will be included. objects which do not have clear boundaries like sky, sea, grass, will not be included. In current object recognition research, algorithms perform well on images with iconic views. These images always contains the single object category in the center of the image. To accomplish the goal of detecting the contextual relationships between objects, more complex images with multiple objects or natural images which comes from our daily life are gathered for the Dataset.</p>
<p>2.Image collection and annotation 2.1 categories2.2 non-iconic2.3 annotation
COCO is a large-scale richly annotated Datatset, the progress of building consists of two phases:Data collection and image annotation.</p>
<p>In order to select representative object categories for Images in COCO, researchers collected several categories from different dataset like PASCAL VOC and other sources. All these object categories can be recognized by children between 4 to 8. The quality of the object categories were ensured by co-authors.Co-authors scale the categories from 1 to 5 depending on their common occurrence, practical applications and diversity from other categories <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>.The final number pf the list is 91, which includes all the categories from PASCAL VOC</p>
<p>With the help of representative object categories, COCO want to collect a dataset which a majority of these images are non-iconic. Images are roughly divided into three types:iconic-object images, iconic-scene images and non-iconic images<span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span> (Images needs to be added)
Images are collected through two strategies, firstly images from Flickr which contains photos uploaded by amateur photographer with keywords are collected. Secondly, Searching for pairwise combination of object categories like “dog + car” are used by researchers to gather more non-iconic images and images with rich contextual relationships.</p>
Due to the the scale of the dataset and the high cost of the annotation process, the design of a high quality annotation pipeline with efficient cost is a difficult task.
The annotation pipeline for COCO is splitted into three primary tasks: 1. category labeling, 2.instance spotting, and 3. instance segmenting.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="figures/02-chapter2/2.1%20annotation%20pipeline.png" alt="Left, [@mccoco]" width="100%" />
<p class="caption">
FIGURE 3.2: Left, <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>
</p>
</div>
<p>As we can see in the Image(), object categories in each image will be determined in the first step. Due to the large number of Datasets and categories, they used a hierarchical approach instead of doing binary classification for each category. All the 91 categories have divided into 11 super-categories.The worker will examine the existence of a single instance for a given super-category. This hierarchical approach has helped to reduce the time for labeling. However, the first phase still took ∼20k worker hours to complete.
In the next step, all instances of the object categories in an image were labeled, at most 10 instances of a given category per image will be labeled by each worker. Each image was labeled by 8 workers for a total of ∼10k worker hours.
In the final segmenting stage, each object instance is segmented, the segmentations for other instances and the specification of the object instance by a worker in the previous stage will also shown to the worker.
( all workers are required to complete a training task for each object category. The training task required workers to segment an object instance. )
To ensure good quality an explicit verification step on each segmented instance was performed.
(high cost of time and money …….)</p>
<p>3.datasets —&gt; further development, the pro cons
In recent years, researchers have developed several pre-trained datasets and benchmarks which helped the developemnt of Algorithms for CV.(from …. simple ones?)
Each of these datasets varies significantly in size, list of labeled categories and types of images.
ImageNet containing millions of images has enabled breakthroughs in both object classification and detection research using a new class of deep learning algorithms.ImageNet was created to capture a large number of object categories, many of which are fine-grained. SUN focuses on labeling scene types and the objects that commonly occur in them. Finally, PASCAL VOC’s primary application is object detection in natural images. MS COCO is designed for the detection and segmentation of objects occurring in their natural context. <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>
3.1.comparison with other datasets like ImageNet Pascal and SUN using the Fig from <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-3"></span>
<img src="figures/02-chapter2/2.1%20coco%20cmparison.png" alt="Left, [@mccoco]" width="100%" />
<p class="caption">
FIGURE 3.3: Left, <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>
</p>
</div>
<p>3.2.conclusion further development and pros cons…
new large scale data set for detecting and segmenting objects found in everyday life
vast cost and over 70,000 worker hours
advancement of object detection and segmentation algorithms
focus non-iconic images of objects in natural environments
rich contextual information with many objects present per image.
a good benchmark for other types of labels, including scene types, attributes and full sentence written descriptions
using coco for the Meshed-Memory Transformer in 2.1.2</p>
<p>Questions &amp; pros cons
only label “things”, but labeling “stuff” may also provide significant contextual information
typical vision datasets are labor intensive and costly to create
teaching only a narrow set of visual concepts;
standard vision models are good at one task and one task only, and require significant effort to adapt to a new task;
models that perform well on benchmarks have disappointingly poor performance on stress tests</p>
</div>
<div id="meshed-memory-transformer-for-image-captioning-m2" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.2</span> 2.1.2 Meshed-Memory Transformer for Image Captioning (<span class="math inline">\(M^2\)</span>)<a href="multimodal-architectures.html#meshed-memory-transformer-for-image-captioning-m2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li>what is <span class="math inline">\(m^2\)</span> intro, the goal of it.
Transformer-based architectures not only for language understanding.
Exploring their applicability to multi-modal contexts like image captioning<span class="citation">(<span class="citeproc-not-found" data-reference-id="cornia2020m2"><strong>???</strong></span>)</span></li>
</ol>
<p>Image captioning: describe visual content of an image in human language.
Understand and model the relationships between visual and textual elements
Generate a sequence of output words.
<span class="math inline">\(m^2\)</span>
A Meshed Transformer with Memory for Image Captioning
Improves both the image encoding and the language generation steps
Encoder: a multi-level representation of the relationships between image regions with a priori knowledge
Decoder: a mesh-like connectivity betwwen encoder and decoder to exploit low- and high-level features
Compare performance of the Transformer and different fully-attentive models with recurrent ones
2. <span class="math inline">\(m^2\)</span> Transformer architecture <span class="citation">(<span class="citeproc-not-found" data-reference-id="cornia2020m2"><strong>???</strong></span>)</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="figures/02-chapter2/2.1%20m2.png" alt="Left, [@mccoco]" width="100%" />
<p class="caption">
FIGURE 3.4: Left, <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>
</p>
</div>
<p>inspiration from the Transformer model[5]for machine translation with two new concerns
a. Image regions and their relationships encoded through multilevel encoder, take low and high level relations into account
use using persistent memory vectors to learn and encode a priori knowledge
b. exploits both low- and high-level visual relationships through the multi-layer decoder using the weights from a learnable gating mechanism fat each level
A mesh connectivity schema between encoder and decoder layers
2.1 Transformer ( should i provide short revisit for thr Transformer architecture? THE BASIC?)
All interactions between word and image-level features are modeled by using scaled dot-product attention
Attention operates on three sets of vectors, namely a set of queries Q, keys K and values V , and takes a weighted sum of value vectors according to a similarity distribution between query and key vectors.
where Q is a matrix of nq query vectors, K and V both contain nk keys and values, all with the same dimensionality, and d is a scaling factor.
<span class="math display">\[ Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d}}) V, \]</span>
2.2 Encoder with stacks of attentive layers.
Process image regions and their relationships between regions
Image region X
Attention used to get permutation invariant encoding of X through the self-attention operations
<span class="math display">\[ S(X) = Attention(W_q X, W_k X, W_vX) \]</span>
$ W_q, W_k, W_v$ are learnable weights (depend solely on the pairwise similarities between linear projections of the input set X)
Output : a weighted sum of the values X
Significant limitation of self-attention: cannot model prior knowledge on relationships between image regions.
To overcome the limitation, introduce Memory-Augmented Attention by extending the keys and values with additional prior information which does not depend on image region X.
Initialize additional keys and values as plain learnable vectors which can be directly updated via SGD.
<span class="math display">\[M_{mem}(X) = Attention(W_qX, K, V )\]</span>
<span class="math display">\[K = [W_kX,M_k]\]</span>
<span class="math display">\[V =[W_vX,M_v]\]</span>
<span class="math inline">\(M_k\)</span> and <span class="math inline">\(M_v\)</span> are learnable matrices
Encoding layer: embed memory-augmented operator into a Transformer-like layer, output applied to position-wise feed-forward layer
<span class="math display">\[ F(X)_i= U\sigma(V X_i +b)+c;\]</span>
<span class="math inline">\(X_i\)</span> indicates the i-th vector of the input set, and <span class="math inline">\(F(X)_i\)</span> the i-th vector of the output. Also, <span class="math inline">\(\sigma(·)\)</span> is the ReLU activation function, V and U are learnable weight matrices, b and c are bias terms.</p>
<p>Enclose the output within a residual connection and a layer norm operation.</p>
<p><span class="math display">\[ Z = AddNorm(M_{mem}(X))\]</span>
<span class="math display">\[ \tilde{X}=AddNorm(F(Z))\]</span></p>
<p>Full encoder: multiple encoding layers in sequence, the i-th layer uses the output set computed by layer i − 1.
higher encoding Layers can exploit and refine relationships already identified by previous layers,
N encoding layers <span class="math inline">\(\rightarrow\)</span> multi level output <span class="math inline">\(\tilde{X} = (\tilde{X}^1 \dots \tilde{X}^n)\)</span></p>
<p>2.3 decoder with stacks of attentive layers
Conditioned on both previously generated words and region encodings
Input: Vector <span class="math inline">\(Y\)</span> and output from all encoding layers <span class="math inline">\(\tilde{X}\)</span> , connected through gated cross-attentions
Meshed Cross-Attention.
Perform a cross-attention with all encoding layers</p>
<p><span class="math inline">\(C(·, ·)\)</span> stands for the encoder-decoder cross-attention
<span class="math display">\[M_{mesh}(\tilde{X}, Y) =\sum_{i = 1}^{N}\alpha_i C(\tilde{X^i}, Y) \]</span>
C(·, ·) stands for the encoder-decoder cross-attention
<span class="math display">\[ C(\tilde{X^i}, Y) = Attention(W_q Y, W_k \tilde{X^i}, W_v \tilde{X^i}) \]</span>
<span class="math inline">\(\alpha_i\)</span> is a matrix of weights same size as the cross-attention results
models single contribution of each encoding layer, and the relative importance between different layers.
<span class="math display">\[\alpha_i = \sigma(W_i [Y,C(\tilde{X^i}, Y)]+b_i)\]</span>
<span class="math inline">\(\sigma\)</span> sigmoid activation function
Prediction of a word should only depend on previously predicted words
Decoder layer comprises a masked self- attention operation
Connection between queries derived from the t-th element of its input sequence Y with keys and values
Contains a position-wise feed-forward layer as well
<span class="math display">\[Z = AddNorm(M_{mesh}(X,AddNorm(S_{mask}(Y )))\]</span>
<span class="math display">\[\tilde{Y} = AddNorm(F(Z)),\]</span>
<span class="math inline">\(S_{mask}\)</span>: a masked self-attention over time
Input word vectors, and the t-th element of its output sequence make the prediction of a word at time t + 1, conditioned on <span class="math inline">\(Y≤t\)</span>. After taking a linear projection and a softmax operation, this encodes a probability over words in the dictionary.</p>
<p>2.4. Comparison
(not sure)detailed ? test on coco
or just simple explained</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-5"></span>
<img src="figures/02-chapter2/2.1%20m2%20example.png" alt="Left, [@mccoco]" width="100%" />
<p class="caption">
FIGURE 3.5: Left, <span class="citation">(<span class="citeproc-not-found" data-reference-id="mccoco"><strong>???</strong></span>)</span>
</p>
</div>
<p>3.conclusion and bridge to next subsection</p>
<p>connections with other subtopics
multimodal tasks</p>
<p>–References( not finished)—————————
1.J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet: A Large-Scale Hierarchical Image Database,” in CVPR, 2009.
2.M. Everingham, L. Van Gool, C. K. I. Williams, J. Winn, and A. Zis- serman, “The PASCAL visual object classes (VOC) challenge,” IJCV, vol. 88, no. 2, pp. 303–338, Jun. 2010
3.L. Fei-Fei, R. Fergus, and P. Perona, “Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories,” in CVPR Workshop of Generative Model Based Vision (WGMBV), 2004
4.G. Griffin, A. Holub, and P. Perona, “Caltech-256 object category dataset,” California Institute of Technology, Tech. Rep. 7694, 2007
5.N. Dalal and B. Triggs, “Histograms of oriented gradients for
human detection,” in CVPR, 20
6. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko- reit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, 2017.
7. StevenJRennie,EtienneMarcheret,YoussefMroueh,Jarret Ross, and Vaibhava Goel. Self-critical sequence training for image captioning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017
8..Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE Con- ference on Computer Vision and Pattern Recognition, 2018.</p>

<p><em>Author:</em> Karol Urbańczyk
## Text-2-image</p>
<p><em>Supervisor:</em> Jann Goschenhofer</p>
<ul>
<li>introduce the concept in few sentences</li>
<li>choice of recent break-throughs is subjective, many important ones not mentioned (GAWWN, LAFITE, Make-a-Scene, probably many others)</li>
</ul>
<p>Intention of this chapter is to grasp how the field of text-2-image modelling has been changing over the recent years. We will start with basic concepts that has been around since 2014 and end with the state-of-the-art approaches, as of August 2022. Since the field is developing in a rapid pace, with break-through models being announced every quarter, we are aware this chapter might soon not be fully covering the field. However, we must notice that cutting edge capabilities of these models tend to come from the scale and software engineering tricks. Therefore, we believe that focusing on the core concepts should make this chapter have a universal character.</p>
</div>
<div id="seeking-objectivity" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.3</span> Seeking objectivity<a href="multimodal-architectures.html#seeking-objectivity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Objectivity in comparing generated images is very hard to grasp</li>
<li>However, there are some most common datasets and measures that are being used</li>
<li>This subchapter will quickly present them</li>
</ul>
<div id="datasets-1" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.3.1</span> Datasets<a href="multimodal-architectures.html#datasets-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>COCO</li>
<li>CUB</li>
<li>Oxford 102</li>
</ul>
</div>
<div id="measures" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.3.2</span> Measures<a href="multimodal-architectures.html#measures" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>FID (Frechet Inception Distance)</li>
<li>IS (Inception Score)</li>
<li>Human evaluations - photorealism / caption similarity</li>
</ul>
</div>
</div>
<div id="generative-adversarial-networks" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.4</span> Generative Adversarial Networks<a href="multimodal-architectures.html#generative-adversarial-networks" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>quick intro focusing on why it is crucial to start from GANs</li>
</ul>
<div id="vanilla-gan-for-image-generation" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.4.1</span> Vanilla GAN for Image Generation<a href="multimodal-architectures.html#vanilla-gan-for-image-generation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>intro of GAN</li>
</ul>
</div>
<div id="conditioning-on-text" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.4.2</span> Conditioning on Text<a href="multimodal-architectures.html#conditioning-on-text" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>how to encode the text and use it in the generation process</li>
<li>show some results</li>
</ul>
</div>
<div id="stacking-generators" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.4.3</span> Stacking generators<a href="multimodal-architectures.html#stacking-generators" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>intro of StackGAN, show some results</li>
</ul>
</div>
<div id="is-attention-all-you-need" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.4.4</span> Is attention all you need?<a href="multimodal-architectures.html#is-attention-all-you-need" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>intro of AttGAN, show some results</li>
</ul>
</div>
<div id="variational-autoencoder" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.1.4.5</span> Variational Autoencoder<a href="multimodal-architectures.html#variational-autoencoder" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Introducing the concept of VAE</li>
<li>How is it helpful in generating images</li>
</ul>
</div>
</div>
<div id="dall-e-starting-post-gan-era" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.5</span> Dall-E starting post-GAN era<a href="multimodal-architectures.html#dall-e-starting-post-gan-era" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Intro: OpenAI, dataset used, not public, etc</li>
<li>VQ-VAE and dVAE</li>
<li>Details how it’s working. Combining Transformer with VQ-VAE. Training vs inference</li>
<li>Results and image examples</li>
</ul>
</div>
<div id="glide" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.6</span> GLIDE<a href="multimodal-architectures.html#glide" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Intro</li>
<li>Diffusion concept</li>
<li>details how GLIDE is working</li>
<li>results / scores</li>
<li>Limitations / strengths &amp; weaknesses</li>
</ul>
</div>
<div id="dall-e-2" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.7</span> Dall-E 2<a href="multimodal-architectures.html#dall-e-2" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Intro (mention PR move)</li>
<li>details how it is working</li>
<li>results / scores</li>
<li>Limitations / strengths &amp; weaknesses</li>
</ul>
</div>
<div id="imagen" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.8</span> Imagen<a href="multimodal-architectures.html#imagen" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Intro</li>
<li>details how it is working</li>
<li>results / scores</li>
<li>Limitations / strengths &amp; weaknesses</li>
</ul>
</div>
<div id="parti" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.9</span> Parti<a href="multimodal-architectures.html#parti" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Intro</li>
<li>details how it is working</li>
<li>results / scores</li>
<li>Limitations / strengths &amp; weaknesses</li>
</ul>
</div>
<div id="open-source-community" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.10</span> Open-Source Community<a href="multimodal-architectures.html#open-source-community" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>Although most of the recent work comes from OpenAI and Google, there are very interesting directions taken by the open community</li>
<li>Mentioning the models and quickly what is happening. VQGAN+CLIP, Latent Diffusion models for sure</li>
<li>Maybe some links for the reader to play with?</li>
</ul>
</div>
<div id="discussion" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.1.11</span> Discussion<a href="multimodal-architectures.html#discussion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Mention the following points and why they matter</p>
<ul>
<li>potential business use cases</li>
<li>open vs closed-source (mention dall-e mini)</li>
<li>copyrights</li>
<li>biases</li>
</ul>

</div>
</div>
<div id="images-supporting-language-models" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.2</span> Images supporting language models<a href="multimodal-architectures.html#images-supporting-language-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>*Author: Giacomo Loss</p>
<p>*Supervisor: Matthias Assenmacher</p>
<div id="words-in-non-symbolic-contexts" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.1</span> Words In (Non-Symbolic) Contexts<a href="multimodal-architectures.html#words-in-non-symbolic-contexts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Imagine you were alone in a foreign country, you could not speak the language and the only resource you had were a dictionary in the foreign language. You see a word written on a sign but you cannot understand its meaning. What could you do? One idea would be do open the dictionary and look the word up. The problem is that the word is defined by using other words in the foreign language. As a second step you would thus look these new words up and continue like that in further steps to the “infinity and beyond” (cit. Buzz Lightyear). But even after looking every single word in the dictionary up, you would still not be able to understand the meaning of the word written on the sign. If on that sign, next to the unknown word, something else was instead depicted, for example an image of a fork and a knife, you might speculate that the word indicates something which has to do with food, like a restaurant. And this without explicitly knowing the meaning of the word. This example is inspired by the work of Stevan Harnad, which formulated at the beginning of the 90’s the so called <em>Symbol Grounding Problem</em> (<span class="citation">(<span class="citeproc-not-found" data-reference-id="harnad1990symbol"><strong>???</strong></span>)</span>). It asserts that it is not possible to understand the meaning (semantics) of a word by just looking at other words because words are essentially meaningless symbols. It is possible to understand the meaning only if the word is put in a context, a perceptual space, other than that of written language: the word must be <em>grounded</em> in non-symbolic representations, like images, for example. Over the past 10 years there has been a whopping development of distributional semantic models (DSMs, henceforth), especially after the Word2vec (<span class="citation">(<span class="citeproc-not-found" data-reference-id="mikolov2013efficient"><strong>???</strong></span>)</span>) revolution. This family of models assumes that the meaning of words and sentences can be inferred by the “distribution” of those words and sentences within a text corpus (the <em>Distributional Hypothesis</em> formulated by <span class="citation">(<span class="citeproc-not-found" data-reference-id="harris1954distributional"><strong>???</strong></span>)</span>). But the <em>Symbol Grounding Problem</em> mentioned earlier suggests that DSMs do not resemble the way words are learned by humans, which is in multimodal perceptual contexts. For these reasons, models have been developed with the goal to integrate further modalities (like visual ones) in pure language models, assuming that grounding words and sentences in other perceptual contexts should lead to a better understanding of their semantics and, as a result, to better performance in pure language tasks.
<br></p>
<p>The focus of this subchapter are models which empower pure language models with visual modalities in form of images: their goal is to obtain better semantic representations (in form of embedding vectors) of words. First, a quick recap of the main pure language models will be provided. After that, the historical evolution of the integration of images as visual modalities into pure language models will be discussed: from simple concatenation of textual and visual modalities, to the projection of visual elements in a common grounded space and more recently, the use of Transformers (see figure <a href="multimodal-architectures.html#fig:img-hist">3.6</a>). Eventually, a comprehensive evaluation of the different models against benchmarks will be carried out.
<br></p>
<p>Again, the focus is on how to employ visual elements to obtain embeddings able to capture the semantics of words. More concrete applications, such as those in the field of machine translation are out of scope and will be only marginally addressed at the end of the subchapter.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-hist"></span>
<img src="figures/02-03-img-support-text/Img-Hist.png" alt="Historical evolution of models which integrate visual information into pure language models. " width="100%" />
<p class="caption">
FIGURE 3.6: Historical evolution of models which integrate visual information into pure language models.
</p>
</div>
</div>
<div id="word-embeddings-survival-kit" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.2</span> Word-Embeddings: Survival-Kit<a href="multimodal-architectures.html#word-embeddings-survival-kit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In other parts of this books, the most important NLP-models and the latest developments in the field are extensively described. In this section, some information will be provided, which might be helpful to understand some of the aspects discussed in this subchapter. As it may have been inferred in the introduction, the starting point is always a pure language model, namely a model which employs only textual inputs in order to generate word embeddings, which are representations of words in form of numerical vectors.
The most widely used pure language models in the papers presented in this subchapter are the following three:</p>
<ul>
<li><strong>Skipgram</strong> (Word2vec, <span class="citation">(<span class="citeproc-not-found" data-reference-id="mikolov2013efficient"><strong>???</strong></span>)</span>), where given a target word, the probability of the neighboring (surrounding) words in a pre-defined window has to be maximized. Trainig takes place either through a <em>hierarchical softmax</em> or through <em>negative sampling</em>, which involves maximizing the probability of words which are real neighbors and minimizing that of words which are not real neighbors (the “negative samples”)</li>
<li><strong>GloVe</strong> (<span class="citation">(<span class="citeproc-not-found" data-reference-id="pennington2014glove"><strong>???</strong></span>)</span>), which is based on words co-occurrence across the <em>entire</em> corpus, with the goal of minimizing the difference between the dot product of the embedding vectors of two words and the logarithm of the number of co-occurrences</li>
<li><strong>BERT</strong> (<span class="citation">(<span class="citeproc-not-found" data-reference-id="devlin2018bert"><strong>???</strong></span>)</span>): two pre-training tasks to obtain word-embeddings:
<ul>
<li>Masked Language Modelling (MLM): given a sentence with [MASK]ed tokens, the goal is to predict these masked tokens</li>
<li>Next Sentence Prediction (NSP): given two sentences A and B, the goal is to predict if B follows from A</li>
</ul></li>
</ul>
<p>Two additional remarks to conclude this section. First, Skipgram and GloVe generate embeddings which are <em>“context-free”</em>: they do not take into account the context in which words occur. On the contrary, BERT is designed to represent words given the context (sentence) in which they occur: we can thus have different embeddings for the same word, depending on the context.
Second, the inputs of these models are <em>tokens</em>: with the help of a <em>tokenizer</em>, which can be different for different models, the text is split in “chunks”, called <em>tokens</em> (and they are not necessarily single words).</p>
</div>
<div id="the-beginning-sequential-multimodal-embeddings" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.3</span> The Beginning: Sequential Multimodal Embeddings<a href="multimodal-architectures.html#the-beginning-sequential-multimodal-embeddings" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Supposing we add linguistic and visual feature representations related to a particular word, how could we fuse them? One intuitive idea would be to <em>concatenate</em> the textual and visual modalities. Let <span class="math inline">\(V_{text}\)</span> be the textual (vectorial) representation of a word and let <span class="math inline">\(V_{img}\)</span> be its visual (vectorial) representation, a fused representation <span class="math inline">\(F\)</span> of a certain word <span class="math inline">\(w\)</span> might take the following simplified form:</p>
<p><span class="math display">\[F=\gamma(V_{text})\bigoplus(1-\gamma)V_{img}\]</span></p>
<p>where <span class="math inline">\(\gamma\)</span> is a tuning parameter which controls the relative contribution of both modalities to the final fused representation. <span class="citation">(<span class="citeproc-not-found" data-reference-id="bruni2014multimodal"><strong>???</strong></span>)</span> propose a model where the meaning of a target word is represented in the form of a semantic vector and all vectors are collected in a <em>text-based semantic matrix</em>; textual embeddings are computed based on (transformed) co-occurrence counts of words in a pre-defined window. The starting point to obtain an image-based representation of certain target word is a dataset of labeled images. For each image associated to the target word (which means that the target word is to be found in the image’s caption), low-level features called “local descriptors” - which incorporate geometric information of specific areas of a certain picture - are extracted and then these descriptors are assigned to clusters (<em>bags</em>) of “visual words”<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>. Afterwards, for each target word, visual word occurrences are summed up together to obtain the occurrence counts related to the target word. These image-based semantic vectors are then transformed and collected in an <em>image-based semantic matrix</em>. The two matrices are then concatenated and projected into a common latent multimodal space with a singular value decomposition. Thanks to this process a <em>textual <strong>mixed</strong> matrix</em> and a <em>visual <strong>mixed</strong> matrix</em> are extracted and then combined together according to different fusion strategies to build the multimodal embeddings. In this first, relatively cumbersome (historically motivated) example, the vector representation of an image is obtained with non-trivial features engineering.
<br></p>
<p>In recent years, the use of neural networks has made an “automatic feature selection” possible. This is what for example <span class="citation">(<span class="citeproc-not-found" data-reference-id="kiela2014learning"><strong>???</strong></span>)</span> propose, extracting visual features from the first seven layers of a convolutional neural network (proposed by <span class="citation">(<span class="citeproc-not-found" data-reference-id="krizhevsky2012imagenet"><strong>???</strong></span>)</span>) trained on 1.6 million images from the ImageNet database (<span class="citation">(<span class="citeproc-not-found" data-reference-id="deng2009imagenet"><strong>???</strong></span>)</span>), which produces scores for 1,512 object categories. The linguistic part of the model relies on the Skipgram model by <span class="citation">(<span class="citeproc-not-found" data-reference-id="mikolov2013efficient"><strong>???</strong></span>)</span> and consists of 100-dimensional vector representations. The multimodal representation is again obtained by concatenation of both modalities.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-kiela2014-01"></span>
<img src="figures/02-03-img-support-text/img-kiela2014-01.png" alt="From @kiela2014learning. Textual and visual features vectors are concatenated." width="100%" />
<p class="caption">
FIGURE 3.7: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="kiela2014learning"><strong>???</strong></span>)</span>. Textual and visual features vectors are concatenated.
</p>
</div>
<p>Another notable example of concatenation/sequential combination of textual and visual modalities is the work of <span class="citation">(<span class="citeproc-not-found" data-reference-id="silberer2014learning"><strong>???</strong></span>)</span>: textual and visual modalities are represented by separate vectors of textual and visual attributes. During training, these textual and visual inputs vectors are separately fed to denoising (unimodal) autoencoders, the training objective of which is the reconstruction of a certain corrupted input - e.g. through masking noise - from a latent representation. Their outputs are then jointly fed to a bimodal autoencoder to be mapped to a multimodal space, on which a softmax layer (classification layer) is added, which allows the architecture to be fine-tuned for different tasks.</p>
</div>
<div id="the-grounded-space" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.4</span> The Grounded Space<a href="multimodal-architectures.html#the-grounded-space" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The aforementioned models assume implicitly a one-to-one correspondence between text and images: a visual representation is extracted only from words which are associated to a concrete image. This is a limitation, for two partially overlapping reasons. One one hand, how can we depict words for which no image is available in our training set? Is it possible to <em>imagine</em> visual representations purely from linguistic ones? On the other hand, could we hypothetically find a visual representation for each word? This might be true for concrete words but when it comes to abstract ones, it is not always possible to find suitable visual representations or, said in other terms, many words are not visually grounded. For this reasons, researches have addressed the question: could we map textual and visual elements to a grounded space and design models able to generalize images and words beyond those in the training set? Well, the answer is yes!</p>
<p><br></p>
<p><span class="citation">(<span class="citeproc-not-found" data-reference-id="lazaridou2015combining"><strong>???</strong></span>)</span> propose a multimodal Skip-gram architecture where the objective function of a Skip-gram is “augmented” with an additional visual objective: <span class="math display">\[\frac{1}{T}\sum_{t=1}^{T}\left(\mathcal{L}_{ling}(w_{t})+\mathcal{L}_{vision}(w_{t})\right)\]</span></p>
<p>where <span class="math inline">\(\mathcal{L}_{ling}\)</span> is the Skip-gram loss function and <span class="math inline">\(\mathcal{L}_{vision}\)</span> is the additional visual loss for the target word <span class="math inline">\(w_{t}\)</span>. In particular, <span class="math inline">\(\mathcal{L}_{vision}\)</span> has the form of a hinge loss, the goal of which is to make the (vectorial) linguistic representation of a certain word more similar to its visual representation:</p>
<p><span class="math display">\[\mathcal{L}_{vision}(w_{t})=-\sum_{w^{&#39;}\sim P_{n}(w)}\left(max(0,\gamma-cos(z_{w_{t}},v_{w_{t}})+cos(z_{w_{t}},v_{w^{&#39;}})\right)\]</span></p>
<p>where <span class="math inline">\(v_{w^{&#39;}}\)</span> is a visual representation of a randomly chosen word <span class="math inline">\(w^{&#39;}\)</span> (drawn from a probability distribution <span class="math inline">\(P_{n}(w)\)</span>) used as negative sample, <span class="math inline">\(v_{w_{t}}\)</span> is the corresponding visual vector and <span class="math inline">\(z_{w_{t}}\)</span> is the target multimodal word representation which has to be learned by the model. It is nothing more than a linear transformation of a word representation <span class="math inline">\(u_{w_{t}}\)</span>: <span class="math inline">\(z_{w_{t}}=M^{u\rightarrow v}u_{w_{t}}\)</span> and <span class="math inline">\(M^{u\rightarrow v}\)</span> is a cross-modal mapping matrix from linguistic inputs to a visual representation. It is important to remark that during training, for words which do not have associated images, <span class="math inline">\(\mathcal{L}_{vision}\)</span> gets set to zero. When this cross-modal mapping matrix is estimated, it is then possible to find a visual representation for new words, which do not have a related image in the training set: the model allows to <em>imagine</em> new words. This is what is meant with grounded space: a perceptual (visual, in this case) space where a word is <em>grounded</em>, put in context.</p>
<p><br></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-lazaridou2015-01"></span>
<img src="figures/02-03-img-support-text/img-lazaridou2015combining01.png" alt="From @lazaridou2015combining. The linguistic embedding of the word 'cat' is mapped to a visual space, such that the similarity of vector representations of words and associated images is maximized." width="80%" />
<p class="caption">
FIGURE 3.8: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="lazaridou2015combining"><strong>???</strong></span>)</span>. The linguistic embedding of the word ‘cat’ is mapped to a visual space, such that the similarity of vector representations of words and associated images is maximized.
</p>
</div>
<p><br></p>
<p>Similar instances of a cross-modal mapping can be found for example in <span class="citation">(<span class="citeproc-not-found" data-reference-id="kottur2016visual"><strong>???</strong></span>)</span> (a multimodal extension of the CBOW model specification of word2vec) and in <span class="citation">(<span class="citeproc-not-found" data-reference-id="collell2017imagined"><strong>???</strong></span>)</span>, where visual features are obtained from the forward pass of a CNN, pre-trained on ImageNet (<span class="citation">(<span class="citeproc-not-found" data-reference-id="deng2009imagenet"><strong>???</strong></span>)</span>) and a mapping function from the textual space to the visual space is obtained as a result of the training process. Also in this case it is possible to generate a visual representation from the embedding of a certain word, not necessarily present in the training set. In particular, they propose two specifications of the mapping function: a simple linear mapping and neural network with a single hidden layer. Last but not least, <span class="citation">(<span class="citeproc-not-found" data-reference-id="hill2014learning"><strong>???</strong></span>)</span> recognize that concrete nouns are more likely to have a visual representation. For this reason, they map a set of concrete words (CSLB, <span class="citation">(<span class="citeproc-not-found" data-reference-id="devereux2014centre"><strong>???</strong></span>)</span>) to “bags of perceptual/visual features” and every time one of these words is encountered during training, the Skip-gram model they are using stops training on that sentence and instead continues the training on a newly created “pseudo-sentence”, which takes into consideration the aforementioned bag of perceptual features. This list is unfortunately not exhaustive and there are other models with similar ideas, for example <span class="citation">(<span class="citeproc-not-found" data-reference-id="ailem2018probabilistic"><strong>???</strong></span>)</span> or <span class="citation">(<span class="citeproc-not-found" data-reference-id="kiros2018illustrative"><strong>???</strong></span>)</span>.
<br></p>
<p>The aforementioned papers and related models focus on the modeling of semantics of words. Nonetheless, there are models designed to address tasks at sentence-level, such as sentiment analysis or sentence entailment. <span class="citation">(<span class="citeproc-not-found" data-reference-id="kiela2017learning"><strong>???</strong></span>)</span> employ a bidirectional Long Short-Term Memory (LSTM, <span class="citation">(<span class="citeproc-not-found" data-reference-id="hochreiter1997long"><strong>???</strong></span>)</span>) architecture to model sentence representations, in order to gain information from the text in both directions. The goal is again to encode a sentence and ground it in an image. Textual embeddings are obtained with GloVe (<span class="citation">(<span class="citeproc-not-found" data-reference-id="pennington2014glove"><strong>???</strong></span>)</span>) and they are then projected on a grounded space with a linear mapping. This grounded word vector serves as input for the bidirectional LSTM, which is trained together with the linear mapping. Their model is versatile and depending on the loss function specification, it can not only propose alternative captions to an image (which is a way to frame sentence equivalence tasks) but also predict captions from images or perform both tasks at the same time. This last point highlights an important characteristic of many of the models discussed in this subchapter: even though the focus is on the empowerment of pure language models with the addition of visual elements, some of the models discussed here can be used for purposes other than pure language tasks. The control over which task is performed is usually exercised by either specifying different loss functions (as in the last model described) or setting properly certain hyperparameters (such as in the previously described model by <span class="citation">(<span class="citeproc-not-found" data-reference-id="silberer2014learning"><strong>???</strong></span>)</span>).<br />
<br></p>
</div>
<div id="the-transformers-era" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.5</span> The Transformers Era<a href="multimodal-architectures.html#the-transformers-era" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A turning point for the field of NLP was <span class="citation">(<span class="citeproc-not-found" data-reference-id="vaswani2017attention"><strong>???</strong></span>)</span>’s paper “Attention is all you need”, where the authors proposed for two machine translation tasks a novel architecture, the Transformer (not to be confused with the giant robots from the Michael Bay’s blockbuster movies!), which leverages only the attention mechanism. Even though an exhaustive description of the Transformer architecture is beyond the scope of this subchapter, it is worth mentioning why they became so popular over the past four years in the field of NLP (among others), in comparison to Recurrent Neural Networks (RNNs) and Long Short-Term Memory networks (LSTMs).
<br></p>
<p>Well, the three main properties of Transformers are the following:</p>
<ul>
<li>Self-Attention</li>
<li>Parallel input processing</li>
<li>Positional embeddings<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a></li>
</ul>
<p>When feeding for example a textual sentence to a RNN, the network deals with one word after the other in a sequential fashion and one of the known issues is the fact that information contained in earlier parts of the sequence tend to “fade away” as the sentence is analyzed further: newer inputs carry a larger influence on the outputs at a given step. LSTMs try to mitigate this problem by introducing a component called “gate”, which regulates the information flow, namely which information from the past inputs need to be “remembered” by the model. The goal is to capture long-term dependencies among different parts of the sentence fed into the model.<br />
On the contrary, thanks to the Self-Attention mechanism, at each step Transformers can access previous steps, thus limiting to a minimum the loss of information. Moreover, inputs are processed not sequentially but all at the same time, thus allowing to capture dependencies by looking at the sentence <em>as a whole</em> and this could make a fundamental difference in many downstream applications: for example in the German language, in dependent clauses (“Nebensaetze”), the verb comes at the end of the phrase but it determines the verbal case of the nouns that come <em>before</em> the verb. Thus Transformer could potentially capture the dependencies between the verb coming at the end of the sentence and the words at the beginning. Lastly, Transformers encode for every input information on its position within a sentence, since it is often the case, that the importance and meaning of a certain word varies depending on its position within a sentence. These were the Transformers, in a nutshell.
<br></p>
<p>But Transformers did not only bring a change of paradigm in terms of architectures. First, while for models in the pre-Transformers era described before, the focus was on the ability of word embeddings to capture similarity among words, now the focus has shifted more on downstream tasks (more on this later in the evaluation section), encompassing not only pure linguistic ones but also tasks with visual components, such as for example, visual question answering. It is now more difficult (but not impossible) to draw a line between models where “images support pure language models” (the object of this subchapter) and models which could be actually categorized as “vision and language” models but they can be employed also to solve pure linguistic tasks. This issue brings another peculiarity of many Transformers-base models, namely their “universal vocation”: without loss of generality we could say that the idea is now to design powerful (multimodal) pre-training (mostly <em>self-supervised</em>) tasks capable of generating task-agnostic representations, whose encoded knowledge can be efficaciously transferred to diverse downstream tasks, limiting the amount of labeled data necessary to fine-tune the models (this is the so-called <em>few-shot learning</em>).
<br></p>
<p>Let’s briefly discuss two examples, Flava (<span class="citation">(<span class="citeproc-not-found" data-reference-id="singh2022flava"><strong>???</strong></span>)</span>) and UniT (<span class="citation">(<span class="citeproc-not-found" data-reference-id="hu2021unit"><strong>???</strong></span>)</span>). Flava has two separate encoders for images and text and a multimodal encoder, all based on the Vision Transformer (<span class="citation">(<span class="citeproc-not-found" data-reference-id="dosovitskiy2020image"><strong>???</strong></span>)</span>). Unimodal pre-training consists of masked image modeling (where a set of image patches are to be reconstructed from other unmasked image patches) and masked language modeling. Multimodal pre-training tasks consist instead of a global contrastive loss (maximization of cosine similarities between paired images and text), a masked multimodal modeling (where image patches and text tokens are masked) and an image-text matching task. The model is pre-trained jointly on unimodal and multimodal datasets and then evaluated (fine-tuned) on 22 vision tasks, 8 pure linguistic tasks and 5 vision and language tasks.<br />
UniT has an image encoder and a text encoder, a multimodal domain-agnostic decoder and task-specific heads. There is no pre-training on multimodal data and the model is trained end-to-end on 7 tasks (vision, language and vision an language) and 8 datasets, with the idea that solving different tasks across domains in a jointly fashion should prevent general knowledge from being lost due to fine-tuning over particular downstream tasks.
<br></p>
<p>These two examples clearly show what it is meant by “universal vocation” of many modern Transformer-based models. But there are still models specifically designed to solve pure language tasks and in the following pages, two of them will be described.</p>
<div id="vokenization" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.2.5.1</span> Vokenization<a href="multimodal-architectures.html#vokenization" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>It is often difficult for a child to describe the meaning of a certain word. A child might not be able to describe what a lion is but if he is given pictures of different animals he might be very well able to point at the picture of a lion. <em>Visual pointing</em> could thus act as a form of supervision to natural language. Is it possible to build within a pure language model a form of visual supervision, which mimics the visual pointing often adopted by children? This is exactly the problem that <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span> try to address: how to associate to each textual representation (token) a visual representation (Voken).
<br></p>
<p>Let’s suppose we had a dataset of word(token)-image pairs. We could integrate in the pre-training framework of pure language models the following <em>Voken-Classification</em> task:</p>
<p><span class="math display">\[\mathcal{L}_{VOKEN-CLS}(s)=-\sum_{i=1}^{l}log\ p_{i}(v(w_{i};s)|s) \]</span>
<span class="math display">\[\textbf{h}_{1}, \textbf{h}_{2},...,\textbf{h}_{l}=languagemodel(w_{1},w_{2},...,w_{l}) \]</span>
<span class="math display">\[p_{i}(v|s)=softmax_{v}\{W\textbf{h}_{i}+b\}\]</span>
where <span class="math inline">\(\{h_i\}\)</span> is the feature representation of each token in a sentence <span class="math inline">\(s=\{w_i\}\)</span> extracted from a language model (such as BERT) and the vokens originate from a <strong>finite</strong> set of images <span class="math inline">\(X\)</span>. Each <span class="math inline">\(h_i\)</span> is then transformed into a probability distribution through a softmax layer, with the voken-classification loss defined as the negative log-likelihood of all related vokens.<br />
The model architecture would then be:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-tan2020-04"></span>
<img src="figures/02-03-img-support-text/img-tan2020-04.png" alt="From @tan2020vokenization. Visually supervised the language model with token-related images, called Vokens." width="70%" />
<p class="caption">
FIGURE 3.9: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span>. Visually supervised the language model with token-related images, called Vokens.
</p>
</div>
<p>Everything sounds fantastic! There is only one small pitfall: a set of <span class="math inline">\(X\)</span> of images for all tokens does not exist! Could we find a proxy for such a set? One might consider image-captioning datasets such as MS COCO (<span class="citation">(<span class="citeproc-not-found" data-reference-id="lin2014microsoft"><strong>???</strong></span>)</span>). But also this suboptimal solution is problematic.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-tan2020-01"></span>
<img src="figures/02-03-img-support-text/img-tan2020-01.png" alt="From @tan2020vokenization. Statistics of image-captioning dataset and other natural language corpora. VG, CC, Eng Wiki, and CNN/DM denote Visual Genome, Conceptual Captions, English Wikipedia, and CNN/Daily Mail, respectively. JSD represents Jensen–Shannon divergence to the English Wikipedia corpus." width="100%" />
<p class="caption">
FIGURE 3.10: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span>. Statistics of image-captioning dataset and other natural language corpora. VG, CC, Eng Wiki, and CNN/DM denote Visual Genome, Conceptual Captions, English Wikipedia, and CNN/Daily Mail, respectively. JSD represents Jensen–Shannon divergence to the English Wikipedia corpus.
</p>
</div>
<p>The <em>Grounding Ratio</em> is defined as the proportion of tokens in a dataset which are related to a specific visual representation (i.e. the tokens are <em>visually grounded</em>), such as “dog”, “table” and the like. In figure <a href="multimodal-architectures.html#fig:img-tan2020-01">3.10</a> it is striking that only around one third of tokens contained in pure language corpora such Wiki103, English Wikipedia and CNN/DM are visually grounded in image captioning datasets<a href="#fn3" class="footnote-ref" id="fnref3"><sup>3</sup></a>. It is not possible to rely (only) on image captioning datasets to build the Voken-Classification task. But the fact that a word/token does not have a visual representation in one of these datasets, it does not mean that it is not possible to visually represent the word/token. Would it be possible to associate images to words/tokens not directly visually grounded? Well, the answer is yes!</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-tan2020-05"></span>
<img src="figures/02-03-img-support-text/img-tan2020-05.png" alt="From @tan2020vokenization. The Vokenization process. A contextualized image (visual token, Voken) is retrieved for every token in a sentence and with this visual token, visual supervision is performed." width="80%" />
<p class="caption">
FIGURE 3.11: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span>. The Vokenization process. A contextualized image (visual token, Voken) is retrieved for every token in a sentence and with this visual token, visual supervision is performed.
</p>
</div>
<p>The <strong>Vokenization</strong> is a process to <em>assign</em> every token <span class="math inline">\(w_i\)</span> contained in a sentence <span class="math inline">\(s\)</span> to a visual representation (called <em>voken</em>) originating not from a generative model but rather from a finite set of images <span class="math inline">\(X=\{x_1,...,x_n\}\)</span>. The voken <span class="math inline">\(v(w_i;s)\)</span> is the image from <span class="math inline">\(X\)</span> which maximizes the following <em>Relevance Score Function</em>:
<span class="math display">\[v(w_i;s)=arg\ max_{x\in X}r_{\theta^{*}}(w_i,x,s)\]</span>
This function takes into account not only the token <span class="math inline">\(w_i\)</span> itself, but also the context (the sentence) and it is parametrized by <span class="math inline">\(\theta\)</span> with <span class="math inline">\(\theta^{*}\)</span> being the optimal value (which has to be estimated).</p>
<div id="the-relevance-score-function-model-training-inference" class="section level5 hasAnchor">
<h5><span class="header-section-number">3.2.5.1.1</span> The Relevance Score Function: Model, Training, Inference<a href="multimodal-architectures.html#the-relevance-score-function-model-training-inference" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p><br />
The Relevance Score Function is defined as the inner product of the language feature representation <span class="math inline">\(f_{\theta}(w_i,s)\)</span> and the visual feature representation <span class="math inline">\(g_{\theta}(x)\)</span>:
<span class="math display">\[f_{\theta}(w_i,s)^Tg_{\theta}(x)\]</span>
Supposing <span class="math inline">\(h_1,...,h_l\)</span> and <span class="math inline">\(e\)</span> are the embeddings originating from pre-trained language and visual encoders respectively (in the paper the authors use BERT and ResNeXt), the language and visual representations are obtained first by applying multi-layer perceptrons <span class="math inline">\(w\_mlp_{\theta}\)</span> and <span class="math inline">\(x\_mlp_{\theta}\)</span> to downproject the embeddings from the pre-trained models to a common vector space and secondly they are normalized (with L2-Norm):</p>
<p><span class="math display">\[ \textbf{f}_{\theta}(w_{i};s)= \frac{w{\_}mlp_{\theta}(\textbf{h}_{i})}{||w{\_}mlp_{\theta}(\textbf{h}_{i})||} \]</span>
<span class="math display">\[ \textbf{g}_{\theta}(x)= \frac{x{\_}mlp_{\theta}(\textbf{e})}{||x{\_}mlp_{\theta}(\textbf{e})||} \]</span>
With respect to the training of the model, to estimate the optimal value for the parameter <span class="math inline">\(\theta\)</span>, image-captioning datasets, which are collections of sentence-image pairs, are employed. Operationally, for every sentence <span class="math inline">\(s_k\)</span> associated to image <span class="math inline">\(x_k\)</span> in the image-captioning dataset, each token <span class="math inline">\(w_i\)</span> in <span class="math inline">\(s\)</span> is associated to <span class="math inline">\(x_k\)</span> and the <em>hinge loss</em> is used to estimate the optimal value of <span class="math inline">\(\theta^*\)</span>:</p>
<p><span class="math display">\[ \mathcal{L}_{\theta}(s,x,x&#39;)=\sum_{i=1}^{l}max(0,M-r_{\theta}(w_{i},x,s)+r_{\theta}(w_{i},x&#39;,s))\]</span></p>
<p>The goal is to maximize the Relevance Score Function between aligned token-image pairs <span class="math inline">\((w_i,x;s)\)</span> and to minimize the score for unaligned pairs <span class="math inline">\((w_i,x^{&#39;};s)\)</span> by at least a margin <span class="math inline">\(M\)</span>, with <span class="math inline">\(x^{&#39;}\)</span> being a randomly sampled image from the image captioning dataset <strong>not</strong> associated to sentence <span class="math inline">\(s\)</span>.
<br></p>
<p>Once we have the language feature representation <span class="math inline">\(f_{\theta}(w_i,s)\)</span> for each token in our language corpus and the optimal estimate of <span class="math inline">\(\theta\)</span>, how is it possible to find the image <span class="math inline">\(x\)</span> encoded with the visual feature representation <span class="math inline">\(g_{\theta}(x)\)</span>, which maximizes the Relevance Score Function? As said earlier, the function is expressed as the inner product of the textual and visual representations and since the feature vectors have euclidean norm equal to 1, the inner product maximization problem is equivalent to a nearest neighbor search problem. It is just sufficient to find the vector <span class="math inline">\(g_{\theta}(x)\)</span> which is the nearest neighbor of <span class="math inline">\(f_{\theta}(w_i,s)\)</span><a href="#fn4" class="footnote-ref" id="fnref4"><sup>4</sup></a>.</p>
<p>With this process, it is thus possible to assign a visual representation, a voken, to any word/token in a language corpus, pooling from a finite set of images. The problem of the low Grounding Ratio outlined above is solved and the Voken-Classification task could be integrated in the pre-training framework of any pure language model. Moreover, the authors propose a method called <em>Revokenization</em>, which allows to transfer vokens generated using a particular tokenizer to frameworks which employ other tokenizers.</p>
</div>
</div>
<div id="one-step-further-the-power-of-imagination" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.2.5.2</span> One Step Further: The Power Of Imagination<a href="multimodal-architectures.html#one-step-further-the-power-of-imagination" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Wikipedia defines <em>imagination</em> as “the production or simulation of novel objects, sensations, and ideas in the mind without any immediate input of the senses”. Indeed, humans do not only associate words with real images, but also leverage the ability to <em>imagine</em> words/concepts: imagination can help the human brain solve problems with limited supervision or sample points by empowering its generalization capabilities. Until now we discussed language models supported by visual information in form of <em>real</em> images (e.g. those retrieved from image-captioning datasets). But with the recent advancements in the field of generative models for images, it is for sure worth investigating if these generative models can help pure language models to produce better representations of words. In particular, the framework proposed by <span class="citation">(<span class="citeproc-not-found" data-reference-id="lu2022imagination"><strong>???</strong></span>)</span>, <strong>iACE (Imagination-Augmented Cross-Modal Encoder)</strong> will now be discussed: the idea is simply to use a generative model to obtain a visual representation of a textual input and then use these imagined representations as “imagination supervision” to pure language models.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-lu2022-01"></span>
<img src="figures/02-03-img-support-text/img-lu2022-01.png" alt="From @lu2022imagination. The generator $G$ visualize imaginations close to the encoded texts by minimizing $\mathcal{L}_{GAN}$. The cross-modal encoder $E_c$ learns imagination-augmented language representation. Two-step learning procedure consists of: 1) pre-train a Transformer with visual supervision from large-scale language corpus and image set, 2) fine-tune the visually supervised pre-trained Transformer and the imagination-augmented cross-modal encoder on downstream tasks." width="100%" />
<p class="caption">
FIGURE 3.12: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="lu2022imagination"><strong>???</strong></span>)</span>. The generator <span class="math inline">\(G\)</span> visualize imaginations close to the encoded texts by minimizing <span class="math inline">\(\mathcal{L}_{GAN}\)</span>. The cross-modal encoder <span class="math inline">\(E_c\)</span> learns imagination-augmented language representation. Two-step learning procedure consists of: 1) pre-train a Transformer with visual supervision from large-scale language corpus and image set, 2) fine-tune the visually supervised pre-trained Transformer and the imagination-augmented cross-modal encoder on downstream tasks.
</p>
</div>
<p>This framework has two main components:</p>
<ul>
<li>the <strong>imagination generator <span class="math inline">\(G\)</span></strong>: given an input text <span class="math inline">\(x\)</span>, VQGAN (<span class="citation">(<span class="citeproc-not-found" data-reference-id="esser2021taming"><strong>???</strong></span>)</span>) is used to render an “imagination” <span class="math inline">\(i\)</span> of <span class="math inline">\(x\)</span> and CLIP (<span class="citation">(<span class="citeproc-not-found" data-reference-id="radford2021learning"><strong>???</strong></span>)</span>) is used to see how well the generated image <span class="math inline">\(i\)</span> is aligned to the input text <span class="math inline">\(x\)</span>. This generative framework is known as VQGAN+CLIP</li>
<li><strong>Cross-modal Encoder <span class="math inline">\(E_c\)</span></strong>: the input text and the rendered imagination are firstly encoded with a language and a visual encoder respectively and then CLIP is employed as cross-modal encoder with inputs being text-imagination pairs</li>
</ul>
<p>The learning procedure is composed of two main steps (depicted in figure <a href="multimodal-architectures.html#fig:img-lu2022-01">3.12</a>): the first step consists in the pre-training of a visually supervised Transformer. In particular, the Voken-Classification task described before is employed, alongside a masked language modeling task. This is the baseline model, where no information from the “imagination” procedure comes yet into play. The second step is the <em>imagination-augmented fine-tuning</em> with two downstream datasets <span class="math inline">\(D\)</span> (GLUE, <span class="citation">(<span class="citeproc-not-found" data-reference-id="wang2018glue"><strong>???</strong></span>)</span> and SWAG, <span class="citation">(<span class="citeproc-not-found" data-reference-id="zellers2018swag"><strong>???</strong></span>)</span>).<br />
On one side, the visually-supervised Transformer (the baseline) relies only on the textual input during the fine-tuning phase and the following loss function is employed:</p>
<p><span class="math display">\[ \mathcal{L}_{Lang}=-\sum_{j=1}^{|D|}\sum_{k=1}^{K}y_{k}\ log\ p_{k}(d_{j}(t)|D) \]</span></p>
<p>On the other hand, the <em>iACE</em> is trained to minimize the following cross-entropy loss:</p>
<p><span class="math display">\[ \mathcal{L}_{Imagine}=-\sum_{j=1}^{|D|}\sum_{k=1}^{K}y_{k}\ log\ p_{k}(d_{j}(t,v)|D) \]</span></p>
<p>with <span class="math inline">\(t\)</span> and <span class="math inline">\(v\)</span> being the textual and imagined features representations respectively, <span class="math inline">\(j\)</span> indicates the <span class="math inline">\(j\)</span>-th data sample in dataset belonging to dataset <span class="math inline">\(D\)</span>, <span class="math inline">\(K\)</span> is the number of classes and <span class="math inline">\(p_k\)</span> is the conditional distribution of <span class="math inline">\(d_j\)</span>.
Training takes place in a jointly fashion and both losses, the imagination-augmented one <span class="math inline">\(\mathcal{L}_{Imagine}\)</span> and the pure language loss <span class="math inline">\(\mathcal{L}_{Lang}\)</span> are linearly combined, with <span class="math inline">\(\lambda\)</span> being a balance factor:</p>
<p><span class="math display">\[\mathcal{L}=\lambda\mathcal{L}_{Imagine}+(1-\lambda)\mathcal{L}_{Lang} \]</span></p>
<p>To sum up, this model-agnostic framework uses <em>generated images</em> for visual supervision and could be integrated on top of pure language models (such as BERT) or visually supervised models (such as the Voken model, which uses Vokens, real images for visual supervision).</p>
</div>
</div>
<div id="was-it-worth" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.6</span> Was It Worth?<a href="multimodal-architectures.html#was-it-worth" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this subchapter we investigated how visual inputs can support pure language models in capturing the semantics of words. We started with simple concatenation of linguistic and visual features and ended up with Transformer-based models, which are able to shape different word embeddings for the same word by taking into account also the context (the sentence). But now the question arises: with the addition of visual information, do we obtain word embeddings that are better than those from pure language models? In other words, is what we all have so far discussed worth? Well, as it is often the case in scientific research, the answer is: “it depends!”
<br></p>
<p>Individual evaluation of each single model might not be ideal because each model has its peculiarities and it is impractical to make a direct comparison among them. It is more useful to capture and discuss the themes which are common to many models, in order to understand their strengths and weaknesses. This is how we will proceed and we will also differentiate between evaluation before Transformers and evaluation after Transformers.</p>
<div id="evaluation-in-the-pre-transformers-era" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.2.6.1</span> Evaluation In The Pre-Transformers Era<a href="multimodal-architectures.html#evaluation-in-the-pre-transformers-era" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Before the advent of Transformers, the evaluation focus was on the degree of alignment between learned semantic representations (word embeddings) and representations by human speakers, in form of correlation between model-based and human-based word-similarity judgments. Three main types of similarity are usually considered:</p>
<ul>
<li><p>Semantic similarity, e.g. “pasta is similar to rice”</p></li>
<li><p>Semantic relatedness, e.g. “Bear is related to mountain”</p></li>
<li><p>Visual similarity, e.g. “cucumbers look like zucchinis”</p></li>
</ul>
<p>The evaluation pipeline could be summarized as follows:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-eval01"></span>
<img src="figures/02-03-img-support-text/img-eval01.png" alt="Pipeline for intrisinsic evaluation of semantic representations. In the first step, the cosine similarity between two word embeddings w1 and w2 is used as similariry measure and in a second step, the correlation with human speakers'assessment is computed to gauge the quality of the embeddings. The higher the correlation, the better the embeddings." width="100%" />
<p class="caption">
FIGURE 3.13: Pipeline for intrisinsic evaluation of semantic representations. In the first step, the cosine similarity between two word embeddings w1 and w2 is used as similariry measure and in a second step, the correlation with human speakers’assessment is computed to gauge the quality of the embeddings. The higher the correlation, the better the embeddings.
</p>
</div>
<p>Word embeddings are vectors and to measure the degree of similarity between two vectors, the <em>Cosine Similarity</em> is often used in the literature. In an ideal setting, we would have word embeddings with the following characteristics: if two words are semantically similar, the two embedding vectors should be similar and their cosine similarity should go towards 1. If the two words are unrelated, the embedding vectors should be orthogonal to each other and as a consequence, the cosine similarity should go towards zero. Lastly, if two words are negatively related, the two embedding vectors should point at opposite directions and the cosine similarity should go towards -1.
<br>
Once these similarity measures between word pairs are computed, in order to measure the quality of the embeddings several benchmarks can be employed, such as MEN (<span class="citation">(<span class="citeproc-not-found" data-reference-id="bruni2014multimodal"><strong>???</strong></span>)</span>), WordSim353 (<span class="citation">(<span class="citeproc-not-found" data-reference-id="agirre2009study"><strong>???</strong></span>)</span>) and SimLex999 (<span class="citation">(<span class="citeproc-not-found" data-reference-id="hill2015simlex"><strong>???</strong></span>)</span>). These datasets could be described as collections of word pairs and associated similarity ratings by human speakers. Operationally, this means that real people were asked if a pair of words was related or not and to which degree, on a scale between -1 (negatively related) to +1 (semantically equivalent). The higher the correlation between the cosine similarity and the similarity judgments by humans, the higher the quality of the word embeddings. Having done this methodological premise, let’s discuss the performance of these pre-Transformer models!
<br></p>
<p>Since the goal of these models is to enhance pure language models with the addition of visual inputs, the baseline in the evaluation is always one (or more) pure language model(s). Well, do visually grounded embeddings outperform non-grounded ones? What emerges from virtually all papers is that visual grounding can actually help get a better semantic representation of <em>concrete</em> concepts, such as “cat”, “table”, “bicycle”, whereas they do not help much with the representation of abstract concepts such as “love” and “peace”.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-2014hill-01"></span>
<img src="figures/02-03-img-support-text/img-2014hill-01.png" alt="From @hill2014learning: Each bar represents a different model settings and the dashed line indicates the pure linguistic benchmark model." width="100%" />
<p class="caption">
FIGURE 3.14: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="hill2014learning"><strong>???</strong></span>)</span>: Each bar represents a different model settings and the dashed line indicates the pure linguistic benchmark model.
</p>
</div>
<p>In figure <a href="multimodal-architectures.html#fig:img-2014hill-01">3.14</a> we can see that pure language models still perform better than models with visual inputs when it comes to the representation of abstract <em>nouns</em>. Another example is <span class="citation">(<span class="citeproc-not-found" data-reference-id="kiela2017learning"><strong>???</strong></span>)</span>: they found that their models perform better when tested on datasets with a higher degree of concreteness and the same conclusion is reached by <span class="citation">(<span class="citeproc-not-found" data-reference-id="collell2017imagined"><strong>???</strong></span>)</span>, which state that visual information can empower the representations of concepts that are to a certain extent visual. To sum up, effective semantic representation of abstract concepts constitute the main limitation common to many of the models discussed in this section.</p>
</div>
<div id="evaluation-in-the-post-transformers-era" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.2.6.2</span> Evaluation In The Post-Transformers Era<a href="multimodal-architectures.html#evaluation-in-the-post-transformers-era" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A limitation of the <em>intrinsic</em> evaluation metrics is the high degree of subjectivity: the <em>similarity</em> between two concepts depends in many instances on the experience, cultural background and preferences of the human observers. This is why the evaluation focus has now shifted to a more <em>extrinsic</em> dimension: how well do the models perform in downstream tasks? The problem of the “lack of objectivity” is thus solved because on downstream tasks there is no room for opinions. The datasets used to train the models are also different and the most widely used are:</p>
<ul>
<li>GLUE (<span class="citation">(<span class="citeproc-not-found" data-reference-id="wang2018glue"><strong>???</strong></span>)</span>): 9 tasks, including single-sentence tasks (e.g. sentiment analysis), similarity tasks (e.g. paraphrasing), inference tasks (e.g. textual entailment)</li>
<li>SQuAD (<span class="citation">(<span class="citeproc-not-found" data-reference-id="rajpurkar2016squad"><strong>???</strong></span>)</span>): question/answer pairs</li>
<li>SWAG (<span class="citation">(<span class="citeproc-not-found" data-reference-id="zellers2018swag"><strong>???</strong></span>)</span>): multiple choice questions about grounded situations</li>
</ul>
<p>As previously discussed, many Transformer-based models have universal vocation: they are built to solve a heterogeneous range of tasks from the language and vision domain. If we thus consider only performance on pure language tasks, the following two tables from <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span> are insightful:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-tan2020-02"></span>
<img src="figures/02-03-img-support-text/img-tan2020-02.png" alt="From @tan2020vokenization. Results of vision-and-language pre-trained models (universal models) on GLUE tasks compared to baseline models (BERT)." width="100%" />
<p class="caption">
FIGURE 3.15: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span>. Results of vision-and-language pre-trained models (universal models) on GLUE tasks compared to baseline models (BERT).
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-tan2020-03"></span>
<img src="figures/02-03-img-support-text/img-tan2020-03.png" alt="From @tan2020vokenization. Fine-tuning results of different pre-trained models w/ or w/o the voken classification task (denoted as“Voken-cls”)." width="100%" />
<p class="caption">
FIGURE 3.16: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="tan2020vokenization"><strong>???</strong></span>)</span>. Fine-tuning results of different pre-trained models w/ or w/o the voken classification task (denoted as“Voken-cls”).
</p>
</div>
<p>It is straightforward: unlike in the pre-Transformers Era, where grounded word embeddings could improve performance over baselines, Transformer-based universal models <strong>do not</strong> outperform pure language models such as BERT or RoBERTa. Nonetheless, the addition of visual supervision (the Voken-Classification task) in the pre-training framework can boost performance above the level of pure language models.</p>
<p><br></p>
<p><span class="citation">(<span class="citeproc-not-found" data-reference-id="pezzelle2021word"><strong>???</strong></span>)</span> analyzed the <em>intrinsic</em> quality of embeddings of some vision and language (“universal”) models:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-pezzele2021-01"></span>
<img src="figures/02-03-img-support-text/img-pezzele2021-01.png" alt="From @pezzelle2021word. Spearman’s rank correlation between similarities computed with representations by all tested models and human similarity judgments in the five evaluation benchmarks." width="100%" />
<p class="caption">
FIGURE 3.17: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="pezzelle2021word"><strong>???</strong></span>)</span>. Spearman’s rank correlation between similarities computed with representations by all tested models and human similarity judgments in the five evaluation benchmarks.
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-pezzele2021-02"></span>
<img src="figures/02-03-img-support-text/img-pezzele2021-02.png" alt="From @pezzelle2021word. Correlation between model and human similarity ratings on WordSim353, SimLex999 and MEN. Each barplot reports results on both the whole benchmark and the most concrete subset of it." width="100%" />
<p class="caption">
FIGURE 3.18: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="pezzelle2021word"><strong>???</strong></span>)</span>. Correlation between model and human similarity ratings on WordSim353, SimLex999 and MEN. Each barplot reports results on both the whole benchmark and the most concrete subset of it.
</p>
</div>
<p>From this <em>intrinsic</em> evaluation perspective (which was popular in the pre-Transformers Era), vision and language models do not generally outperform domain-specific models such as BERT and also in this case the only real competitor of pure language models is a model with visual supervision (again, Vokenization).<br />
The bar plots depict correlation between human- and model-based similarity ratings, differentiating between the most <em>concrete</em> concepts contained in a certain dataset<a href="#fn5" class="footnote-ref" id="fnref5"><sup>5</sup></a> and the whole dataset (thus including more abstract concepts). The results confirm the trend: multimodal models are more effective than pure language models at representing concrete words but in many instances they still lag behind when it comes to more abstract concepts.
<br></p>
<p>Last but not least, few words need to be spent on a topic which has been steadily gaining relevance: <strong>Few-Shot Learning</strong>. To train and test models, a large pool of paired images and texts is often needed and the creation of many of the datasets used in fine-tuning required a huge data collection effort, which had to be performed by human agents. This implies that the creation of such data pools can be very costly. For this reason, there is a growing interest in creating models able to cope with low-resource settings. This boils down to the question: can a model perform well on downstream tasks even with just a <em>limited number</em> of training examples? The goal is actually once again, to mimic how humans learn: a person does not need to see one thousand pictures of a table, to be able to recognize a table…</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:img-lu2022-02"></span>
<img src="figures/02-03-img-support-text/img-lu2022-02.png" alt="From @lu2022imagination. Model-agnostic improvement in Few-shot Setting with GLUE benchmark." width="100%" />
<p class="caption">
FIGURE 3.19: From <span class="citation">(<span class="citeproc-not-found" data-reference-id="lu2022imagination"><strong>???</strong></span>)</span>. Model-agnostic improvement in Few-shot Setting with GLUE benchmark.
</p>
</div>
<p>This table from <span class="citation">(<span class="citeproc-not-found" data-reference-id="lu2022imagination"><strong>???</strong></span>)</span>, where models are trained using only up to 5% of the training set, shows for example the ability for a model supervised with “imagination” (which was a generated visual representation of a certain textual input) to outperform models with only simple visual supervision (the Voken-model). This is just an example, but the ability to perform well in <em>few-shot</em> settings has become the touchstone of the evaluation modern multimodal models.</p>
</div>
</div>
<div id="the-end-of-this-story" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.7</span> The End Of This Story<a href="multimodal-architectures.html#the-end-of-this-story" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We started this story with the <em>Symbol Grounding Problem</em>, which affirms that to grasp the meaning of a word, the word has to be put in a context other than the pure linguistic one. We thus investigated some of the architectures proposed to ground words in a visual space in form of static images. The goal (hope) is to better capture the semantics of words, in form of better word embeddings, to be employed in heterogeneous tasks, from <em>semantic-similarity</em> to downstream tasks, such as <em>sentiment analysis</em>.<br />
From this brief analysis it emerges that grounding words in images can actually improve the representation of <em>concrete</em> concepts, whereas visual grounding does not seem to add value to pure language models when it comes to <em>abstract</em> concepts. Nonetheless, forms of visual supervision like the <em>Voken-Classification</em> task or the employment of generative models which allow to <em>imagine</em> words, such as in the <em>iACE-Framework</em>, might be the right way to bridge this gap.<br />
The Transformers have been a revolution in the field of NLP and with their advent, the trend has now become to build models with pre-training tasks capable of generating powerful task-agnostic word representations. The knowledge gained with these tasks can be then transferred to downstream tasks with the goal to limit the amount of labeled data necessary to fine-tune models. Labeling data is indeed costly: this is why the ability of a model to generalize well when exposed to just few training examples has been steadily gaining importance as evaluation metric. This was the so called <em>few-shot learning</em>. Moreover, Transformer-based models have “universal vocation”: they tend to be multimodal and multi-task, encompassing vision, language and vision and language tasks. This idea might be appealing because humans learn by being exposed to a multitude of different inputs and tasks. But as we have seen, pure language models such as BERT tend to still outperform multimodal multi-task models. There is definitely room for improvement.<br />
One might wonder whether the grounding of words in images is the right way to seek a better representation of words. Well, humans learn using all five senses and maybe the answer might be to incorporate in the models more heterogeneous perceptual information: not only static images but also videos, speech and the like. The debate is still open: the story <em>goes on</em>…
<br></p>
<p>Last but not least, a mention needs to be made on concrete applications of these image-empowered word-embeddings. The use of images to support linguistic models has been experimented in several fields, from <em>Dialogue Response Generation</em> (e.g. <span class="citation">(<span class="citeproc-not-found" data-reference-id="sun2021multimodal"><strong>???</strong></span>)</span>) to <em>Machine Translation</em>, where for example <span class="citation">(<span class="citeproc-not-found" data-reference-id="ive2019distilling"><strong>???</strong></span>)</span> found images to improve the quality of translation when the textual context is generic and/or ambiguous. The number of potential applications of the models described in this subchapter is growing steadily in the scientific community. But this is yet <em>another</em> story…</p>
</div>
<div id="appendix-selected-models---summary" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.8</span> Appendix: Selected Models - Summary<a href="multimodal-architectures.html#appendix-selected-models---summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The following table contains a summary of selected language models augmented with visual components. For each model, the following information are reported:</p>
<ul>
<li><p>Pure language model and pretraining data</p></li>
<li><p>Visual features and pretraining data</p></li>
<li><p>Fusion strategy of the two modalities</p></li>
<li><p>Benchmarks/baselines for evaluation:</p>
<p><span class="math inline">\(\color{green}\blacktriangle\)</span> better performance over baseline(s)<br />
<span class="math inline">\(\color{orange}\bullet\)</span> mixed performance results over baseline(s)<br />
<span class="math inline">\(\color{red}\blacktriangledown\)</span> worse performance over baseline(s)<br />
</p></li>
</ul>
<p>The table is available in a more readable format <a href="Table-ch2-3-final.pdf">here</a>.</p>
<table>
<colgroup>
<col width="0%" />
<col width="8%" />
<col width="5%" />
<col width="2%" />
<col width="9%" />
<col width="6%" />
<col width="34%" />
<col width="10%" />
<col width="6%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Year</th>
<th>Paper</th>
<th>Language model (LM)</th>
<th>LM-Pre-training sources</th>
<th>Visual elements (IMG)</th>
<th>IMG-Pre-training sources</th>
<th>Multimodal representation and model description</th>
<th>Testset/Fine-tuning</th>
<th>Baseline(s)/model settings/comparison to other models</th>
<th>Results</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>2014</td>
<td>Bruni, Elia, Nam-Khanh Tran, and Marco Baroni. “Multimodal distributional semantics.” Journal of artificial intelligence research 49 (2014): 1-47.</td>
<td>Distributional model expressed as a matrix with rows as “semantic vectors” representing the meaning of a set of target words. The model is based on co-occurrence counts of words (as a result, the matrix is a squared one).</td>
<td>- ukWaC, 1.9B tokens<br>- Wackypedia, 820M tokens.</td>
<td>(i) Local descriptors to extract low-level visual features<br>(ii) Assign local descriptors to cluster of visual words (bag of words) to build the vector representation of an image<br>(iii) Sum up visual words co-occurrence to across all images/instances to get co-occurrence counts related to a target word (the resulting matrix is a squared one).</td>
<td>ESP-Game dataset, 100K images.</td>
<td>Only words for which there is a related image are considered.<br>Two steps to build multimodal representations:<br>(i) Textual and visual matrices are concatenated and projected into a common latent multimodal space with a singular value decomposition. From this matrix, the “textual mixed matrix” and the “visual mixed matrix” are extracted<br>(ii) Association between words is assessed with cosine similarity<br><br>Two fusion methods to estimate similarity of pairs:<br>- Feature level fusion: linear combination of textual and visual mixed matrix and then similarity estimation<br>- Scoring level fusion: word similarity computed on both textual and visual mixed matrices separately and then the final score is a linear combination of the two<br><br>In both methods the weights in the linear combinations are hyperparameter.</td>
<td>- WordSim353<br>- MEN.</td>
<td>- Text mixed embeddings only<br>- Visual mixed embeddings only<br>- Equally weighted versions of feature and scoring level fusion model settings<br>- Several “fine tuned” versions of fusion and scoring level fusion model settings.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Multimodal word representations enhance performance of purely textual or visual embeddings<br><span class="math inline">\(\color{orange}\bullet\)</span>No alternative model used as a means of comparison.</td>
</tr>
<tr class="even">
<td>2014</td>
<td>Hill, Felix, and Anna Korhonen. “Learning abstract concept embeddings from multi-modal data: Since you probably can’t see what I mean.” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2014.</td>
<td>Skipgram.</td>
<td>400m word Text8 Corpus.</td>
<td>Mapping of words w to a bag of perceptual features b(w), extracted from external sources and encoded in an associative array P. Generation of pseudo sentences based on these perceptual features to be fed into the language model.</td>
<td>- ESP-Game (100K images)<br>- CSLB Property Norms.</td>
<td> Extension of the Skipgram injecting perceptual information by generating pseudo-sentences based on a bag-of-visual-words. A hyperparameter <span class="math inline">\(\alpha\)</span> controls the level of perceptual information relative to linguistic input.</td>
<td>USF Dataset.</td>
<td>- Concatenation of linguistic and perceptual features<br>- Canonical Correlation Analysis applied on vectors of both modalities<br>- SVD of matrix of concatenated multimodal representations.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Concepts, which can directly be represented in the perceptual modality (e.g. concrete verbs and nouns)<br><span class="math inline">\(\color{green}\blacktriangle\)</span>Propagation of perceptual input from concrete concepts (nouns and verbs) to enhance the representation of abstract verbs, those for which no direct representation in the visual space is available<br><span class="math inline">\(\color{red}\blacktriangledown\)</span>Abstract nouns (for which is more difficult to find a concrete visual representation) are still more efficiently learned from language-only models.</td>
</tr>
<tr class="odd">
<td>2014</td>
<td>Douwe Kiela and Léon Bottou. 2014. Learning Image Embeddings using Convolutional Neural Networks for Improved Multi-Modal Semantics. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 36–45, Doha, Qatar. Association for Computational Linguistics.</td>
<td>Skipgram.</td>
<td>- Text8 Corpus (400M words)<br>- British National Corpus (100M words).</td>
<td>Seventh layer of a CNN to extract 6144-d features vectors for images, obtained in two ways:<br>- CNN-Mean (average of all features vectors representing images)<br>- CNN-Max (component-wise maximum of all features vectors)</td>
<td>- ImageNet (12.5M images)<br>- Esp-Game (100K images).</td>
<td>Concatenation of visual and textual embeddings.</td>
<td>- MEN<br>- WordSim353 (it captures not only “relatedness” but also “similarity”.</td>
<td>- Skipgram (text-only baseline)<br>- Embeddings - visual only.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>CNN-Mean better on MEN: averaging might capture relatedness better. CNN-Max better on WordSim353.</td>
</tr>
<tr class="even">
<td>2014</td>
<td>Silberer, Carina, and Mirella Lapata. “Learning grounded meaning representations with autoencoders.” Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2014.</td>
<td>Vectors of textual attributes are extracted.</td>
<td>- McRae et al.’s (2005).</td>
<td>Vectors of visual attributes are extracted.</td>
<td>Same dataset as in Silberer et al. (2013): taxonomy of 636 visual<br>attributes (e.g., has wings, made of wood) and<br>nearly 700K images from ImageNet (Deng et al.,<br>2009) describing more than 500 of McRae et al.’s<br>(2005) nouns.</td>
<td>Stacked (denoising) autoencoders for each single modality and the outputs are concatenated and fed to a stacked bimodal autoencoder which map the inputs to a joint hidden layer.</td>
<td>With McRae et al.’s (2005), two tasks:<br>- word similarity<br>- word categorization.</td>
<td>- Unimodal autoencoders only<br>- Kernelized Canonical Correlation, Hardoon et al. (2004)<br>- Bruni et al. (2014).</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Bimodal models outperform unimodal ones<br><span class="math inline">\(\color{red}\blacktriangledown\)</span>Training is on attribute-based inputs. Not widely used in the field.</td>
</tr>
<tr class="odd">
<td>2015</td>
<td>Lazaridou, Angeliki, Nghia The Pham, and Marco Baroni. “Combining language and vision with a multimodal skip-gram model.” arXiv preprint arXiv:1501.02598 (2015).</td>
<td>Skipgram.</td>
<td>Wikipedia 2009, 800M Tokens.</td>
<td>Visual information for 5100 words with an entry in ImageNet, occur &gt;500 times in the text corpus and have a  concreteness score <span class="math inline">\(\geq\)</span> 0.5; sample 100 images for each word and extract a 4096-d array with a CNN; average the vectors of 100 pictures associated to each word to get visual representation.</td>
<td>ImageNet.</td>
<td>The objective function is a linear composition of the language objective L-ling from the Skipgram and a visual objective L-vision.<br>For the L-vision objective two variants are proposed:<br>- MM Skipgram A (MMSA): aligning vectors of visual and linguistic representations (1:1 correspondence assumed)<br>- MM Skipgram B (MMSB): estimate a cross-modal mapping matrix from linguistic onto visual representations.</td>
<td>- MEN<br>- SemSim<br>- VisSim.</td>
<td>- Kiela and Bottou (2014)<br>- Bruni et al. (2014)<br>- Silberer &amp; Lapata (2014)<br>- Skipgram (text-only baseline)<br>- Embeddings - visual only<br>- Concatenation<br>- SVD.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Both MMSA and MMSB better than simpler models (linguistic/vision only, concatenation SVD)<br><span class="math inline">\(\color{orange}\bullet\)</span>MMSA and MMSB competitive in relatedness and visual similarity, despite having often less training data than other models<br><span class="math inline">\(\color{orange}\bullet\)</span>Visual grounding less effective with abstract words</td>
</tr>
<tr class="even">
<td>2017</td>
<td>Collell, Guillem, Ted Zhang, and Marie-Francine Moens. “Imagined visual representations as multimodal embeddings.” Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 31. No. 1. 2017.</td>
<td>300-d GloVe.</td>
<td>Common Crawl corpus, 840B tokens, 2.2M words.</td>
<td>To extract visual features, the last hidden layer of  a CNN is taken. For each concept, two different ways to combine the extracted visual features:<br>- Averaging (averaging of al features vectors)<br>- Maxpooling (component-wise maximum).</td>
<td>Imagenet.</td>
<td>Mapping from language to vision. No need of 1:1 correspondence between linguistic and visual inputs. Two different mappings are considered:<br>- Linear (MAP-Clin)<br>- Neural Network (MAP-Cnn).</td>
<td>- MEN<br>- WordSim353<br>- SemSim<br>- Simlex999<br>-SimVerb3500<br>- VisSim.</td>
<td>- Kiela and Bottou (2014)<br>- Lazaridou et al. (2015)<br>- Silberer &amp; Lapata (2014)<br>- GloVe (text-only baseline)<br>- Concatenation.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Outperformance in all instances where words have associated images in the training set<br><span class="math inline">\(\color{red}\blacktriangledown\)</span>Performance on the zero-shot learning still inferior in many instances to the textual baselines.</td>
</tr>
<tr class="odd">
<td>2018</td>
<td>Kiela, Douwe, et al. “Learning visually grounded sentence representations.” arXiv preprint arXiv:1707.06320 (2017).</td>
<td>- GloVe for word embeddings<br>- Bidirectional LSTM for sentence representation.</td>
<td>WebCrawl</td>
<td>Image features obtained from the final layer of a ResNet-101.</td>
<td>MS COCO.</td>
<td>Word embeddings are projected to a ground space with a linear mapping. Linear mapping and Bi-LSTM are trained jointly.<br>Three methods to ground sentences in images, captions or both:<br>- Cap2Img: predict latent features of an image from its caption by mapping the (final) hidden state h(T) of the Bi-LSTM to the latent representation of the image. A ranking loss is to be minimized<br>- Cap2Cap: given the caption pair (x,y) describing the same image, the goal is to maximize the joint probability of y given x. Negative log-likelihood as loss.<br>- Cap2Both: Goal is to minimize the two loss functions above.<br><br>In another setting, grounded and sentence-only (Skipthought) representations are concatenated with layer normalization  to get the final sentence representations. Goal is to include information on less concrete concepts which are not likely to be represented in image-captioning databases but are present in language corpora.</td>
<td>Intrinsic evaluation of word embeddings:<br>- MEN<br>- SimLex 999<br>- Rare Words<br>- WordSim-353<br><br>Extrinsic evaluations:<br>- Movie review Sentiment (MR)<br>- Product reviews (CR)<br>- Subjectivity classification (SUBJ)<br>- Opinion polarity (MPQA)<br>- Paraphrase identification (MSRP)<br>- Sentiment classification (SST)<br>- SNLI (Entailment)<br>- SICK (Entailment).</td>
<td>- Skipthought (text-only baseline).</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Word embeddings are of higher quality than those obtained with GloVe, measured on the following similarity benchmarks: MEN, SimLex999, Rare Words and WordSim-353<br><span class="math inline">\(\color{green}\blacktriangle\)</span>In extrinsic evaluations, grounding increases performance but it is not clear which one of the three grounding strategies considered is dominant<br><span class="math inline">\(\color{orange}\bullet\)</span>Performance seems to be driven in a smaller amount of instances by a larger number of parameters rather than effectiveness of grounding<br><span class="math inline">\(\color{orange}\bullet\)</span>Performance is better when dataset have a higher level of concreteness.</td>
</tr>
<tr class="even">
<td>2020</td>
<td>Bordes, Patrick, et al. “Incorporating visual semantics into sentence representations within a grounded space.” arXiv preprint arXiv:2002.02734 (2020).</td>
<td>- Skipthought.</td>
<td>Toronto Book Corpus: 11M books, 74M ordered sentences, 13 words per sentence on average.</td>
<td>Processing of visual elements with a pre-trained Inception v3 network (Szegedy et al., 2016).</td>
<td>MS COCO: 118K/5K/41K (train/val/test) images.</td>
<td>The objective function is composed of:<br>- a textual objective Lt<br>- a grounding objective Lg, which among its parameters has also those of the textual objective, which in turn profit from both objective functions.<br><br>Lg is not applied directly on the sentence embeddings; it is trained on an intermediate space called the “grounded space”. The sentence embeddings are projected to the grounded space with the projection function being a multi-layer perceptron. The goal is to move away from the 1:1 correspondence between textual and visual space.<br>Lg the  can be decomposed in two components, whose individual contribution is controlled by two hyperparameters:<br>- Cluster Information (Cg): sentences associated with the same image(s) should be similar. The visual space is thus used to asses sentence similarity. The  Max-margin ranking loss is used<br>- Perceptual information (Pg): similarity between sentences in the grounded space  should be correlated with similarity between corresponding images in the visual space. The loss is based on the negative Pearson correlation.<br><br>Model scenarios include many compositions of the above mentioned elements.</td>
<td>Intrinsic evaluation of word embeddings:<br>- STS<br>- SICK<br><br>Extrinsic evaluations:<br>- Movie review Sentiment (MR)<br>- Product reviews (CR)<br>- Subjectivity classification (SUBJ)<br>- Opinion polarity (MPQA)<br>- Paraphrase identification (MSRP)<br>- Sentiment classification (SST)<br>- SNLI (Entailment)<br>- SICK (Entailment).</td>
<td>- Skipthought (text-only baseline)<br>For extrinsic evaluations:<br>- Kiros et al. (2014)<br>- Kiela et al. (2018)<br>- Lazaridou et al. (2015) - cross-modal<br>- Collell et al. (2017) - sequential/concatenation.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Word embeddings are  better than the textual benchmark for data with a high level of concreteness and are similar in performance with respect to more abstract concepts<br><span class="math inline">\(\color{green}\blacktriangle\)</span>Projections on the grounded space are more effective than cross-modal projection and concatenation<br><span class="math inline">\(\color{orange}\bullet\)</span>Not always best performance on entailment tasks (benchmarks SNLI, SICK).</td>
</tr>
<tr class="odd">
<td>2020</td>
<td>Tan, Hao, and Mohit Bansal. “Vokenization: Improving language understanding with contextualized, visual-grounded supervision.” arXiv preprint arXiv:2010.06775 (2020).</td>
<td>BERT, but it can be adapted to any language model (through Revokenization).</td>
<td>English Wikipedia.</td>
<td>ResNeXt.</td>
<td>MS COCO.</td>
<td>Language model with visual supervision. Each token in a sentence obtains a corresponding image (voken) assigned from a finite set of images. The voken is the image which maximize a Relevance Score Function between a token and all images in the aforementioned finite set of images. With this token-voken pairs a voken classification pre-training task is performed that can be built in pure language models alongside other pre-training tasks such MLM or Next-Sentence Prediction.</td>
<td>- GLUE (only SST-2, QNLI, QQP, MNLI)<br>- SQuAD<br>- SWAG.</td>
<td>- BERT (various versions)<br>- VilBert<br>- VL-BERT<br>- VisualBERT<br>- Oscar<br>- LXMERT.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span> Improvement over the purely self-supervised language model on multiple language tasks.</td>
</tr>
<tr class="even">
<td>2021</td>
<td>Hu, Ronghang, and Amanpreet Singh. “Unit: Multimodal multitask learning with a unified transformer.” Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.</td>
<td>BERT-base with a learned task-specific vector (to capture task-specific information) as additional input, which is positioned at the beginning of the embedded token sequence.</td>
<td>Pretrained version of the embeddings</td>
<td>CNN (ResNet-50) to extract visual features map + transformer encoder to encode the features map to a set of hidden states. A learned task-task specific vector (to capture task-specific information) is concatenated to the beginning of the visual feature list before entering the encoder (architecture inspired by DETR).</td>
<td>- MS COCO<br>- Visual Genome.</td>
<td>To both modalities is then applied a domain agnostic transformer architecture. As input the transformer takes the hidden states of either language or visual encoders or concatenation of both together with a task specific query embedding sequence. Self attention is applied in each layer among decoder hidden states and cross attention is applied to the encoded input modalities. The output is a set of decoded hidden states to which a task-specific head is applied (two-layer MPLP with GeLU activation and cross entropy loss).<br>Training is done jointly on multiple tasks. At each training iteration, a task is randomly selected.<br><br>Three settings (third one is the model described above):<br>1.) Single-task training: each model is trained separately on each task<br>2.) Multi-task training with separate decoders: a specific decoder for each task and jointly trained on all tasks<br>3.) Multi-task training with shared decoder. In this setting, there are still task-specific heads for each task.</td>
<td>Extrinsic evaluation.: GLUE:<br>- QNLI<br>- QQP<br>- MNLI<br>- SST2.</td>
<td>BERT (text-only baseline).</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Model setting (1), single task training, outperforms all other settings and is comparable to the text-only baseline<br><span class="math inline">\(\color{red}\blacktriangledown\)</span>Model setting (3), domain-agnostic, multi-task training with shared decoder across modalities exhibits a lower performance compared to domain-specific transformer models like BERT, the text-only baseline.</td>
</tr>
<tr class="odd">
<td>2021</td>
<td>Shahmohammadi, Hassan, Hendrik Lensch, and R. Harald Baayen. “Learning zero-shot multifaceted visually grounded word embeddings via multi-task training.” arXiv preprint arXiv:2104.07500 (2021).</td>
<td>- GloVe, 300d, 2.2M words<br>- fastText, 300d, 2M.</td>
<td>Pretrained version of the embeddings</td>
<td>Image vectors obtained by transferring the penultimate layer of pretrained Inception-V3 trained on ImageNet. A neural network with one hidden layer and tanh activation is used to  project the image vectors into the initial hidden state of the GRUs employed in the model.</td>
<td>MS COCO.</td>
<td>Given embeddings originating from a pretrained text-only model, the goal is to generate a mapping matrix M to ground word embeddings visually (the mapping matrix is used in both directions, to map text to grounded space and to map grounded embeddings back to the textual space)<br><br>This is obtained by performing three different tasks:<br>(i) Next word prediction with a GRU, given previous words in the sentence provided as image caption, together with the  related image embedding vector<br>(ii) Same as (i) but the sentence is provided backwards to another GRU<br>(iii) Binary classification task if the representation of a given sentence in the grounded space obtained from (i) and (ii) matched the associated image.</td>
<td>Limited to intrinsic evaluation:<br>- MEN<br>- SimLex999<br>- Rare Words<br>- MTurk771<br>- WordSim353<br>- SimVerb3500.</td>
<td>- GloVe (text-only baseline)<br>- fastText (text-only baseline)<br>- Collell et al. (2017)<br>- Park &amp; Myaeng (2017)<br>- Kiros et al. (2018)<br>- Kiela et al. (2018).</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Textual baselines and related models are outperformed and the model seems to improve the textual vector space by aligning it with real-world relations from the images (similarity appears to be favoured by the model over relatedness)<br><span class="math inline">\(\color{green}\blacktriangle\)</span>Embeddings related to less concrete words exhibit good quality compared to baselines.</td>
</tr>
<tr class="even">
<td>2022</td>
<td>Hsu, Chan-Jan, Hung-yi Lee, and Yu Tsao. “XDBERT: Distilling Visual Information to BERT from Cross-Modal Systems to Improve Language Understanding.” arXiv preprint arXiv:2204.07316 (2022).</td>
<td>- BERT<br>- ELECTRA.</td>
<td>Wikipedia.</td>
<td>CLIP as image-text matching system: two components, a text encoder (CLIP-T) and an image encoder (CLIP-ViT).</td>
<td>not specified.</td>
<td>3 adaptive tasks:<br>- Joint Masked Language Modelling (MLM)<br>- Same Sentence Prediction (MATCH)<br>- CLIP Token Classification<br>After that, concatenation with cross-modal encoder is performed.</td>
<td>- GLUE<br>- SWAG<br>- READ.</td>
<td>- BERT<br>- ELECTRA.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Better performance than pure language models, in particular in smaller datasets, which suggests that visual inputs improve generalization when the amount of training data is limited.</td>
</tr>
<tr class="odd">
<td>2022</td>
<td>Lu, Yujie, et al. “Imagination-Augmented Natural Language Understanding.” arXiv preprint arXiv:2204.08535 (2022).</td>
<td>- BERT-base<br>- RoBERTa.</td>
<td>Wikipedia.</td>
<td>Same as in VOKENIZATION paper.</td>
<td>MS COCO.</td>
<td>The framework iACE is composed of two modules:<br>1.) Imagination generator G: for each text input, VQGAN generates an “imagined visual” and CLIP is used to test how the generated image corresponds to the text and encodes the text and the image in a cross-modal embedding space and the objective function Lgan is to minimize the distance between these two embeddings<br>2.) Imagination augmented cross-modal encoder. Specifically, CLIP is used, with the embeddings from the textual and visual encoder (fed with the visualized imaginations)  within CLIP are then “late fused”. The output is a set of “imagination-augmented” language representations.<br><br>Learning procedure:<br>1.) pre-training of a visually-supervised transformer following the Vokenization method<br>2.) Imagination-augmented fine tuning, composed of two losses to be minimized:<br>(i) L-imagine where the iACE framework tries to minimize the cross-entropy loss based on text embeddings and visually imagination embeddings, given a testsets and a number of classes to predict<br>(ii)L-lang, where the visually supervised transformer only relies on textual inputs<br><br>The “imagination-augmented” is a composition of (i) and (ii) and the relative contribution of each loss is controlled with an hyperparameter.</td>
<td>From GLUE and SWAG:<br>- SST-2<br>- QNLI<br>- QQP<br>- MultiNLI<br>- MRPC<br>- STS-B<br><br>Focus is on few-shots learning (considering from 0.1% to 5% of the training dataset).</td>
<td>- BERT (text-only baseline)<br>- RoBERTa (text-only baseline)<br>With and w/o Vokenization.</td>
<td><span class="math inline">\(\color{green}\blacktriangle\)</span>Better performance of iACE over visually supervised transformers (VOKEN) in all instances of few-shots learning. Imagination can help existing language models to perform better in a setting with small training set (which means “less human annotated data”).</td>
</tr>
</tbody>
</table>

</div>
</div>
<div id="text-supporting-computer-vision-models" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.3</span> Text supporting computer vision models<a href="multimodal-architectures.html#text-supporting-computer-vision-models" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: Max Schneider</em></p>
<p><em>Supervisor: Jann Goschenhofer</em></p>
<div id="intro" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.1</span> Intro<a href="multimodal-architectures.html#intro" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The text supported CV architectures presented in this chapter follow the spirit of <!-- ref to other chapter mentioning NLP progress -->.
This means, they stem from a line of research which takes a lot of inspiration from preceding advancements in NLP.
The aim is the incorporation of respective new findings into CV in order to improve the SOTA in this field, which has to main aspects:</p>
<ol style="list-style-type: decimal">
<li>Researchers try to translate architectural concepts firstly used in NLP to the CV scenario, e.g., the vision transformer <!-- ref -->.</li>
<li>They leverage the power of these NLP models as building blocks inside bigger models, where they are used as text encoders for models where natural language is used as a very compelling source of supervision.</li>
</ol>
<p>This chapter is dedicated mainly to the second.
It has subchapters on the recent and relevant CV models CLIP, ALIGN, Florence, … and discusses some of their core concepts related to natural language supervision. <!-- ref for all papers --><br />
For example, all the architectures employ some form of transformer-based language encoder <span class="citation">(<span class="citeproc-not-found" data-reference-id="vaswani2017attention"><strong>???</strong></span>)</span> and CLIP excels even more when using a vision transformer as its image encoder. <!-- ref vision transformer -->
They confirm that the potential, impressively demonstrated by models like GPT-3 <span class="citation">(<span class="citeproc-not-found" data-reference-id="brown2020language"><strong>???</strong></span>)</span>, of this relatively recent architecture type is relevant for CV.<br />
But language models like BERT <!-- ref BERT --> and GPT-3 have a big impact on another aspect of their field:
They become to serve as so called foundation models, future architectures use them as building blocks. <!-- ref to foundation model paper, ref to DALLE oder so -->
A trend which is observable for this new wave of CV models, too.
They show a large potential to be the CV counterparts to NLP foundation models. <!-- ref Florence paper, ref models using CLIP as image encoder --></p>
</div>
<div id="concepts" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.2</span> Concepts<a href="multimodal-architectures.html#concepts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="scaling-sample-size" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.3.2.1</span> Scaling sample size<a href="multimodal-architectures.html#scaling-sample-size" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- Reference to bitter pill to swallow: More compute beats complex architecture. -->
<p>Arguably the most important aspect of these models is scale. <!-- ref to bitter pill to swallow -->
Making use of their available resources and the aggressive parallelization capabilities of transformer architectures, sample sizes range from 400 million <span class="citation">(CLIP; <span class="citeproc-not-found" data-reference-id="radford2021learning"><strong>???</strong></span>)</span> over 900 million <span class="citation">(Florence; <span class="citeproc-not-found" data-reference-id="yuan2021florence"><strong>???</strong></span>)</span> to 1.8 billion <span class="citation">(ALIGN; <span class="citeproc-not-found" data-reference-id="jia2021scaling"><strong>???</strong></span>)</span>.
The datasets are obtained through web-scraping and, because of the use of natural language supervision, cost and labor intensive manual labeling is completely avoided.
But this readily available web-scale data comes with some drawbacks.
Because of its noisy nature, some form of pre-processing is needed, e.g., filtering for language, excluding graphic content and images with non-informative captions.
<!-- TODO: ALIGN doesn't filter this strongly --></p>
<ul>
<li>Social biases are reproduced.</li>
</ul>
<!-- TODO: Other chapters discussing web-scale data? -->
</div>
</div>
<div id="contrastive-loss" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.3</span> Contrastive loss<a href="multimodal-architectures.html#contrastive-loss" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The maximizing scale approach explains a lot of further design choices.
The so called contrastive loss turned out to be very suitable for that.
* Ref to CLIP inspiration from medical field with contrastive objective formula
* Pro: Efficient training <!-- citation e.g., ALIGN -->
* Pro: Out of box zero shot -&gt; can serve as <em>foundation model</em> <span class="citation">(<span class="citeproc-not-found" data-reference-id="bommasani2021opportunities"><strong>???</strong></span>)</span> <!-- ref to zero shot chapter -->
* Contra: No longer a generative model, e.g., no flexible caption generation
* Extra paper - but also in ALIGN?</p>
<div id="zero-shooting-and-foundation-models" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.3.3.1</span> Zero shooting and foundation models<a href="multimodal-architectures.html#zero-shooting-and-foundation-models" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>Zero shooting</em> is a paradigm coming from NLP research.
It means the previously fitted model is applied to a new, unseen dataset.
In a way each dataset can be seen as a different task and used to evaluate the models ability to perform it.
<!-- TODO: Name some datasets and their associated tasks. -->
This is done in order to avoid a bias in performance evaluation, where the model overfitted on the specific data-generating distribution.
<!-- TODO: Look up perfomance gain. -->
This is possible due to the flexible text encoding of CLIP.
<!-- TODO: Explain this better. -->
The model can readily function as a classifier by:</p>
<ol style="list-style-type: decimal">
<li>Encoding all class labels.</li>
<li>Predicting for an image, which encoded class label is most likely to come with it.</li>
</ol>
<p>But in order to enhance performance by a margin of %d percent the prompts are engineered further.
They embed the class labels in sentence, e.g., “Picture of a (word)”, which seemingly was necessary for the model to make full use of its learned parameters.</p>
</div>
<div id="connecting-image-representations-to-language" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.3.3.2</span> Connecting image representations to language<a href="multimodal-architectures.html#connecting-image-representations-to-language" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Semantic concepts</li>
<li>Learn a representation <em>and</em> connect it to language (-&gt; NLP)</li>
<li>Directly communicate visual concepts to the model like “picture” or “macro” or “drawing”</li>
</ul>
</div>
</div>
<div id="clip" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.4</span> CLIP<a href="multimodal-architectures.html#clip" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- Intro -->
<ul>
<li>Focus on <em>task learning</em> (datasets as proxies to tasks) instead of <em>representation learning</em></li>
<li>Contrastive, Language, Image, Pre-training</li>
</ul>
<div id="architecture" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.3.4.1</span> Architecture<a href="multimodal-architectures.html#architecture" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>Original transformer with modifications used for GPT family as a text encoder</li>
<li>ResNet or vision transformer as a image encoder.
<ul>
<li>Vision transformer: much less compute</li>
</ul></li>
<li>High parallelization capabilities (transformer)</li>
<li>Can CLIP be seen as a step closer to human-like AI?
<ul>
<li>No: performance drop from zero- to one-shot setting</li>
<li>No: contrastive objective?</li>
<li>Yes: visual representations connected to natural language</li>
</ul></li>
</ul>
</div>
</div>
<div id="align" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.5</span> ALIGN<a href="multimodal-architectures.html#align" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- Intro -->
<ul>
<li>Over one billion image alt-text pairs</li>
<li>Name comes from alignment of visual and language representations trough the beloved and known contrastive loss or, very intuitively, “A Large-scale ImaGe and Noisy-text embedding”</li>
<li>Dual encoder architecture</li>
<li>Image + text image retrieval (e.g., Image of Eiffel tower + “snow” -&gt; snowy Eiffel tower)</li>
<li>Key difference to CLIP: training data. ALIGN does not filter that strongly, “dataset doesn’t require expert knowledge to curate”</li>
</ul>
</div>
<div id="florence" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.6</span> Florence<a href="multimodal-architectures.html#florence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<!-- Intro -->
<!-- Object detection vs scene level -->
<ul>
<li>More fine-grained, dynamic, multimodal representations</li>
<li>Focus shift to finding <em>foundation model</em> as CLIP turned out to be especially useful for that.
<ul>
<li>Pre-trained core</li>
<li>Flexible addition of modules
<ul>
<li><em>Dynamic Head</em> for object detection - citations coming later</li>
<li><em>METER</em> as a adapter for vision-language (e.g., visual question answering)</li>
<li>Adaptation to video recognition through <em>CoSwin</em></li>
</ul></li>
</ul></li>
<li>General trend in this direction, better and better predictions <span class="citation">(CoCa; <span class="citeproc-not-found" data-reference-id="yu2022coca"><strong>???</strong></span>)</span></li>
<li>Optimization inside image-label-description space</li>
<li>Encoders
<ul>
<li>Uses CLIP pendant as the language encoder</li>
<li>Swin transformer as the image encoder</li>
<li>CoSwin for embedding</li>
</ul></li>
</ul>
<div id="architecture-1" class="section level4 hasAnchor">
<h4><span class="header-section-number">3.3.6.1</span> Architecture<a href="multimodal-architectures.html#architecture-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
</div>
<div id="performance-comparison" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.7</span> Performance comparison<a href="multimodal-architectures.html#performance-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li>As all of these models are orders of magnitudes too large for performing a benchmark, findings reported inside the papers are believed here
<!-- TODO: Other comparisons --></li>
</ul>
</div>
<div id="resources" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.8</span> Resources<a href="multimodal-architectures.html#resources" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One can find the pre-trained CLIP models on <a href="https://github.com/openai/CLIP">Github</a>.
They even found their way into simple command line tools already.
For example there is an application named <a href="https://github.com/yurijmikhalevich/rclip">rclip</a>, which can be used for personal image retrieval, wrapping the <em>ViT-B/32</em> CLIP architecture.
On my (mid-range) laptop I was able to find seemingly good matches for search terms tried out inside a folder with about 100 pictures.
After an initial caching one request took about ten seconds.
<!-- TODO: Look up if data if available --></p>
</div>
<div id="outlook" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.9</span> Outlook<a href="multimodal-architectures.html#outlook" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>CLIP as buildingblock
CLASP
LAION dataset
TODO: CLIP as module</p>

</div>
</div>
<div id="text-image" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.4</span> Text + Image<a href="multimodal-architectures.html#text-image" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Author: Steffen Jauch-Walser </em></p>
<p><em>Supervisor: Daniel </em></p>
<div id="todo" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.1</span> Todo<a href="multimodal-architectures.html#todo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>communicate with marco about perceiver and data2vec
communicate about who does attention how detailled</p>
</div>
<div id="challenges-in-ai" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.2</span> challenges in AI<a href="multimodal-architectures.html#challenges-in-ai" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There have been many advances made in machine learning over the past years. However, there are two caveats. One model follows the next in short sequence. The overabundance of different models makes it hard to keep track. More importantly, however, it is often unclear whether advances in a particular field, for example with a specific type of input data, will carry over to another setting. On top of that, any model that requires labelled data inherently suffers from capacity constraints. Typically, models are trained on a handful of well-known data sets which have been created with great effort. How would a perfect model look like? Ideally, we would want to find that one general model to rule them all, a model structure that works with different inputs, little oversight and readily adapts to new tasks, similar as the human brain.<br />
Although the human brain has been used as an inspiration for neural networks, mimicking brain structures is not the aim of machine learning nor should it be. Human learning is nevertheless useful in defining potential goals. There is more to machine learning than simply finding better predictions. Making models interpretable, making models independent of human capacity constraints, making models which work across different modalities and with potentially unknown inputs and creating model structures that are reusable as well as understandable are valuable aims, too.<br />
Nevertheless, the main challanges in machine learning currently evolved around data. The rise of transformer models (<span class="citation">(<span class="citeproc-not-found" data-reference-id="vaswani2017attention"><strong>???</strong></span>)</span>) highlights how impactful the computational power to handle more data can be. Being parrallizable, they outperform sequential neural networks not through complexity, but through the combination of simplicity and the capability to handle vast amounts of data.<br />
</p>
<p>### data2vec
In their paper, data2vec (<span class="citation">(<span class="citeproc-not-found" data-reference-id="baevski2022data2vec"><strong>???</strong></span>)</span>), data scientists at Meta, formerly facebook, developed an architecture that addresses some of those goals. Their algorithmic structure is able to work with either text, image or speech. On top of that, the model is self supervised with a teacher-student relationship which reduces the need for human labelling. It is not a universal model in the sense that it works with any input, nor is it even a general model in the sense that the algorithm is exactly the same for each modality. However, the overall model structure remains the same for either text, speech or image input data, while only the specific encoding, normalization and masking strategies are modality-specific. In that regard, it is a step towards a more general way of dealing with different modalities and it is very effective at doing so given the benchmark results on typical data sets.</p>
<p>— add benchmarks here?</p>
<p>In the following, we’ll take a closer look at the data2vec framework. According to the authors, the core idea of the framework is to “predict latent representations of the full input data based on a masked view of the input in a self-distillation set-up using a standard Transformer architecture” (citation).</p>
<p>— add paragraph about transformers here?</p>
<p>More specifically, the framework is self-supervised, i.e. its core building blocks are a student and a teacher model whereby the teacher only differs in that it uses weights which are “an exponentially decaying average of the student model”. The transformer architecture itself follows an off-the-shelf network proposed by Vaswani et all, 2017 (citation). The exact setup can been in the following picture:</p>
<p>— add picture</p>
<p>It is important to note that while the teach model is presented the full input data, the student model only obtains a masked, i.e a partial, view of the input data. Given that masked input, the task of the student model is to predict the latent representations created by the teach model. Specifically, the output of the top K blocks of the teacher model as highlighted in the graphic. It is notable that those latent representations are created from the complete input data and hence they are contextualized, which is not the case if you use visual tokens or pixels isolated to a current patch.</p>
<p>Diving deeper in to the model structure, the authors use the following loss function:</p>
<p>— L = either L1 regularized or L2 depending on a parameter beta
The advantage of that particular loss function is that it is less sensitive to outlier, but one has to finetune beta.</p>
<p>As far as the parameterization of the teacher model weights are concerned,
they are implemented as</p>
<p>— show equation</p>
<p>In essence, this means that the teacher model update more frequently at the start of the training process when the model is still random and slower towards the end when meaningful weights have been learned. Aside from that, the teacher and student model are identical. Parameters of the feature encoder and positional encoder are shared between both models.</p>
<p>As far as the targets are concerned, they are constructed based on the outcome of the top K blocks of the teacher model as mentioned above. Specifically, a normalization is applied to each block and then outcomes are averaged across K blocks. The authors mention that averaging turned out to be more efficient than predicting each block separately at similar prediction rates. Normalization is important to help prevent model collapse as well as the domination of certain layers. As mentioned before, the normalization step is one of the parts of the model that is modality specific. For speech representations, instanced normalization is used. For natural language processing (NLP) and computer vision (CV), parameterless layer-normalization is used.</p>
<p>— potentially explain more about normalization and variance-invariance-covariance normalization that was not used.</p>
<p>The other modality specific parts of the model are the encoding and the masking strategies.</p>
<p>Computer Vision:</p>
<ul>
<li><p>224x224 pixel as patches of 16x16 pixels</p></li>
<li><p>each patch linearly transformed and a sequence of 196 representations is input into</p></li>
<li><p>following BEit (Bao et al, 2021).</p></li>
<li><p>—show picture of paper and explanation</p></li>
<li><p>masking blocks of multiple adjacent patches where each block contains at least 16 patches o * with random aspect ratio</p></li>
<li><p>masking 60% of patches instead of 40%. apparently more accurate</p></li>
<li><p>pre-trained Vit-B and Vit-L for 800 epochs</p></li>
</ul>
<p>Speech:</p>
<ul>
<li>fairseq implementation (Ott et al, 2019)</li>
<li>16 kHz input</li>
<li>feature encoder containing several temporal convolutions with   512 channels, strides (5,2,2,2,2,2,2) and kernal widths (10,3,3,3,2,2)</li>
<li>as a result: 50Hz output with stride of 20ms between samples and receptive field of 400 input samples or 25ms of audio, raw waveform input to the encoder normalized to zero mean and unit variance</li>
<li>masking identical to (Baevski et al 2020b): samples p=0.065 of all time steps and mask the subsequent ten timesteps -&gt; approx 50% of timesteps masked</li>
</ul>
<p>NLP:</p>
<ul>
<li>input tokenized using byte pair encoding. 50k types</li>
<li>BERT masking strategy appliedto 15% uniformly selcted tokens</li>
<li>also considered, wave2vec strategy to mask a span of four tokens</li>
</ul>
<p>Other models:
* NLP Bert 
* Dino, Byol 
* HuBert  
* wave2vec
—-
* PeCo
* flamingo</p>
<p>How do they relate to data2vec? Create tables?</p>
<p>Findings:
CV:</p>
<ul>
<li>ImageNet 1K</li>
<li>top1 accuracy. data2vec outperforms Vit-L and Vit-B in single model setting.</li>
<li>accuracy similar to PeCo (multiple models setting)</li>
</ul>
<p>Speech Processing:</p>
<ul>
<li>Librispeech 960 (audiobooks in engl, clear speech)</li>
<li>improvements particularly in the section with shorter training (10min - 1h)</li>
</ul>
<p>NLP:</p>
<ul>
<li>Books Corpus and English Wikipedia data. GLUE benchmark</li>
<li>first successful pre-trained nlp model not sureing discrete units as training target</li>
<li>outperforms roberta baseline</li>
</ul>
<p>Generally, best accuracy at around 10-12 layers.
The model performs best when teacher is given full input.</p>
<p>What do the findings mean for the future of the field?
The authors succeed in designing a single learning mechanism for different modalities. As a caveat, they still use modality specific encoding and masking strategies, but input data is also quite different. Is it possible to go beyond that? One of the main advances of the framework is the use of contextualized training targets through the use of the teacher self-attention mechanism.</p>
</div>
<div id="vilbert" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.3</span> vilbert<a href="multimodal-architectures.html#vilbert" class="anchor-section" aria-label="Anchor link to header"></a></h3>
</div>
<div id="flamingo-alayrac2022flamingo" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.4</span> flamingo <span class="citation">(<span class="citeproc-not-found" data-reference-id="alayrac2022flamingo"><strong>???</strong></span>)</span><a href="multimodal-architectures.html#flamingo-alayrac2022flamingo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Not only obtaining labelled data, but also training time itself is prohibitively costly for many real world scenarios. It is incrediblly valuable
if a model needs little training time. In the quest for more general AI models, that also corrsponds to the adaptability to new tasks. While researchers have
used pre-trained models in conjunction with fine-tuning in order to adapt models to new tasks, that approach still requires substantial retraining. Another
approach is ‘few shot learning’. After pre-training a model, it has to adapt to a new task simply through being given a couple of prompting examples.
One such model is Deepmind’s Flamingo.<br />
-picture-<br />
Flamingo combines a vision model and large language model through a several achitectural advances. Rather than finetuning those models with a
combined 80 billion parameters, the initial models are frozen after pretraining and connected through a perceiver resampler component as well as gateed
cross attention layers. Both those components are trainable and during training transform the model from an initial large language model into a fully
functioning visual language model with great expressive capabilities. Freezing the models severly cuts down on the required amount of training and also
ensures that the models always retain their full capabilities.<br />
However, bridging pre-trained vision-only and language-only is not the only innovation in the flamingo architecture. The model can also handle arbitrary
sequences of interleaved text and images, scraped from the web. Based on data from 43 million websites, the researches create three different data sets
(interleaved data, text-image pairs and video-image pairs). They specifically avoid typical machine learning data sets and leverage the contextualization of
web data, similar to data2vec.<br />
Formally, Flamingo models the probablity of text y interleaved with a sequence of videos or images.<br />
equation<br />
The perceiver resampler connects the vision encoder and the language model. Cleverly, it resamples a variable size of input tokens into a fixed amount of visual outputs. This resampling significantly reduces computational complexity, especially of the vision-text cross attention. As learnable component, it contains a predefined number of latent queries. The number of output tokens is equal to the number of learned queries.</p>
<p><br />
The other important trainable compontent are gated cross attention layers. They can be inserted at variable depths into the frozen language model and define the complexity of the final model. They attend the visual inputs with a specific masking system.
The gating mechanism ensures that the first pass through the model corresponds to the original model. The amount of cross attention layers also lets
the researcher choose the ratio between old (frozen) and new paramters.</p>

</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>See for example <span class="citation">(<span class="citeproc-not-found" data-reference-id="bosch2007image"><strong>???</strong></span>)</span> for more details on this technique, called “bag-of-visual-words”.<a href="multimodal-architectures.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>It may be argued that this point is a necessity to be able to work on sequences rather than a strength.<a href="multimodal-architectures.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>From an operative point of view, the authors consider a token type “visually grounded” if it has more than 100 occurrences in MS COCO<a href="multimodal-architectures.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>The proof is straightforward. Let <span class="math inline">\(X\in \mathbb{R}^l\)</span> and have euclidean norm equal to 1, which means <span class="math inline">\(||X||_{2}=1\)</span>. In the nearest neighbor search we need to find the vector <span class="math inline">\(Y\in \mathbb{R}^l\)</span>, also with norm equal to 1, which has minimal euclidean distance with <span class="math inline">\(X\)</span>. This is the quantity to be minimized:
<span class="math display">\[\begin{align*}
d(X,Y) &amp;=\sqrt{\sum_{i=1}^{l}{(x_i-y_i)^2}}
  \\&amp;\stackrel{squared}{=} \sum_{i=1}^{l}{x_i^2}+\sum_{i=1}^{l}{y_i^2}-2\sum_{i=1}^{l}{x_iy_i}
  \\&amp;\stackrel{}{=}||X||_{2}^2+||Y||_2^2-2X^TY
  \\&amp;\stackrel{Norm-1}{=}1+1-2X^TY
  \\&amp;\stackrel{}{=}2(1-X^TY)
\end{align*}\]</span>
And through these simple algebraic manipulations, it is possible to see that minimizing the euclidean distance between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is equivalent to maximize <span class="math inline">\(X^TY\)</span>, which is the inner product. This proves the equivalence between inner product maximization and nearest neighbor search.<a href="multimodal-architectures.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>See <span class="citation">(<span class="citeproc-not-found" data-reference-id="brysbaert2014concreteness"><strong>???</strong></span>)</span> for information on how <em>concreteness</em> of a word can be estimated.<a href="multimodal-architectures.html#fnref5" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introducing-the-modalities.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="further-topics.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
