<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 1 Introduction | Multimodal Deep Learning</title>
  <meta name="description" content="." />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 1 Introduction | Multimodal Deep Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 1 Introduction | Multimodal Deep Learning" />
  
  <meta name="twitter:description" content="." />
  

<meta name="author" content="" />


<meta name="date" content="2022-11-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="foreword.html"/>
<link rel="next" href="c01-00-intro-modalities.html"/>
<script src="libs/jquery/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook/css/style.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block/empty-anchor.js"></script>
<link href="libs/anchor-sections/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections/anchor-sections.js"></script>




<link rel="stylesheet" href="css/style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">Multimodal Deep Learning</li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html"><i class="fa fa-check"></i>Foreword</a><ul>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#citation"><i class="fa fa-check"></i>Citation</a></li>
<li class="chapter" data-level="" data-path="foreword.html"><a href="foreword.html#technical-setup"><i class="fa fa-check"></i>Technical Setup</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#introduction-to-multimodal-deep-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Multimodal Deep Learning</a></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#outline-of-the-booklet"><i class="fa fa-check"></i><b>1.2</b> Outline of the Booklet</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html"><i class="fa fa-check"></i><b>2</b> Introducing the modalities</a><ul>
<li class="chapter" data-level="2.1" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-01-sota-nlp"><i class="fa fa-check"></i><b>2.1</b> State-of-the-art in NLP</a></li>
<li class="chapter" data-level="2.2" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-02-sota-cv"><i class="fa fa-check"></i><b>2.2</b> State-of-the-art in Computer Vision</a></li>
<li class="chapter" data-level="2.3" data-path="c01-00-intro-modalities.html"><a href="c01-00-intro-modalities.html#c01-03-benchmarks"><i class="fa fa-check"></i><b>2.3</b> Resources and Benchmarks for NLP, CV and multimodal tasks</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html"><i class="fa fa-check"></i><b>3</b> Multimodal architectures</a><ul>
<li class="chapter" data-level="3.1" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-01-img2text"><i class="fa fa-check"></i><b>3.1</b> Image2Text</a></li>
<li class="chapter" data-level="3.2" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-02-text2img"><i class="fa fa-check"></i><b>3.2</b> Text-to-image</a></li>
<li class="chapter" data-level="3.3" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-03-img-support-text"><i class="fa fa-check"></i><b>3.3</b> Images supporting Language Models</a></li>
<li class="chapter" data-level="3.4" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-04-text-support-img"><i class="fa fa-check"></i><b>3.4</b> Text supporting Computer Vision Models</a></li>
<li class="chapter" data-level="3.5" data-path="c02-00-multimodal.html"><a href="c02-00-multimodal.html#c02-05-text-plus-img"><i class="fa fa-check"></i><b>3.5</b> Models for both modalities</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="c03-00-further.html"><a href="c03-00-further.html"><i class="fa fa-check"></i><b>4</b> Further Topics</a><ul>
<li class="chapter" data-level="4.1" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-01-further-modalities"><i class="fa fa-check"></i><b>4.1</b> Including Further Modalities</a></li>
<li class="chapter" data-level="4.2" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-02-structured-unstructured"><i class="fa fa-check"></i><b>4.2</b> Structured + Unstructured Data</a></li>
<li class="chapter" data-level="4.3" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-03-multi-purpose"><i class="fa fa-check"></i><b>4.3</b> Multipurpose Models</a></li>
<li class="chapter" data-level="4.4" data-path="c03-00-further.html"><a href="c03-00-further.html#c03-04-usecase"><i class="fa fa-check"></i><b>4.4</b> Generative Art</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="conclusion-1.html"><a href="conclusion-1.html"><i class="fa fa-check"></i><b>5</b> Conclusion</a></li>
<li class="chapter" data-level="6" data-path="epilogue.html"><a href="epilogue.html"><i class="fa fa-check"></i><b>6</b> Epilogue</a><ul>
<li class="chapter" data-level="6.1" data-path="epilogue.html"><a href="epilogue.html#new-influential-architectures"><i class="fa fa-check"></i><b>6.1</b> New influential architectures</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="acknowledgements.html"><a href="acknowledgements.html"><i class="fa fa-check"></i><b>7</b> Acknowledgements</a></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://bookdown.org" target="_blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Multimodal Deep Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 1</span> Introduction<a href="introduction.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><em>Author: Nadja Sauter</em></p>
<p><em>Supervisor: Matthias Assenmacher</em></p>
<div id="introduction-to-multimodal-deep-learning" class="section level2 hasAnchor">
<h2><span class="header-section-number">1.1</span> Introduction to Multimodal Deep Learning<a href="introduction.html#introduction-to-multimodal-deep-learning" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>There are five basic human senses: hearing, touch, smell, taste and sight. Processing these five modalities, we are able to perceive and understand the world around us. Thus, multimodal means to combine different channels of information simultaneously to understand our surroundings. For example, when toddlers learn the word “cat”, they use different modalities by saying the word out loud, pointing on cats and making sounds like “meow”. Using the human learning process as a role model, AI researchers also try to combine different modalities to train deep learning models. On a superficial level, deep learning algorithms are based on a neural network that is trained to optimize some objective which is mathematically defined via the so-called loss function. The optimization of minimizing the loss is done by a mathematical operation called gradient descent. Consequently, deep learning models can only handle numeric input and can only result in a numeric output. However, in multimodal tasks we are often confronted with unstructured data like pictures or text. Thus, the first major problem is how to represent the input numerically. The second issue with regard to multimodal tasks is how exactly to combine different modalities. For instance, a typical task could be to train a deep learning model to generate a picture of a cat. First of all, the computer needs to understand the text input “cat” and then somehow translate this information into a specific image. Therefore, it is necessary to identify the contextual relationships between words in the text input and the spatial pixel relationships in the image output. What might be easy for a toddler in pre-school, is a huge challenge for the computer. Both have to learn some understanding of the word “cat” that comprises the meaning and appearance of the animal. A common approach in artificial intelligence (AI) is to generate a high-level embedding that represents the cat numerically as a vector in some latent space. However, to achieve this, different approaches and algorithmic architectures have been developed in recent years. This book gives an overview of the different methods used in state-of-the-art multimodal deep learning to overcome challenges arising with unstructured data and combining inputs of different modalities.</p>
</div>
<div id="outline-of-the-booklet" class="section level2 hasAnchor">
<h2><span class="header-section-number">1.2</span> Outline of the Booklet<a href="introduction.html#outline-of-the-booklet" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Since multimodal models often use text and images as input or output, methods of Natural Language Processing (NLP) and Computer Vision (CV) are introduced as foundation in Chapter <a href="c01-00-intro-modalities.html#c01-00-intro-modalities">2</a>. Methods in the area of NLP try to handle text data, whereas CV deals with image processing. With regard to NLP (subsection <a href="c01-00-intro-modalities.html#c01-01-sota-nlp">2.1</a>), one concept of major importance is the so-called word embedding, which is nowadays an essential part of all mutlimodal deep learning architectures. This concept also sets the foundation for transformer-based models like BERT <span class="citation">(Devlin et al. <a href="#ref-BERT" role="doc-biblioref">2018</a><a href="#ref-BERT" role="doc-biblioref">a</a>)</span>, which achieved a huge improvement in several NLP tasks. Especially the attention mechanism <span class="citation">(Vaswani, Shazeer, Parmar, Uszkoreit, Jones, Gomez, Kaiser, et al. <a href="#ref-attention" role="doc-biblioref">2017</a><a href="#ref-attention" role="doc-biblioref">a</a>)</span> of transformers revolutionized NLP models which is why most of them rely on the transformer as a backbone. In Computer Vision (subsection <a href="c01-00-intro-modalities.html#c01-02-sota-cv">2.2</a>) different network architectures, namely ResNet <span class="citation">(He et al. <a href="#ref-ResNet" role="doc-biblioref">2015</a>)</span>, EfficientNet <span class="citation">(M. Tan and Le <a href="#ref-EfficientNet" role="doc-biblioref">2019</a><a href="#ref-EfficientNet" role="doc-biblioref">a</a>)</span>, SimCLR <span class="citation">(T. Chen, Kornblith, Norouzi, and Hinton <a href="#ref-SimCLR" role="doc-biblioref">2020</a><a href="#ref-SimCLR" role="doc-biblioref">a</a>)</span> and BYOL <span class="citation">(Grill, Strub, Altché, et al. <a href="#ref-BYOL" role="doc-biblioref">2020</a>)</span>, will be introduced. Here it is of great interest to compare the different approaches in CV and their performance on challenging benchmarks. For this reason, the last subsection <a href="c01-00-intro-modalities.html#c01-03-benchmarks">2.3</a> of chapter 1 gives an overall overview of different data sets, pre-training tasks and benchmarks for CV as well as for NLP.</p>
<p>The second Chapter (see <a href="c02-00-multimodal.html#c02-00-multimodal">3</a>) focuses on different multimodal architectures, covering a wide variety of how text and images can be combined. The presented models combine and advance different methods of NLP and CV. First of all, looking at Img2Text tasks (subsection <a href="c02-00-multimodal.html#c02-01-img2text">3.1</a>), the data set Microsoft COCO for object recognition <span class="citation">(T.-Y. Lin, Maire, Belongie, Bourdev, et al. <a href="#ref-COCO" role="doc-biblioref">2014</a>)</span> and the meshed-memory transformer for Image Captioning (M<sup>2</sup> Transformer) <span class="citation">(Cornia et al. <a href="#ref-meshed_memory" role="doc-biblioref">2019</a>)</span> will be presented. Contrariwise, researchers developed methods to generate pictures based on a short text prompt (subsection <a href="c02-00-multimodal.html#c02-02-text2img">3.2</a>). The first models accomplishing this task were generative adversarial networks (GANs) <span class="citation">(Ian J. Goodfellow et al. <a href="#ref-GAN" role="doc-biblioref">2014</a><a href="#ref-GAN" role="doc-biblioref">a</a>)</span> and Variational Autoencoders (VAEs) <span class="citation">(Kingma and Welling <a href="#ref-VAE" role="doc-biblioref">2019</a>)</span>. These methods were improved in recent year and today state-of-the art transformer architectures with text-guided diffusion models like DALL-E <span class="citation">(Ramesh, Pavlov, et al. <a href="#ref-DALLE" role="doc-biblioref">2021</a><a href="#ref-DALLE" role="doc-biblioref">a</a>)</span> and GLIDE <span class="citation">(Nichol et al. <a href="#ref-GLIDE" role="doc-biblioref">2021</a><a href="#ref-GLIDE" role="doc-biblioref">a</a>)</span> achieve remarkable results. Another interesting question is how images can be utilized to support language models (subsection <a href="c02-00-multimodal.html#c02-03-img-support-text">3.3</a>). This can be done via sequential embeddings, more advanced grounded embeddings or, again, inside transformers. On the other hand, you can also look at text supporting CV models like CLIP <span class="citation">(Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, et al. <a href="#ref-CLIP" role="doc-biblioref">2021</a><a href="#ref-CLIP" role="doc-biblioref">a</a>)</span>, ALIGN <span class="citation">(C. Jia, Yang, Xia, Chen, et al. <a href="#ref-ALIGN" role="doc-biblioref">2021</a><a href="#ref-ALIGN" role="doc-biblioref">b</a>)</span> and Florence <span class="citation">(Yuan et al. <a href="#ref-yuan2021florence" role="doc-biblioref">2021</a>)</span> (subsection <a href="c02-00-multimodal.html#c02-04-text-support-img">3.4</a>). They use foundation models meaning reusing models (e.g. CLIP inside DALL-E 2) as well as a contrastive loss for connecting text with images. Besides, zero-shooting makes it possible to classify new and unseen data. Especially the open-source architecture CLIP <span class="citation">(Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger, et al. <a href="#ref-CLIP" role="doc-biblioref">2021</a><a href="#ref-CLIP" role="doc-biblioref">a</a>)</span> for image classification and generation attracted a lot of attention last year. In the end of the second chapter, some further architectures to handle text and images simultaneously are introduced (subsection <a href="c02-00-multimodal.html#c02-05-text-plus-img">3.5</a>). For instance, Data2Vec uses the same learning method for speech, vision and language and in this way aims to find a general approach to handle different modalities in the same way. Furthermore, VilBert <span class="citation">(J. Lu et al. <a href="#ref-VilBert" role="doc-biblioref">2019</a><a href="#ref-VilBert" role="doc-biblioref">a</a>)</span> extends the popular BERT architecture to handle both image and text as input by implementing co-attention. This method is also used in Google’s Deepmind Flamingo <span class="citation">(Alayrac, Donahue, Luc, Miech, Barr, Hasson, Lenc, Mensch, Millican, Reynolds, Ring, et al. <a href="#ref-Flamingo" role="doc-biblioref">2022</a>)</span>. In addition, Flamingo aims to tackle multiple tasks with a single visual language model via few-shot learning and freezing the pre-trained vision and language model.</p>
<p>In the last chapter (see <a href="c03-00-further.html#c03-00-further">4</a>), methods are introduced that are also able to handle modalities other than text and image, like e.g. video or speech. The overall goal here is to find a general multimodal architecture based on challenges rather than modalities. Therefore, one needs to handle problems of multimodal fusion and alignment and decide whether you use a join or coordinated representation (subsection <a href="c03-00-further.html#c03-01-further-modalities">4.1</a>). Moreover we go more into detail about how exactly to combine structured and unstructured data (subsection <a href="c03-00-further.html#c03-02-structured-unstructured">4.2</a>). Therefore, different fusion strategies which evolved in recent years will be presented. This is illustrated in this book by two use cases in survival analysis and economics. Besides this, another interesting research question is how to tackle different tasks in one so called multi-purpose model (subsection <a href="c03-00-further.html#c03-03-multi-purpose">4.3</a>) like it is intended to be done by Google researchers <span class="citation">Barham et al. (<a href="#ref-Pathways" role="doc-biblioref">2022</a>)</span> and their Pahtway model. Last but not least, we show one exemplary application of Multimodal Deep Learning in the arts scene where image generation models like DALL-E <span class="citation">(Ramesh, Pavlov, et al. <a href="#ref-DALLE" role="doc-biblioref">2021</a><a href="#ref-DALLE" role="doc-biblioref">a</a>)</span> are used to create art pieces in the area of Generative Arts (subsection <a href="c03-00-further.html#c03-04-usecase">4.4</a>).</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-Flamingo">
<p>Alayrac, Jean-Baptiste, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, et al. 2022. “Flamingo: A Visual Language Model for Few-Shot Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2204.14198">https://doi.org/10.48550/ARXIV.2204.14198</a>.</p>
</div>
<div id="ref-Pathways">
<p>Barham, Paul, Aakanksha Chowdhery, Jeff Dean, Sanjay Ghemawat, Steven Hand, Dan Hurt, Michael Isard, et al. 2022. “Pathways: Asynchronous Distributed Dataflow for Ml.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2203.12533">https://doi.org/10.48550/ARXIV.2203.12533</a>.</p>
</div>
<div id="ref-SimCLR">
<p>Chen, Ting, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020a. “A Simple Framework for Contrastive Learning of Visual Representations.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2002.05709">https://doi.org/10.48550/ARXIV.2002.05709</a>.</p>
</div>
<div id="ref-meshed_memory">
<p>Cornia, Marcella, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2019. “Meshed-Memory Transformer for Image Captioning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1912.08226">https://doi.org/10.48550/ARXIV.1912.08226</a>.</p>
</div>
<div id="ref-BERT">
<p>Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018a. “BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1810.04805">https://doi.org/10.48550/ARXIV.1810.04805</a>.</p>
</div>
<div id="ref-GAN">
<p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014a. “Generative Adversarial Networks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1406.2661">https://doi.org/10.48550/ARXIV.1406.2661</a>.</p>
</div>
<div id="ref-BYOL">
<p>Grill, Jean-Bastien, Florian Strub, Florent Altché, Corentin Tallec, Pierre H. Richemond, Elena Buchatskaya, Carl Doersch, et al. 2020. “Bootstrap Your Own Latent: A New Approach to Self-Supervised Learning.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2006.07733">https://doi.org/10.48550/ARXIV.2006.07733</a>.</p>
</div>
<div id="ref-ResNet">
<p>He, Kaiming, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2015. “Deep Residual Learning for Image Recognition.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1512.03385">http://arxiv.org/abs/1512.03385</a>.</p>
</div>
<div id="ref-ALIGN">
<p>Jia, Chao, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021b. “Scaling up Visual and Vision-Language Representation Learning with Noisy Text Supervision.” <em>CoRR</em>. <a href="https://arxiv.org/abs/2102.05918">https://arxiv.org/abs/2102.05918</a>.</p>
</div>
<div id="ref-VAE">
<p>Kingma, Diederik P., and Max Welling. 2019. “An Introduction to Variational Autoencoders.” <em>CoRR</em>. <a href="http://arxiv.org/abs/1906.02691">http://arxiv.org/abs/1906.02691</a>.</p>
</div>
<div id="ref-COCO">
<p>Lin, Tsung-Yi, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. 2014. “Microsoft Coco: Common Objects in Context.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1405.0312">https://doi.org/10.48550/ARXIV.1405.0312</a>.</p>
</div>
<div id="ref-VilBert">
<p>Lu, Jiasen, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019a. “ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks.” arXiv. <a href="https://doi.org/10.48550/ARXIV.1908.02265">https://doi.org/10.48550/ARXIV.1908.02265</a>.</p>
</div>
<div id="ref-GLIDE">
<p>Nichol, Alex, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021a. “GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2112.10741">https://doi.org/10.48550/ARXIV.2112.10741</a>.</p>
</div>
<div id="ref-CLIP">
<p>Radford, Alec, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, et al. 2021a. “Learning Transferable Visual Models from Natural Language Supervision.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2103.00020">https://doi.org/10.48550/ARXIV.2103.00020</a>.</p>
</div>
<div id="ref-DALLE">
<p>Ramesh, Aditya, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021a. “Zero-Shot Text-to-Image Generation.” arXiv. <a href="https://doi.org/10.48550/ARXIV.2102.12092">https://doi.org/10.48550/ARXIV.2102.12092</a>.</p>
</div>
<div id="ref-EfficientNet">
<p>Tan, Mingxing, and Quoc V. Le. 2019a. “EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.” <a href="https://doi.org/10.48550/ARXIV.1905.11946">https://doi.org/10.48550/ARXIV.1905.11946</a>.</p>
</div>
<div id="ref-attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017a. “Attention Is All You Need.” In <em>Advances in Neural Information Processing Systems</em>, edited by I. Guyon, U. Von Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett. Vol. 30. Curran Associates, Inc. <a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf</a>.</p>
</div>
<div id="ref-yuan2021florence">
<p>Yuan, Lu, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, et al. 2021. “Florence: A New Foundation Model for Computer Vision.” <em>arXiv Preprint arXiv:2111.11432</em>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="foreword.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="c01-00-intro-modalities.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook/js/app.min.js"></script>
<script src="libs/gitbook/js/clipboard.min.js"></script>
<script src="libs/gitbook/js/plugin-search.js"></script>
<script src="libs/gitbook/js/plugin-sharing.js"></script>
<script src="libs/gitbook/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook/js/plugin-bookdown.js"></script>
<script src="libs/gitbook/js/jquery.highlight.js"></script>
<script src="libs/gitbook/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["book.pdf", "book.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
